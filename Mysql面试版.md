# 一、基础

## 1、Mysql有哪些数据类型？

### 数值类型

有包括 TINYINT、SMALLINT、MEDIUMINT、INT、BIGINT，分别表示 1 字节、2 字节、3 字节、4 字节、8 字节的整数类型。

1）任何整数类型都可以加上 UNSIGNED 属性，表示无符号整数。

2）任何整数类型都可以指定长度，但它不会限制数据的合法长度，仅仅限制了显示长度。

还有包括 FLOAT、DOUBLE、DECIMAL 在内的小数类型。

### 字符串类型

包括 VARCHAR、CHAR、TEXT、BLOB。

注意：VARCHAR(n) 和 CHAR(n) 中的 **n 并不代表字节个数，而是代表字符的个数**。

### 日期和时间类型

常用于表示日期和时间类型为 DATETIME、DATE 和 TIMESTAMP。

**尽量使用 TIMESTAMP，空间效率高于 DATETIME。**

## 2、CHAR 和 VARCHAR 区别？

### 1）存储长度： 

CHAR 是定长的，而 VARCHAR 是可以变长。

CHAR 会根据声明的字符串长度分配空间，并会使用空格对字符串右边进行尾部填充。所以在检索 CHAR 类型数据时**尾部空格会被删除**，如保存的是字符串 'char  '，但最后查询到的是 'char'。又因为**长度固定，所以存储效率高于 VARCHAR 类型**。

VARCHAR 在 MySQL 5.0 之后长度支持到 65535 字节，但会在数据**开头使用额外 1~2 个字节存储字符串长度**（列长度小于 255 字节时使用 1 字节表示，否则 2 字节），**在结尾使用 1 字节表示字符串结束**。

### 2）在存储方式上：

**CHAR 对英文字符（ASCII）占用 1 字节，对一个汉字使用用 2 字节**。

 **VARCHAR 对每个字符均使用 2 字节**。

## 3、CHAR 和 VARCHAR 如何选择？

1. 从原则上来讲，将来字符串长度**不固定**的话，选择**varchar类型**，
    字符串长度**固定**不变则选择**char**类型

2. 实际上我们生产中在考虑性能问题的方面，
    需要有**大量插入**（insert）操作的应用中，我们可以考虑使用**char**去代替varchar。

3. 如果我们业务中，大量是查询类操作的应用中，数据量级又比较大情况下，变长长度数据类型，可以考虑采用varchar，一方面节省空间，可以有效的减少***索引树***的高度， 从而提高索引的优化查询的效果。
   
4. 为什么很多人喜欢使用varchar(255)？

  InnoDB存储引擎的表**索引单一字段或前缀长度**，**最长是767bytes**。
  存中文时，utf8字符集，那么767/3=255,
  如果是utf8mb4应当是，767/4=191.

5. 字符串长度值视实际数据长度，需占用 1 或 2 个字节存储。
     当实际数据长度 <= 255 时，varchar字段长度 = 1 + N
       当实际数据长度 > 255 时，varchar字段长度 = 2 + N

## 4、什么是三大范式？

- 第一范式（1NF）：**字段**（或属性）是**不可分割的最小单元**，即**表中的每个列都不可以再进行拆分**。体现**原子性**
- 第二范式（2NF）：**在满足1NF的前提下，表中不存在部分依赖，非主键列要完全依赖于主键。即存在唯一主键，体现唯一性**，专业术语则是消除部分函数依赖

- 第三范式（3NF）：满足 2NF 前提下，**非主属性必须互不依赖**，消除传递依赖

### 4.1 什么样的表越容易符合3NF？

非主键列越少的表。(1NF强调列不可再分；2NF和3NF强调非主属性列和主属性列之间的关系)

## 5、一条sql语句的执行流程？

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/sql%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B/mysql%E6%9F%A5%E8%AF%A2%E6%B5%81%E7%A8%8B.png" alt="查询语句执行流程" style="zoom:80%;" />

Mysql可以分为两层：**Server层和存储引擎层**

- **Server 层负责建立连接、分析和执行 SQL**。MySQL 大多数的核心功能模块都在这实现，主要包括**连接器，查询缓存、解析器、预处理器、优化器、执行器**等。另外，所有的内置函数（如日期、时间、数学和加密函数等）和所有跨存储引擎的功能（如存储过程、触发器、视图等。）都在 Server 层实现。
- **存储引擎层负责数据的存储和提取**。支持 InnoDB、MyISAM、Memory 等多个存储引擎，**不同的存储引擎共用一个 Server 层。**

Server 层按顺序执行 SQL 的步骤为：

- **客户端请求 -> 连接器**（验证用户身份，给予权限）

  - 连接的过程需要先经过 TCP 三次握手，因为 MySQL 是基于 TCP 协议进行传输的
  - MySQL 定义了空闲连接的最大空闲时长，由 `wait_timeout` 参数控制的，默认值是 8 小时（28880秒），如果**空闲连接超过了这个时间，连接器就会自动将它断开。**

- **查询缓存**（存在缓存则直接返回，不存在则执行后续操作）

  - 对于**更新比较频繁的表，查询缓存的命中率很低的**，所以，**MySQL 8.0 版本直接将查询缓存删掉了**，也就是说 MySQL 8.0 开始，执行一条 SQL 查询语句，不会再走到查询缓存这个阶段了。

- **解析器**（对 SQL 进行词法分析和语法分析操作）

- **执行 SQL**

  经过解析器后，接着就要进入执行 SQL 查询语句的流程了，每条`SELECT` 查询语句流程主要可以分为下面这三个阶段：

  - **预处理阶段**，由**预处理器**完成：
    - 检查 SQL 查询语句中的**表或者字段**是否存在；
    - 将 `select *` 中的 `*` 符号，**扩展为表上的所有列**；
  - **优化阶段**，**由优化器完成：**
    - **优化器主要负责将 SQL 查询语句的执行方案确定下来**，比如在表里面有多个索引的时候，优化器会基于查询成本的考虑，来**决定选择使用哪个索引。**
  - **执行阶段**，**由执行器完成**：
    - 执行的过程中，执行器就会**调用与存储引擎交互的API**，执行器会从从存储引擎读到记录，接着判断记录是否符合查询条件，如果符合则发送给客户端，如果不符合则跳过该记录。

## 6.Sql语句执行顺序

![image-20221115161734016](C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221115161734016.png)

(8) SELECT	(9) DISTINCT column,…
选择字段 、去重

(6) AGG_FUNC(column or expression),…
聚合函数

(1) FROM [left_table]
选择表

(3) <join_type> JOIN <right_table>
链接

(2) ON <join_condition>
链接条件

(4) WHERE <where_condition>
条件过滤

(5) GROUP BY <group_by_list>
分组

(7) HAVING <having_condition>
分组过滤

(10) ORDER BY <order_by_list>
排序

(11) LIMIT count OFFSET count;
分页

## 7.完整性约束

数据完整性：存储在数据库中的所有数据值均正确的状态。它是应**防止数据库中存在不符合语义规定的数据**和防止因错误信息的输入输出造成无效操作或错误信息而提出的。

常见的约束有：

- 非空约束（not null）
- 唯一约束（unique）
- 主键约束（primary key）
- 外键约束（foreign key）

当sql语句不满足约束条件时，数据库会报错

# 二、索引篇

## 1、索引的分类

### I.按数据结构分类： 

- B+Tree 索引
- HASH 索引
- Full-Text 索引

<img src="C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221010160443605.png" alt="image-20221010160443605" style="zoom:67%;" />

在创建表时，InnoDB 存储引擎会根据不同的场景选择不同的列作为索引：

- 如果有主键，默认会使用主键作为聚簇索引的索引键（key）；
- 如果没有主键，就选择第一个不包含 NULL 值的唯一列作为聚簇索引的索引键（key）；
- 在上面两个都没有的情况下，InnoDB 将自动生成一个隐式自增 id 列作为聚簇索引的索引键（key）；

其它索引都属于辅助索引（Secondary Index），也被称为二级索引或非聚簇索引。**创建的主键索引和二级索引默认使用的是 B+Tree 索引**。

### II.按物理存储分类：

- 索引分为聚簇索引（主键索引）：叶子节点存放行数据
- 二级索引（辅助索引）：叶子节点存放主键值

查询时使用了二级索引，如果查询的数据能在二级索引里查询的到，那么就不需要回表，这个过程就是覆盖索引。如果查询的数据不在二级索引里，就会先检索二级索引，找到对应的叶子节点，获取到主键值后，然后再检索主键索引，就能查询到数据了，这个过程就是回表。

### III、按照字段特性分类：

- 主键索引：一张表最多只有一个主键索引，索引列的值**不允许有空值**。、

```sql
PRIMARY KEY (index_column_1) USING BTREE
```

- 唯一索引：唯一索引建立在 UNIQUE 字段上的索引，一张表可以有多个唯一索引，**索引列的值必须唯一，但是允许有空值。**

```sql
CREATE UNIQUE INDEX index_name
ON table_name(index_column_1,index_column_2,...); 
```

- 普通索引：普通索引就是建立在普通字段上的索引，既不要求字段为主键，也不要求字段为 UNIQUE。

```sql
CREATE INDEX index_name
ON table_name(index_column_1,index_column_2,...); 
```

- 前缀索引：是指对字符类型字段的前几个字符建立的索引，减少索引占用的存储空间，提高查询效率。

```sql
CREATE INDEX index_name
ON table_name(column_name(length)); 
```

### IV、按照字段个数分类

- 单列索引：建立在单列上的索引称为单列索引，比如主键索引；
- 联合索引（复合索引）：**通过将多个字段组合成一个索引**，该索引就被称为联合索引。

## 2、什么时候需要建立索引或者不用建立索引？

### 什么时候适用索引？

- 字段**有唯一性限制**；
- **经常用于 `WHERE` 查询条件的字段**，这样能够提高整个表的查询速度，如果查询条件不是一个字段，可以建立联合索引。
- **经常用于 `GROUP BY` 和 `ORDER BY` 的字段**，这样在查询的时候就不需要再去做一次排序了，因为我们都已经知道了建立索引之后在 B+Tree 中的记录都是排序好的

### 什么时候不需要创建索引？

- `WHERE` 条件，`GROUP BY`，`ORDER BY` 里用不到的字段，索引的价值是快速定位，如果起不到定位的字段通常是不需要创建索引的，因为索引是会占用物理空间的。
- 字段中**存在大量重复数据**，不需要创建索引，比如性别字段，只有男女，MySQL 有一个查询优化器，查询优化器发现某个值出现在表的数据行中的百分比很高的时候，它一般会忽略索引，进行全表扫描。
- 表数据太少的时候，不需要创建索引；
- **经常更新的字段不用创建索引**，比如不要对电商项目的用户余额建立索引，因为索引字段频繁修改，由于**要维护 B+Tree的有序性**，那么就需要频繁的**重建索引，这个过程是会影响数据库性能的**

## 3.索引优化的方法？

### I.前缀索引优化：

所谓前缀索引就是对一个字段的前几个字符创建索引，前缀索引可以减小索引字段的大小，有效提高查询速度。

前缀索引有一定的局限性，例如：

- order by 就无法使用前缀索引；
- 无法把前缀索引用作覆盖索引；

### II.覆盖索引优化：

覆盖索引是指一条sql语句中查询的字段，在索引B+树的节点上都能找到的那些索引，从二级索引能查得到的记录就不用回表查询簇聚索引了。

我们可以建立一个**联合索引，**联合索引中包含所有要查询的字段。

### III.最好将主键索引设置为自增的

Innodb存储引擎中将数据存储在B+Tree的叶子节点，同一个叶子节点内的各个数据是**按主键顺序存放的**

**如果我们使用非自增主键**，由于每次插入主键的索引值都是随机的，可能会发生**页分裂**。**页分裂还有可能会造成大量的内存碎片，导致索引结构不紧凑，从而影响查询效率**。

### IV.主键索引字段不应过长

主键字段的长度不要太大，因为**主键字段长度越小，意味着二级索引的叶子节点越小（二级索引的叶子节点存放的数据是主键值），这样二级索引占用的空间也就越小**。

### V.索引最好设为NOT NULL

为了更好的利用索引，索引列要设置为 NOT NULL 约束。有两个原因：

- 第一原因：索引列存在 NULL 就会**导致优化器在做索引选择的时候更加复杂，更加难以优化**，因为可为 NULL 的列会使索引、索引统计和值比较都更复杂，比如**进行索引统计时，count 会省略值为NULL 的行**。
- 第二个原因：**NULL 值是一个没意义的值，但是它会占用物理空间**，所以会带来的存储空间的问题，会导致更多的存储空间占用，因为 InnoDB 会用 **1 字节空间存储 NULL 值列表**

### VI.防止索引失效

## 4.索引失效的场景

### I.对索引使用左或者左右模糊匹配（%在右无影响）

```sql
// name 字段为二级索引
select * from t_user where name like '%升';
select * from t_user where name like '%常%';
```

![image-20221020171140373](C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221020171140373.png)



**失效原因：索引 B+ 树是按照「索引值」有序排列存储的，只能根据前缀进行比较。**

### II.对索引使用函数

有时候我们会用一些 MySQL 自带的函数来得到我们想要的结果，这时候要注意了，如果查询条件中对索引字段使用函数，就会导致索引失效。

比如下面这条语句查询条件中对 name 字段使用了 LENGTH 函数，执行计划中的 **type=ALL**，代表了全表扫描：

```sql
// name 为二级索引
select * from t_user where length(name)=6;
```

**失效原因：因为索引保存的是索引字段的原始值，而不是经过函数计算后的值，自然就没办法走索引了。**

不过，从 MySQL 8.0 开始，索引特性增加了函数索引，例：

```sql
alter table t_user add key idx_name_length ((length(name)));
```

### III.对索引进行表达式计算

下面这条查询语句，执行计划中 **type = ALL**，说明是通过全表扫描的方式查询数据的：

```sql
explain select * from t_user where id + 1 = 10;
```

但是，如果把查询语句的条件改成 **where id  = 10 - 1**，这样就不是在索引字段进行表达式计算了，于是就可以走索引查询了。

**失效原因：因为索引保存的是索引字段的原始值，而不是 id + 1 表达式计算后的值，所以无法走索引**

### **IV.对索引隐式类型转换**

如果索引字段是**字符串类型**，但是在条件查询中，**输入的参数是整型**的话，你会在执行计划的结果发现这条语句会走全表扫描。

```sql
在条件查询中，用整型作为输入参数，此时执行计划中 type = ALL，所以是通过全表扫描来查询数据的。

select * from t_user where phone = 1300000001;
```

但是如果**索引字段是整型类型**，查询条件中的**输入参数即使字符串**，是**不会导致**索引失效，还是可以走索引扫描。

```sql
下面这条语句还是走了索引扫描的。
explain select * from t_user where id = '1';
```

原因：

MySQL的数据类型转换规则是遇到字符串和数字比较的时候，会**自动把字符串转为数字**，然后再进行比较。但是不会把数字转换为字符串。

```sql
//例子一的查询语句
select * from t_user where phone = 1300000001;
```

这是因为 phone 字段为字符串，所以 MySQL 要会自动把字符串转为数字，所以这条语句相当于：

```sql
select * from t_user where CAST(phone AS signed int) = 1300000001;
```

**可见相当于在索引上使用了函数，导致索引失效**

### V.联合索引非最左匹配（含索引下推）

如果创建了一个 `(a, b, c)` 联合索引，如果查询条件是以下这几种，就可以匹配上联合索引：

- where a=1；
- where a=1 and b=2 and c=3；
- where a=1 and b=2；
- where b = 3 and a = 1 and c = 3

因为有查询优化器，所以**字段在 where 子句的顺序并不重要**。

但是，如果查询条件是以下这几种，因为**不符合最左匹配原则**，所以就无法匹配上联合索引，联合索引就会失效:

- where b=2；
- where c=3；
- where b=2 and c=3；

**如果建立 (a,b,c,d) 索引**，查询条件 b = 2 是匹配不到索引的，但是如果查询条件是 a = 1 and b = 2 或 a=1 又或 b = 2 and a = 1 就可以，因为优化器会自动调整 a,b 的顺序。

再比如 a = 1 and b = 2 and c > 3 and d = 4，其中 d 是用不到索引的，**因为 c 是一个范围查询**，它之后的字段会停止匹配。
在MySQL5.6之前，会根据a,b索引查询出来的值进行回表查询，扫描出所有结果再过滤掉不满足c,d条件的数据；

在MySQL5.6之后，引入了**索引下推**功能，可以通过索引中存储的数据判断当前索引对应的数据是否符合条件，**只有符合条件的数据才将整行数据查询出来**。查看执行计划时发现extra一栏中有**Using index condition**信息，说明使用了索引下推。

<img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAxOS8xMi8xMC8xNmVlZDY3YTc4MTEyOWEy?x-oss-process=image/format,png" alt="img" style="zoom: 80%;" />

**不使用索引条件下推优化时的查询过程：**

- 获取下一行，首先读取索引信息，然后根据索引将整行数据读取出来

- 然后通过where条件判断当前数据是否符合条件，符合返回数据

**使用索引条件下推优化时的查询过程：**

- 获取下一行的索引信息
- 检查索引中存储的列信息是否符合索引条件，如果符合将整行数据读取出来，如果不符合跳过读取下一行
- 用剩余的判断条件，判断此行数据是否符合要求，符合要求返回数据

### VI、WHERE 子句中的 OR含有没有索引的字段

下面的查询语句，id 是主键，age 是普通列，从执行计划的结果看，是走了全表扫描。

```sql
select * from t_user where id = 1 or age = 18;
```

**失效原因：这是因为 OR 的含义就是两个只要满足一个即可，因此只有一个条件列是索引列是没有意义的，只要有条件列不是索引列，就会进行全表扫描。**

## 5、B+树如何存储数据？

**InnoDB 的数据是按「数据页」为单位来读写的**，也就是说，当需要读一条记录的时候，并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。

数据库的 I/O 操作的最小单位是页，**InnoDB 数据页的默认大小是 16KB**，意味着数据库每次读写都是以 16KB 为单位的，一次最少从磁盘中读取 16K 的内容到内存中，一次最少把内存中的 16K 内容刷新到磁盘中。

**在 File Header 中有两个指针，分别指向上一个数据页和下一个数据页，连接起来的页相当于一个双向的链表**

<img src="https://img-blog.csdnimg.cn/img_convert/557d17e05ce90f18591c2305871af665.png" alt="图片" style="zoom:67%;" />

数据页内包含用户记录，每个记录之间用单向链表的方式组织起来，为了加快在数据页内高效查询记录，设计了一个**页目录**，页目录存储**各个槽（分组）**，**各个槽指向每个分组的最后一个数据**，且主键值是有序的，于是可以通过二分查找法的方式进行检索从而提高效率。

<img src="https://img-blog.csdnimg.cn/img_convert/3104c8c3adf36e8931862fe8a0520f5d.png" alt="图片" style="zoom:80%;" />

## 6.count(1)和count(*)有什么区别？

<img src="C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221020190732595.png" alt="image-20221020190732595" style="zoom:67%;" />

### I、count(主键字段) 执行过程是怎样的？

在通过 count 函数统计有多少个记录时，MySQL 的 server 层会维护一个名叫 count 的变量。

如果表里**只有主键索引，没有二级索引时**，那么，InnoDB **循环遍历聚簇索引**，将读取到的记录返回给 server 层，然后读取记录中的 id 值，就会 id 值判断是否为 NULL，**如果不为 NULL，**就将 count 变量加 1。

但是，如果表里**有二级索引时**，InnoDB **循环遍历的对象就变为二级索引。**

### II、count(1) 执行过程是怎样的？

如果表里只有主键索引，没有二级索引时。，InnoDB 循环遍历聚簇索引（主键索引），将读取到的记录返回给 server 层，**但是不会读取记录中的任何字段的值**，因为 count 函数的参数是 1，不是字段，所以不需要读取记录中的字段值。参数 1 很明显并不是 NULL，因此 server 层每**从 InnoDB 读取到一条记录，就将 count 变量加 1，即使id值为NULL也会读取。**

由于count(1)与count(*)相比**少了读取记录中字段值这个步骤**，所以通常来说**执行效率会高一点。**

### III、count(*) 执行过程是怎样的？

**count(`*`) 其实等于 count(`0`)**，也就是说，当你使用 count(`*`) 时，MySQL 会将 `*` 参数转化为参数 0 来处理。

所以，**count(\*) 执行过程跟 count(1) 执行过程基本一样的**，性能没有什么差异。

在 MySQL 5.7 的官方手册中有这么一句话：

**InnoDB以相同的方式处理SELECT COUNT（`*`）和SELECT COUNT（`1`）操作，没有性能差异。**

而且 MySQL 会对 count(*) 和 count(1) 有个优化，如果**有多个二级索引**的时候，优化器会使用**key_len 最小的二级索引**进行扫描。

只有当**没有二级索引的时候，才会采用主键索引来进行统计。**

### IV、count(字段) 执行过程是怎样的？

count(字段) 的执行效率相比前面的 count(1)、 count(*)、 count(主键字段) 执行**效率是最差**的。

对于这个查询来说，会采用**全表扫描**的方式来计数，所以它的执行效率是比较差的。

![image-20221020192015375](C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221020192015375.png)

## 7、如何优化count(*)?

如果表数据非常大，假设有一千多万条数据，那么count(*)会花费将近5s,这时，我们就可以使用 **show table status** 或者 **explain** 命令来表进行估算。

### 第一种，用EXPLAIN查询近似数

**执行 explain 命令效率是很高的**，因为它并不会真正的去查询，下图中的 rows 字段值就是 explain 命令对表 t_order 记录的**估算值。**如果**需要精确值，则不可使用此方法**

![image-20221020192510483](C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221020192510483.png)

​																					**（explain查询所得）**

### 第二种，额外表保存计数值

如果是想精确的获取表的记录总数，我们可以将这个计数值保存到单独的一张计数表中。

当我们在数据表插入一条记录的同时，将计数表中的计数字段 + 1。也就是说，**在新增和删除操作时，我们需要额外维护这个计数表。**

# 三、SQL优化大全

### **1、尽量避免使用select ***

```sql
#错误实例：
select * from tb_user;
#正确示例
select id, name from tb_user; 
```

**原因：**

- 一次性查询出大量数据会浪费CPU和内存资源
- 多出来的数据在传输过程中会浪费网络IO时间
- select * 无法走覆盖索引，会发生回表

### **2、用union all 代替union**

union all：获取所有数据，包含重复行数据

union ：获取所有数据，不会排除重复行数据

原因：

- 排除重复的数据需要遍历，排序和比较，更加耗时，浪费CPU资源

### **3.小表驱动大表**

小表驱动大表的主要目的是通过**减少表连接创建的次数，**加快查询速度 。

通过EXPLAIN查看SQL语句的执行计划可以判断在谁是驱动表，EXPLAIN语句分析出来的**第一行的表即是驱动表** ;如下，t_blog就是驱动表

![image-20221125143012232](C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221125143012232.png)

假设以下示例 user表中有100条数据，order 表中有10000条数据

```sql
select * from order
where user_id in (select id from user where status = 1)
##上下两条sql语句执行结果相同
select * from order
where exists (select 1 from user where order.user_id = user.id and status = 1)
```

这两条sql语句的区别在于，exists优先执行左边的语句，而in优先执行右边

**总结：in适用于左边大表右边小表，exists适用于左边小表右边大表**

### 4、批量操作优化

```java
for(int i = 0; i < 1000; i++) {
    orderMapper.insert(id);//相当于请求连接了1000次数据库，性能消耗大
}
//代替方案
orderMapper.insertBatch(idList);//只需连接一次数据库
```

**注意：不能一次批量操作过多数据，过多也会导致数据库响应过慢，建议500条数据以内**

### 5、多用limit

以下场景，获取用户所有订单中的第一个

```sql
select id, create_date
from order 
where user_id = 123
order by create_date asc;

List<Order> list = orderMapper.getOrderList();
Order order = list.get(0);
```

以上查询方法浪费资源，可以用如下方法进行优化,利用limit只返回一条即可

```sql
select id, create_date
from order 
where user_id = 123
order by create_date asc;
limit 1

Order order = orderMapper.getOrder();
```

### 6、高效的分页

例如以下示例，分页参数过大，查询出结果过多

```sql
select id, name, age
from user limit 1000000, 10 #会读取出1000010条记录然后丢弃10条
```

**优化为：**

```sql
select id, name, age					
from user where id >= 100001 limit 10 # 只会读取出十条，但要保证id必须是连续的
```

### 7、用连接查询代替子查询

**原因：在创建子查询的时候，数据库会创建临时表，再查询完毕之后会删除临时表，有一些额外的性能消耗**

### 8、join的表不易过多

阿里手册规范：join表的数量不宜超过5个

如果join表过多，再优化阶段很容易选错索引，并且如果没有命中的话，

### 9.尽量使用inner join

原因：

使用inner join数据库会默认使用小表驱动大表

使用left join数据库会默认左表驱动右边，如果左表较大，查询性能下降

### 10、控制索引数量

单表索引数量控制在5个以内，并且单个索引中字段数尽量不要超过5个，在高并发系统下尽量建立联合索引，尽量少建立单列索引。

### 11、为字段选择合适的数据类型

在长度不确定的情况下varchar代替char

能用数字类型就不用字符串，比如性别用0和1来代替，数据类型可选tinyInt

删除字段可用逻辑删除，即用0和1标识是否删除

### 12、提升group by效率

```sql
#先分组再筛选，分组较耗时
select user_id, user_name from order
group by user_id
having order_id <= 200;

#先筛选再分组，把分组放到之后，减少了数据量，分组较为迅速，性能较好
select user_id, user_name from order
where order_id <= 200 
group by user_id
```

### 13、索引优化，explain查看执行计划

对于执行计划，参数有：

- possible_keys 字段表示可能用到的索引；
- key 字段表示实际用的索引，如果这一项为 NULL，说明没有使用索引；
- key_len 表示索引的长度；
- rows 表示扫描的数据行数。
- type 表示数据扫描类型，我们需要重点看这个。

type 字段就是描述了找到所需数据时使用的扫描方式是什么，常见扫描类型的**执行效率从低到高的顺序为**：

- All（全表扫描）；
- index（全索引扫描）；
- range（索引范围扫描）；
- ref（非唯一索引扫描）；
- eq_ref（唯一索引扫描）；
- const（结果只有一条的主键或唯一索引扫描）。

# 四、事务篇

## 1、事务有哪些特性？由什么来保证？

**ACID**

- **原子性：**一个事务中的所有操作**，要么全部完成，要么全部不完成**，不会结束在中间某个环节，而且事务在执行过程中发生错误，会被回滚到事务开始前的状态。
- **一致性：**是指事务操作前和操作后，**数据满足完整性约束（完整性约束不能被破坏），数据库保持一致性状态，即数据库中的数据在事务执行完成后不出现前后矛盾情况，必须符合操作逻辑。**
  - 完整性约束是为了表的数据的正确性！如果数据不正确，那么一开始就不能添加到表中。
    - 主键约束
    - 外键约束
    - 非空和唯一约束

- **隔离性：**数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以**防止多个事务并发执行时由于交叉执行而导致数据的不一致**，因为多个事务同时使用相同的数据时，不会相互干扰，每个事务都有一个完整的数据空间，对其他并发事务是隔离的。
- **持久性：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。**

### InnoDB 引擎通过什么技术来保证事务的这四个特性的呢？

- 原子性由undolog(回滚日志)来保证
- 持久性由redo log来保证
- 隔离性由MVCC或锁来保证
- 一致性则是由原子性 + 持久性 +隔离性 共同保证

## 2、事务并发可能会带来哪些问题？

- **脏读：**一个客户端读取到了另一个客户端未提交事务修改过的数据
- **不可重复读：**一个客户端读取到了另一个客户端已提交修改过的数据
- **幻读：**一个客户端在一次事务过程中读取到记录的数量不同，比如在t1读取到字段数量为5，t2读取到字段数量为6

## 3.事务的隔离级别有哪些？

- **读未提交（read uncommited）:**一个事务还没有提交时，另一个事务可以看到该事务对数据的变更
- **读提交（read commited）:**一个事务提交后，另一个未提交的事务可以看到该事务对数据的变更
- **可重复读（repeatable read）:**指一个事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，**MySQL InnoDB 引擎的默认隔离级别**；
- **串行化（serializable）：**会对记录加上读写锁，在多个事务对这条记录进行读写操作时，如果发生了读写冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行；

### 事务隔离级别水平高低如下：

**串行化 >  可重复读 > 读提交 > 读未提交**

- 在「读未提交」隔离级别下，可能发生脏读、不可重复读和幻读现象；
- 在「读提交」隔离级别下，可能发生不可重复读和幻读现象，但是不可能发生脏读现象；
- 在「可重复读」隔离级别下，可能发生幻读现象，但是不可能脏读和不可重复读现象；
- 在「串行化」隔离级别下，脏读、不可重复读和幻读现象都不可能会发生。

### InnoDB存储引擎如何避免RR隔离级别下的幻读现象？

**MySQL InnoDB 引擎的默认隔离级别虽然是「可重复读」，但是它很大程度上避免幻读现象（并不是完全解决了）**，解决的方案有两种：

- 针对**快照读**（普通 select 语句），是**通过 MVCC 方式解决了幻读**，因为可重复读隔离级别下，事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，即使中途有其他事务插入了一条数据，是查询不出来这条数据的，所以就很好了避免幻读问题。
- 针对**当前读**（select ... for update 等语句），是**通过 next-key lock（记录锁+间隙锁）方式解决了幻读**，因为当执行 select ... for update 语句的时候，会加上 next-key lock，如果有其他事务在 next-key lock 锁范围内插入了一条记录，那么这个插入语句就会被阻塞，无法成功插入，所以就很好了避免幻读问题。
  - **for update、lock in share mode、update、delete、insert**都遵循**当前读**的规则

### 这四种隔离级别具体是如何实现的呢？

- 对于「读未提交」隔离级别的事务来说，因为可以读到未提交事务修改的数据，所以直接读取最新的数据就好了；
- 对于「串行化」隔离级别的事务来说，通过**加读写锁**的方式来避免并行访问；
- 对于「读提交」和「可重复读」隔离级别的事务来说，它们是通过 **Read View来实现的，它们的区别在于创建 Read View 的时机不同。「读提交」隔离级别是在「每个语句执行前」都会重新生成一个 Read View，而「可重复读」隔离级别是「启动事务时」生成一个 Read View，然后整个事务期间都在用这个 Read View**。

## 4、MVCC

### I.什么是MVCC？

MVCC：通过**「版本链」**来控制**并发事务访问同一个记录**时的行为就叫 MVCC（多版本并发控制），**readView + undo log**实现了MVCC机制，MVCC机制解决了**RR隔离级别下快照读（普通select语句）的幻读问题**。

### II.ReadView是什么？

ReadView是**每个事务开启时生成的一种数据结构**，在事务的不同隔离级别下生成时机不同。

:smile:**每个事务维护一个属于自己的ReadView。**

**Read View 有四个重要的字段：**

<img src="C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221020235816297.png" alt="image-20221020235816297" style="zoom:67%;" />

- m_ids ：指的是在创建 Read View 时，当前数据库中「活跃事务」的**事务 id 列表**，注意是一个列表，**“活跃事务”指的就是，启动了但还没提交的事务**。
- min_trx_id ：指的是在创建 Read View 时，当前数据库中「活跃事务」中事务 **id 最小的事务**，也就是 m_ids 的最小值。
- max_trx_id ：这个并不是 m_ids 的最大值，而是**创建 Read View 时当前数据库中应该给下一个事务的 id 值**，也就是全局事务中最大的事务 id 值 + 1；
- creator_trx_id ：指的是**创建该 Read View 的事务的事务 id**。

**聚簇索引记录中的两个隐藏列：**

<img src="https://img-blog.csdnimg.cn/img_convert/f595d13450878acd04affa82731f76c5.png" alt="图片" style="zoom:67%;" />

<img src="C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221021000031504.png" alt="image-20221021000031504" style="zoom:67%;" />

- trx_id，当一个事务对某条聚簇索引记录进行改动时，就会**把该事务的事务 id 记录在 trx_id 隐藏列里**；
- roll_pointer，每次对某条聚簇索引记录进行改动时，都会把**旧版本的记录写入到 undo 日志中**，然后**这个隐藏列是个指针，指向每一个旧版本记录**，**将所有旧数据连在一起，**于是就可以通过它找到修改前的记录。

### III、ReadView在MVCC中是如何工作的？

:happy: **在RR隔离级别下**：

ReadView在**每个事务开启时生成，并且只生成一次**，在该事务执行到**提交之前**的一切读取操作都**一直使用该ReadView**。

当一个事务去读取一条记录时，会先去undolog对该记录生成的版本链上去找，版本链上每条记录都有两个隐藏列，分别是**trx_id和roll_pointer**,在读取记录时，该事务会根据**ReadView中记录的min_trx_id**和**undolog版本链中的trx_id**比较来判断是否能读取，如果**trx_id < min_trx_id才能读取**，否则，事务会**沿着roll_pointer向前寻找**，直到记录的trx_id < min_trx_id才会将该条记录返回给用户。

**:happy:在RC隔离级别下：**

读取规则和在RR隔离级别下时相同的，只不过RC隔离级别下每次读取都会生成ReadView，所以**min_trx_id**会随着其它事务的commit而改变，导致一个事务在提交之前会读到另一个已提交事务修改的内容。

### IV、MVCC完全解决RR隔离级别下快照读的幻读问题了吗？

**没有**，例如以下情况：

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/%E9%94%81/%E5%B9%BB%E8%AF%BB%E5%8F%91%E7%94%9F.drawio.png" alt="img" style="zoom: 80%;" />

**事务A更新了其看不到的但真实存在的记录，再次查询却查出了该记录。**

# 五、MySQL中的各种锁

## 1、全局锁

### I.全局锁的使用

:watermelon: **要使用全局锁，则要执行这条命令：**

```sql
flush tables with read lock
```

执行后，**整个数据库就处于只读状态了**，这时其他线程执行以下操作，都会被阻塞：

- 对数据的增删改操作，比如 insert、delete、update等语句；
- 对表结构的更改操作，比如 alter table、drop table 等语句。

:orange: **如果要释放全局锁，则要执行这条命令：**

```sql
unlock tables
```

**当会话断开了，全局锁会被自动释放。**

### II.全局锁的使用场景

全局锁主要应用于做**全库逻辑备份**，这样在备份数据库期间，不会因为数据或表结构的更新，而出现备份文件的数据与预期的不一样。

### III.加全局锁的缺点

加上全局锁，意味着整个数据库都是只读状态。

那么如果数据库里有很多数据，备份就会花费很多的时间，关键是备份期间，**业务只能读数据，而不能更新数据，这样会造成业务停滞。**

### IV.如何避免全局锁带来的业务停滞

备份数据库的工具是 **mysqldump**，在使用 mysqldump 时加上 `–single-transaction` 参数的时候，就会在**备份数据库之前先开启事务**，在MVCC机制下，备份过程的备份数据也一直是事务开启时的数据，确保了隔离性。但这种方法只适用于**支持「可重复读隔离级别的事务」的存储引擎。**对于 **MyISAM 这种不支持事务的引擎，在备份数据库时就要使用全局锁的方法**。

## 2、表级锁

表级锁有以下四种：

- **表锁**
- **元数据锁（MDL）**
- **意向锁**
- **AUTO-INC锁**

### I.表锁

**加表锁，可以使用下面的命令：**

```sql
//表级别的共享锁，也就是读锁；
lock tables t_student read;

//表级别的独占锁，也就是写锁；
lock tables t_stuent write;
```

**释放所有表锁的命令**：

```sql
unlock tables 
```

### II.元数据锁

**不需要显示的使用 MDL，**因为当我们对数据库表进行操作时，会自动给这个表加上 MDL：

- 对一张表进行 CRUD**（DML语句）** 操作时，加的是 **MDL 读锁**；
- 对一张表做结构变更操作的时候**（DDL语句）**，加的是 **MDL 写锁**；

:elephant:**MDL保证当前用户对一张表执行CRUD操作时，防止其它用户对表结构进行修改。**

:horse: **MDL 是在事务提交后才会释放，事务执行期间，MDL 是一直持有的**。

为了能安全的对表结构进行变更，在对表结构变更前，先要看看数据库中的长事务，是否有事务已经对表加上了 MDL 读锁，如果可以考虑 kill 掉这个长事务，然后再做表结构的变更。

### III、意向锁

当执行**插入(insert)、更新(update)、删除(delete)**操作，需要先对表加上**「意向独占锁」**，然后对该记录加独占锁。

普通的select属于快照读，是利用MVCC机制实现一致性读的，无需加锁，但也可以通过以下两种方法**在读操作时对记录进行加锁：**

- 在对InnoDB的某一张表里的**某条记录加共享锁之前**，需要先在**表级别上加一个意向共享锁**。
- 在对InnoDB的某一张表里的**某条记录加独占锁之前**，需要先在**表级别上加一个意向独占锁**。

```sql
//先在表上加上意向共享锁，然后对读取的记录加共享锁
select ... lock in share mode;

//先表上加上意向独占锁，然后对读取的记录加独占锁
select ... for update;
```

**意向共享锁和意向独占锁是表级锁，不会和行级的共享锁和独占锁发生冲突，而且意向锁之间也不会发生冲突，只会和共享表锁（lock tables ... read）和独占表锁（lock tables ... write）发生冲突。**

####  **意向锁存在的意义：**

:monkey_face:**快速判断一张表里是否有记录被加了行锁。**

意向锁是一个事务对表中的记录加行锁时自动为表上的锁的。当事务A对记录加了行独占写锁，事务B却试图用`lock tables...write`对该表加表独占写锁，这时会发生冲突。所以为避免这种情况，事务B需要在加锁前遍历表，判断是否有记录被加了独占写锁，如果有，则加锁失败。但遍历的过程比较耗时，所以意向锁也就在这时产生了意义，因为一个**被加了行独占写锁的表早已被加上了意向独占锁**，事务**B仅需判断意向独占锁是否存在即可**，大大提高了性能。

### IV.AUTO-INC锁

在为某个字段声明 `AUTO_INCREMENT` 属性时，之后可以在插入数据时，可以不指定该字段的值，数据库会自动给该字段赋值递增的值，这主要是通过 AUTO-INC 锁实现的。

AUTO-INC锁有一个特殊的机制：**不在事务执行完后才会释放锁，而是在执行完一条插入语句后就释放。**

但是， AUTO-INC 锁再对**大量数据进行插入的时候，会影响插入性能**，因为另一个事务中的插入会被阻塞。

因此， 在 MySQL 5.1.22 版本开始，InnoDB 存储引擎提供了一种**轻量级的锁**来实现自增。

一样也是在插入数据的时候，会为被 `AUTO_INCREMENT` 修饰的字段加上轻量级锁，**然后给该字段赋值一个自增的值，就把这个轻量级锁释放了，而不需要等待整个插入语句执行完后才释放锁**。

InnoDB 存储引擎提供了个 innodb_autoinc_lock_mode 的系统变量，是用来控制选择用 AUTO-INC 锁，还是轻量级的锁。

- 当 innodb_autoinc_lock_mode = 0，就采用 AUTO-INC 锁；
- 当 innodb_autoinc_lock_mode = 2，就采用轻量级锁；
- 当 innodb_autoinc_lock_mode = 1，这个是默认值，两种锁混着用，如果能够确定插入记录的数量就采用轻量级锁，不确定时就采用 AUTO-INC 锁。

不过，当 innodb_autoinc_lock_mode = 2 是性能最高的方式，但是会带来一定的问题。因为并发插入的存在，在每次插入时，自增长的值可能不是连续的，**这在有主从复制的场景中是不安全的**

## 3、行锁

行级锁的类型主要有三类：

- Record Lock，记录锁，也就是仅仅把一条记录锁上；
- Gap Lock，间隙锁，锁定一个范围，但是不包含记录本身；
- Next-Key Lock：Record Lock + Gap Lock 的组合，锁定一个范围，并且锁定记录本身。

### I.Recore Lock(记录锁)

记录锁有S锁和X锁之分，**S/S共享；S/X，X/X互斥。**

```sql
mysql > begin;
mysql > select * from t_test where id = 1 for update;
```

当我进行以上操作的时候的时候，会对id=1这条记录加锁，在commit的时候该锁被释放。

### II.Gap Lock(间隙锁)

Gap Lock 称为间隙锁，**只存在于可重复读隔离级别，目的是为了解决可重复读隔离级别下幻读的现象。**

假设事务A持有表中id在（3，5）之间的间隙锁，那么事务B想插入一条id=4的记录就会发生阻塞。

间隙锁虽然存在 X 型间隙锁和 S 型间隙锁，但是并没有什么区别，**间隙锁之间是兼容的，即两个事务可以同时持有包含共同间隙范围的间隙锁，并不存在互斥关系，因为间隙锁的目的是防止插入幻影记录而提出的**。

### III、Next-Key-Lock(临键锁)

Next-Key Lock 称为临键锁，**是 Record Lock + Gap Lock 的组合**，锁定一个范围，并且锁定记录本身。

**next-key lock 是包含间隙锁+记录锁的，如果一个事务获取了 X 型的 next-key lock，那么另外一个事务在获取相同范围的 X 型的 next-key lock 时，是会被阻塞的**。

比如，一个事务持有了范围为 (1, 5] 的 X 型的 next-key lock，那么另外一个事务在获取相同范围的 X 型的 next-key lock 时，就会被阻塞。

虽然相同范围的间隙锁是多个事务相互兼容的，但对于记录锁，我们是要考虑 X 型与 S 型关系，X 型的记录锁与 X 型的记录锁是冲突的。

### IV.插入意向锁

当一个事务A要在某一区间内插入一条记录时，会首先判断该区间是否被其他事务加上了间隙锁（next-key-lock也包含间隙锁），如果间隙锁存在，那么插入操作将被阻塞，并且该事务A会在该记录上加一个**插入意向锁，然后将锁状态设置为等待**。

假设事务 A 已经对表加了一个范围 id 为（3，5）间隙锁。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/%E9%94%81/gap%E9%94%81.drawio.png" alt="img" style="zoom:77%;" />

当事务 A 还没提交的时候，事务 B 向该表插入一条 id = 4 的新记录，这时会判断到插入的位置已经被事务 A 加了间隙锁，于是事物 B 会生成一个插入意向锁，然后将锁的状态设置为等待状态

**（*PS：MySQL 加锁时，是先生成锁结构，然后设置锁的状态，如果锁状态是等待状态，并不是意味着事务成功获取到了锁，只有当锁状态为正常状态时，才代表事务成功获取到了锁*）**

**尽管「插入意向锁」也属于间隙锁，但两个事务却不能在同一时间内，一个拥有间隙锁，另一个拥有该间隙区间内的插入意向锁**

## 4、MySQL加锁的规则

**:smiling_imp:MySQL加锁的基本单位时Next-Key-Lock**

![图片](https://img-blog.csdnimg.cn/img_convert/954708d2f29c2a619e861e57cdf74c11.png)

**明确一点：在判断一条sql语句如何加锁的时候，我们必须要知道加锁的基本单位是next-lock-key,只是不同索引的不同查询方式会对加锁规则作出不同的行为。**

### I、唯一索引等值查询

- 当记录**不存在**时，使用唯一索引等值查询，先向右找到第一条存在的记录然后锁住该条记录与前一条记录之间的间隙，**next-key-lock会退化为gap lock**;

```sql
#该条sql首先是加(4,8]的next-key-lock，由于是唯一索引等值查询且记录不存在，所以退化为(4, 8)gap lock
select * from t_test where id = 5;
```

- 当记录**存在**时，使用唯一索引等值查询，**next-key-lock会退化为record lock;**

```sql
#该条sql首先是加(4,8]的next-key-lock，由于是唯一索引等值查询，所以退化为id = 4这一条记录的record-key
select * from t_test where id = 8;
```



### II.唯一索引范围查询

**会访问到不满足条件的第一个值为止，到不满足条件的值之前都被加上了next-key-lock。**

**先加next-key-lock，如果where条件判断里的范围小于加锁范围，那么next-key-lock就退化为gap lock。**

举个例子，下面这两条查询语句，查询的结果虽然是一样的，但是加锁的范围是不一样的。

```sql
#加锁范围是（4，8]的next-key-lock
select * from t_test where id=8 for update;

#先加(4, 8]的next-lock-key,由于8存在，所以只加id=8的锁
#然后向后访问到不满足查寻条件的范围，即16这条记录，然后加 next-key lock (8, 16]，
#但由于 id = 16 不满足 id < 9，所以会退化成间隙锁，加锁范围变为 (8, 16)。
select * from t_test where id>=8 and id<9 for update;
```

### III.非唯一索引等值查询

向右遍历时最后一个值不满足查询需求时，next-key-lock 退化为间隙锁。 

- 当记录**不存在**时，使用非唯一索引等值查询，**只会加next-key-lock,然后退化成gap lock，只是一把锁**;

```sql
#先加(4, 8]的next-key-lock,由于是非唯一索引等值查询且记录不存在，退化为(4,8)的gap lock
select * from t_test where id=5 for update;
```

- 当记录**存在**时，使用非唯一索引等值查询，**除了加next-key-lock之外，还会加gap lock，也就是两把锁;**

```sql
#先加(4, 8]的next-key-lock,由于是非唯一索引等值查询且记录存在，还会加上(8, 16)的gap lock
select * from t_test where id=8 for update;
```



### IV.非唯一索引范围查寻

**直接加next-key-lock，锁不会发生退化。**

```sql
#先加(4, 8]的next-lock-key,由于8存在，但未非唯一索引，所以锁不发生退化
#然后向后访问到不满足查寻条件的范围为止，即16这条记录，然后加 next-key lock (8, 16]，
#虽然 id = 16 不满足 id < 9，但不会退化成间隙锁，加锁范围为 (8, 16]。
#综上，该条语句的加锁范围为(4,8] U (8, 16]
select * from t_test where id>=8 and id<9 for update;
```

### V.查询条件非索引

**如果查询条件中不含索引或索引失效则会开启全表扫描，将所有记录锁住，即升级为表锁。**

```sql
update t_test set b = 9 where a = 8;# 由于a未加索引，该表所有记录都被锁住。
```

:cry: **如何避免这种情况发生？**

我们可以将 MySQL 里的 `sql_safe_updates` 参数设置为 1，**开启安全更新模式。**

当 sql_safe_updates 设置为 1 时。

**update** 语句**必须满足如下条件之一**才能执行成功：

- 使用 where，并且 where 条件中必须有索引列；
- 使用 limit；
- 同时使用 where 和 limit，此时 where 条件中可以没有索引列；

**delete** 语句必须满足**以下条件**能执行成功：

- 同时使用 where 和 limit，此时 where 条件中可以没有索引列；

如果 where 条件带上了索引列，但是优化器最终扫描选择的是全表，而不是索引的话，我们可以使用 `force index([index_name])` 可以告诉优化器使用哪个索引，以此避免有几率锁全表带来的隐患

## 5、死锁

### I.何时发生死锁？

有两个事务(A和B)，由于**间隙锁是不存在互斥的**，所以事务A和事务B可能同时获取了(20, 30)之间的gap lock，此时事务A想在这个区间内插入一条数据，但由于在此区间存在事务B的gap lock，这时事务A便会阻塞并插入一个**意向插入锁**（***PS：MySQL 加锁时，是先生成锁结构，然后设置锁的状态，如果锁状态是等待状态，并不是意味着事务成功获取到了锁，只有当锁状态为正常状态时，才代表事务成功获取到了锁***），然后锁的状态设置为等待状态，如果此时事务B又在这个区间插入一条记录，但此区间存在事务A的gap lock，**这时死锁发生了。**

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/%E9%94%81/ab%E4%BA%8B%E5%8A%A1%E6%AD%BB%E9%94%81.drawio.png" alt="img" style="zoom:67%;" />

### II、怎么避免死锁？

死锁的四个必要条件：**互斥、占有且等待、不可强占用、循环等待**。只要系统发生死锁，这些条件必然成立，但是只要破坏任意一个条件就死锁就不会成立。

在数据库层面，有两种策略通过「打破循环等待条件」来解除死锁状态：

- **设置事务等待锁的超时时间**。当一个事务的等待时间超过该值后，就对这个事务进行回滚，于是锁就释放了，另一个事务就可以继续执行了。在 InnoDB 中，参数 `innodb_lock_wait_timeout` 是用来设置超时时间的，默认值时 50 秒。

  当发生超时后，就出现下面这个提示：

![图片](https://img-blog.csdnimg.cn/img_convert/c296c1889f0101d335699311b4ef20a8.png)

- **开启主动死锁检测**。主动死锁检测在发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 `innodb_deadlock_detect` 设置为 on，表示开启这个逻辑，默认就开启。

  当检测到死锁后，就会出现下面这个提示：

![图片](https://img-blog.csdnimg.cn/img_convert/f380ef357d065498d8d54ad07f145e09.png)

上面这个两种策略是「当有死锁发生时」的避免方式。

**当InnoDB存储引擎检测到这个错误时，会马上回滚一个事务，应用程序不需手动回滚。**

# 六、日志

## 1、什么是undo log?undo log的作用是什么？

undo log 是一种用于撤销回退的日志。属于**逻辑日志**，所谓逻辑日志,可简单理解为就是sql语句本身，`undo log`主要记录了数据的逻辑变化，比如一条 `INSERT` 语句，对应一条`DELETE` 的 `undo log` ，对于每个 `UPDATE` 语句，对应一条相反的 `UPDATE` 的 `undo log` ，这样在发生错误时，就能回滚到事务之前的数据状态。在事务没提交之前，MySQL 会先记录更新前的数据到 undo log 日志文件里面，当事务回滚时，可以利用 undo log 来进行回滚。

我认为undo log的作用**主要有两点**：

- 当我们不开启事务但执行增删改操作时，mysql会隐式地开启一个事务来执行这条操作，在操作执行完后会自动commit，这是由一个autocommit参数来决定的（默认开启），所以在执行增删改操作时如果我们的mysql服务发生了崩溃，undo log会帮助我们回滚到事务提交前的状态，**保证了事务的原子性**。
- 另外，**undo log 还有一个作用，通过 ReadView + undo log 实现 MVCC（多版本并发控制）**。undo log为每条记录保留历史数据，其中记录有两个字段，事务的id和回滚指针，相邻记录的回滚指针之间连为一条版本链，MySQL 在执行快照读（普通 select 语句）的时候，会根据事务的 Read View 里的信息，顺着 undo log 的版本链找到满足其可见性的记录。**可最大程度上避免快照读下的幻读现象。**

### :hash: undo log 是如何刷盘（持久化到磁盘）的？

undo log 和数据页的刷盘策略是一样的，**都需要通过 redo log 保证持久化。**

buffer pool 中有 undo 页，对 undo 页的修改也都会记录到 redo log。redo log 会每秒刷盘，提交事务时也会刷盘，数据页和 undo 页都是靠这个机制保证持久化的。

### :tea: Undo Log的存储结构

#### **I.回滚段与undo页**

InnoDB对Undo Log的管理采用段的方式，也就是**`回滚段（rollback segment）`**。每个回滚段记录了`1024个Undo Log segment`，而在每个Undo Log segment段中进行**`Undo页`**的申请,**`undo页`,**会缓存在BufferPool中。

在`InnoDB1.1版本`之前（不包括1.1版本），只有一个rollback segment，因此支持同时在线的事务限制为1024。虽然对绝大多数的应用来说都已经够用。

从1.1版本开始InnoDB支持最大`128个rollback segment`，故其支持同时在线的事务限制提高到了`128*1024`。

#### **II.回滚段与事务**

1.每个事务只会使用一个回滚段（rollback segment），一个回滚段在同一时刻可能会服务于多个事务。

2.**当一个事务开始的时候，会制定一个回滚段**，在事务进行的过程中，**当数据被修改时，原始的数据会被复制到回滚段**。

3.在回滚段中，事务会不断填充盘区，直到事务结束或所有的空间被用完。如果当前的盘区不够用，事务会在段中请求扩展下一个盘区，如果所有已分配的盘区都被用完，事务会覆盖最初的盘区或者在回滚段允许的情况下扩展新的盘区来使用。

4.回滚段存在于Undo表空间中，在数据库中可以存在多个Undo表空间，但同一时刻只能使用一个Undo表空间。

5.当事务提交时，InnoDB存储引擎会做以下两件事情：

- 将Undo Log放入列表中，以供之后的**purge线程(清洗、清除)操作**

- 判断Undo Log所在的页是否可以重用(低于3/4可以重用)，**若可以分配给下个事务使用**

#### **III.回滚段中的数据分类**

`未提交的回滚数据(uncommitted undo information)：`该数据所关联的事务并未提交，用于实现读一致性，所以该数据不能被其他事务的数据覆盖。

`已经提交但未过期的回滚数据(committed undo information)：`该数据关联的事务已经提交，但是仍受到undo retention参数的保持时间的影响。

`事务已经提交并过期的数据(expired undo information)：`事务已经提交，而且数据保存时间已经超过undo retention参数指定的时间，属于已经过期的数据。当回滚段满了之后，会优先覆盖"事务已经提交并过期的数据"。

#### **IV.Undo页的重用**

当我们开启一个事务需要写Undo log的时候，就得先去`Undo Log segment`中去找到一个空闲的位置，当有空位的时候，就去申请Undo页，在这个申请到的Undo页中进行Undo Log的写入。我们知道MySQL默认一页的大小是`16k`。

为每一个事务分配一个页，是非常浪费的(除非你的事务非常长)，假设你的应用的TPS(每秒处理的事务数目)为1000，那么1s就需要1000个页，大概需要16M的存储，1分钟大概需要1G的存储。如果照这样下去除非MySQL清理的非常勤快，否则随着时间的推移，磁盘空间会增长的非常快，而且很多空间都是浪费的。

**于是Undo页就被设计的可以重用了**，当事务提交时，并**不会立刻删除**Undo页。因为重用，所以这个Undo页可能混杂着其他事务的Undo Log。Undo Log在commit后，会被放到一个链表中，然后**判断Undo页的使用空间是否小于3/4，如果小于3/4的话，则表示当前的Undo页可以被重用，那么它就不会被回收**，其他事务的Undo Log可以记录在当前Undo页的后面。由于Undo Log是离散的，所以清理对应的磁盘空间时，效率不高。

### :icecream: Undo Log的配置参数

`innodb_max_undo_log_size:`undo日志文件的最大值，默认1GB，初始化大小10M

`innodb_undo_log_truncate:`标识是否开启自动收缩Undo Log表空间的操作

`innodb_undo_tablespaces:`设置独立表空间的个数，默认为0，**标识不开启独立表空间，undo日志保存在ibdata1中**

`innodb_undo_directory:`undo日志存储的目录位置

`innodb_undo_logs:` 回滚的个数 默认128

## 2、Buffer Pool是什么？为什么需要BufferPool？

Mysql中的数据都是存储在磁盘上的，当我们频繁地更新或查询某条记录，就会发生频繁的磁盘IO,因为操作系统访问磁盘数据与访问内存数据相比是相当耗时的，为此，Innodb 存储引擎设计了一个**缓冲池（Buffer Pool）**，将我们读取的操作先缓存起来，这样下次有查询语句命中了这条记录，直接读取缓存中的记录，就不需要从磁盘获取数据了。

### 2.1BufferPool缓存什么？

<img src="C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221012144833556.png" alt="image-20221012144833556" style="zoom:80%;" />

默认条件下BufferPool所占内存为128MB，可以更具需求调整参数，一般调整为可用物理内存大小的60~80.

在 MySQL 启动的时候，**InnoDB 会为 申请一片连续的内存空间，然后按照默认的`16KB`的大小划分出一个个的页， Buffer Pool 中的页就叫做缓存页**。当我们查询一条记录，Buffer  Pool中就会将该条记录所在的页缓存起来。另外Buffer Pool 除了缓存「索引页」和「数据页」，**还包括了 undo 页，插入缓存、自适应哈希索引、锁信息**等等。

为了更好的管理这些在 Buffer Pool 中的缓存页，InnoDB 为每一个缓存页都创建了一个**控制块**，控制块信息包括「缓存页的表空间、页号、缓存页地址、链表节点」等等。

控制块也是占有内存空间的，**它是放在 Buffer Pool 的最前面**，接着才是缓存页，如下图：

<img src="C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221012143026290.png" alt="image-20221012143026290" style="zoom:67%;" />

## ３、redo log

### 3.1、为什么需要redo log

因为在我们读写数据库的时候，并非都通过磁盘IO，innoDB存储引擎中有一个BufferPool来缓存我们需要读取的数据来提高读写效率，而BufferPool又是基于内存存储的，因为内存存储具有易失性，BufferPool中的数据未及时刷新到磁盘中，这就会造成数据的丢失。所以为了防止数据丢失的问题，当有一条记录需要更新的时候，InnoDB 引擎就会先更新内存（同时标记为脏页），然后将本次对这个页的修改以 redo log 的形式记录下来（相当于给更新了但未持久化到磁盘的数据做一个备份），**这个时候更新就算完成了**。这就是**WAL技术**，即**先写日志再写磁盘。**

### 3.2、redo log是什么？

redo log 是物理日志（记录数据页修改后的状态），记录了某个**数据页**做了什么修改，比如**对 XXX 表空间中的 YYY 数据页 ZZZ 偏移量的地方做了AAA 更新**，每当执行一个事务就会产生这样的一条或者多条物理日志。**redo log 是为了防止 Buffer Pool 中的脏页丢失而设计的**

在事务提交时，只要先将 redo log 持久化到磁盘即可，可以不需要等到将缓存在 Buffer Pool 里的脏页数据持久化到磁盘。

当系统崩溃时，虽然脏页数据没有持久化，但是 redo log 已经持久化，接着 MySQL 重启后，可以根据 redo log 的内容，将所有数据恢复到最新的状态。

### 3.3 被修改 Undo 页面，需要记录对应 redo log 吗？

需要，当一个事务更新了一条记录后，首先会先记录undo log, undo log首先会先记录到Buffer Pool中的undo页，不过在修改了undo页后，需要记录对应的redo log。**注意：只要对于Buffer Pool中的页面变脏，这种改动都会记录到redo log中，不单是数据页。**在服务重启时可用redo log重新构造**undo块，表块，索引块等。**这间接保证了undo log数据的持久性。

### 3.4 undo log和redo log的区别？

这两种日志是属于 InnoDB 存储引擎的日志，它们的区别在于：

- redo log 记录了此次事务「**完成后**」的数据状态，记录的是更新**之后**的值；
- undo log 记录了此次事务「**开始前**」的数据状态，记录的是更新**之前**的值；

事务提交之前发生了崩溃，重启后会通过 undo log 回滚事务，事务提交之后发生了崩溃，重启后会通过 redo log **恢复事务**。

我对恢复事务的理解：因为系统崩溃时虽然事务已经提交了，但提交后的数据并未持久化到磁盘中，这并不能真正保证事务的持久性，所以redo log的存在解决了这一问题。

### 3.5、redo log是如何写入磁盘的？

写入 redo log 的方式使用了追加操作， 所以磁盘操作是**顺序写**，而写入数据需要先找到写入位置，然后才写到磁盘，磁盘的「顺序写 」比「随机写」 高效的多，因此 redo log 写入磁盘的开销更小。

### 3.6 redo log 什么时候刷盘？

实际上， 执行一个事务的过程中，产生的 redo log 也不是直接写入磁盘的，因为这样会产生大量的 I/O 操作，而且磁盘的运行速度远慢于内存。

所以，redo log 也有自己的缓存—— **redo log buffer**，每当产生一条 redo log 时，会先写入到 redo log buffer，后续在持久化到磁盘，redo log buffer 默认大小 16 MB，可以通过 `innodb_log_Buffer_size` 参数动态的调整大小，增大它的大小可以让 MySQL 处理「大事务」是不必写入磁盘，进而提升写 IO 性能。



刷盘时机主要有下面几个时机**（这四个时机是必定会刷盘的）：**

1. MySQL 正常关闭时；
2. 当 redo log buffer 中记录的写入量大于 redo log buffer 内存空间的一半时，会触发落盘；
3. 每秒轮询：InnoDB 的后台线程每隔 1 秒，将 redo log buffer 持久化到磁盘。
4. 每次事务提交时都将缓存在 ***redo log buffer*** 里的 redo log 直接持久化到磁盘**（这个策略可由 innodb_flush_log_at_trx_commit 参数控制）**

除此之外，InnoDB 还提供了另外两种策略，由参数 `innodb_flush_log_at_trx_commit` 参数控制，可取的值有：0、1、2，**默认值为 1**，这三个值分别代表的策略如下（**针对上述时机4而言**）：

- 当设置该**参数为 0 时**，表示每次事务提交时 ，还是**将 redo log 留在 redo log buffer 中** ，该模式下在事务提交时不会主动触发写入磁盘的操作。**可能会导致1s的数据丢失。**
- 当设置该**参数为 1 时**，表示每次事务提交时，都**将缓存在 redo log buffer 里的 redo log用`fsync` 直接持久化到磁盘**，这样可以保证 MySQL 异常重启之后数据不会丢失。
- 当设置该**参数为 2 时**，表示每次事务提交时，都只是缓存在 redo log buffer 里的 redo log **写到 redo log 文件，注意写入到「 redo log 文件」并不意味着写入到了磁盘**，因为操作系统的文件系统中有个 Page Cache，Page Cache 是专门用来缓存文件数据的，所以写入「 redo log文件」意味着写入到了操作系统的文件缓存，**所以在系统崩溃的时候是有可能造成数据丢失的。**

### 3.7、redo log写满了怎么办？

默认情况下， InnoDB 存储引擎有 1 个重做日志文件组( redo log Group），「重做日志文件组」由有 **2 个 redo log 文件组成**，这两个 redo 日志的文件名叫 ：`ib_logfile0` 和 `ib_logfile1` 。

![重做日志文件组](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/how_update/%E9%87%8D%E5%81%9A%E6%97%A5%E5%BF%97%E6%96%87%E4%BB%B6%E7%BB%84.drawio.png)

重做日志文件组是以**循环写**的方式工作的，从头开始写，写到末尾就又回到开头，相当于一个环形。所以当一个文件写满以后又会切换到另一个文件。

 **redo log 是为了防止 Buffer Pool 中的脏页丢失而设计的**，那么如果随着系统运行，Buffer Pool 的脏页刷新到了磁盘中，那么 redo log 对应的记录也就没用了，这时候我们擦除这些旧记录，以腾出空间记录新的更新操作。

redo log是以循环写的方式进行的，在两个redo log文件之间存在一个**写指针和一个检查指针**，他们之间的数据一部分是为写满的部分，另一部分就是BufferPool中未刷盘的部分，**如果 write pos 追上了 checkpoint**，就意味着 **redo log 文件满了**，**这时 MySQL 不能再执行新的更新操作，也就是说 MySQL 会被阻塞**，此时**会停下来将 Buffer Pool 中的脏页刷新到磁盘中，然后标记 redo log 哪些记录可以被擦除，接着对旧的 redo log 记录进行擦除，等擦除完旧记录腾出了空间，checkpoint 就会往后移动**，然后 MySQL 恢复正常运行，继续执行新的更新操作。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/how_update/checkpoint.png" alt="img" style="zoom:40%;" />

所以对于并发量大的系统，我们可以根据需求调整redo log的大小，避免redo log频繁被写满。

## 4、bin log

### 4.1为什么需要bin log

binlog 是 MySQL 的 Server 层实现的日志，是逻辑日志，记录sql语句，所有存储引擎都可以使用,它是一个二进制文件，**主要用于主从复制，备份恢复。**

### 4.2 redo log 和 binlog 有什么区别？

**1.适用对象不同：**

- redo log是Innodb存储引擎实现的
- bin log是server层实现的，所有存储引擎都可以使用。

**2.文件格式不同：**

- binlog 有 3 种格式类型，分别是 **STATEMENT（默认格式）**、ROW、 MIXED，区别如下：
  - STATEMENT：每一条修改数据的 **SQL 都会被记录到 binlog** 中（相当于**记录了逻辑操作**，所以针对这种格式， binlog 可以称为逻辑日志），主从复制中 slave 端再根据 SQL 语句重现。但 STATEMENT **有动态函数的问题**，比如你用了 uuid 或者 now 这些函数，你在主库上执行的结果并不是你在从库执行的结果，这种**随时在变的函数会导致复制的数据不一致**；
  - ROW：**记录行数据最终被修改成什么样了（这种格式的日志，就不能称为逻辑日志了）**，**不会出现 STATEMENT 下动态函数的问题**。但 ROW 的**缺点是每行数据的变化结果都会被记录**，比如执行批量 update 语句，更新多少行数据就会产生多少条记录，使 binlog 文件过大，而在 STATEMENT 格式下只会记录一个 update 语句而已；
  - MIXED：包含了 STATEMENT 和 ROW 模式，它会根据不同的情况自动使用 ROW 模式和 STATEMENT 模式；
- redo log 是**物理日志**，记录的是在某个数据页做了什么修改，比如对 XXX 表空间中的 YYY 数据页 ZZZ 偏移量的地方做了AAA 更新；

**3、写入方式不同：**

- binlog 是追加写，写满一个文件，就创建一个新的文件继续写，不会覆盖以前的日志，保存的是**全量的日志。**
- redo log 是循环写，日志空间大小是固定，全部写满就从头开始，保存未被刷入磁盘的脏页日志。

**4、用途不同：**

- binlog 用于**备份恢复、主从复制**；
- redo log 用于掉电等故障恢复，**针对Buffer Pool中未持久化的数据而设计**。

### 4.3 主从复制怎么实现？

主从复制依赖于bin log，主库上每条记录的改动都会以二进制的形式保存到bin log之中，复制过程就是将bin log中的数据复制到从库。

这个过程一般是**异步**的，也就是主库上执行事务操作的线程**不会等待**复制 binlog 的线程同步完成。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/how_update/%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E8%BF%87%E7%A8%8B.drawio.png?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0" alt="MySQL 主从复制过程" style="zoom:80%;" />

**主从复制的三个阶段：**

- MySQL 主库在收到客户端**提交事务的请求之后**，会**先写入 binlog，再提交事务**（在commit那一瞬间会先写binlog），更新存储引擎中的数据，事务提交完成后，返回给客户端“操作成功”的响应。
- 从库会创建一个专门的 I/O 线程，连接主库的 log dump 线程，来接收主库的 binlog 日志，再把 binlog 信息**写入 relay log** 的中继日志里，再返回给主库“复制成功”的响应。
- 从库会创建一个**用于回放 binlog 的线程**，去读 relay log 中继日志，然后回放 binlog 更新存储引擎中的数据，最终实现主从的数据一致性。

### 4.4 从库是不是越多越好？

不是，随着从库的增加，主库要**创建相应数量的log dump线程**来发送binlog给从库，对主库系统资源开销较大，并且还会收到网络带宽的影响。

所以在实际使用中，一个主库一般跟 2～3 个从库（1 套数据库，1 主 2 从 1 备主），这就是一主多从的 MySQL 集群结构。

### 4.5 mysql主从复制主要模型？

- **同步复制**：MySQL 主库提交事务的线程要等待所有从库的复制成功响应，才返回客户端结果。
- **异步复制**（默认模型）：MySQL 主库提交事务的线程并不会等待 binlog 同步到各从库，就返回客户端结果。这种模式一旦主库宕机，数据就会发生丢失。
- **半同步复制**：MySQL **5.7 版本之后**增加的一种复制方式，介于两者之间，事务线程**不用等待所有的从库**复制成功响应，只要一部分复制成功响应回来就行，比如一主二从的集群，只要数据成功复制到任意一个从库上，主库的事务线程就可以返回给客户端。

### 4.6 binlog 何时刷盘？

事务执行过程中，**先把日志写到 binlog cache**（Server 层的 cache），事务**提交的时候**，再把 binlog cache 写到 binlog 文件中。

MySQL 给 binlog cache 分配了一片内存，每个线程一个（每个客户端一个），参数 binlog_cache_size 用于控制单个线程内 binlog cache 所占内存的大小。如果超过了这个参数规定的大小，就要暂存到磁盘。

**在事务提交的时候，执行器把 binlog cache 里的完整事务写入到 binlog 文件中，并清空 binlog cache，虽然每个线程有自己 binlog cache，但是最终都写到同一个 binlog 文件。**

### 4.7 redo log 和 binlog写入磁盘为什么要两阶段提交?

**如果在将 redo log 刷入到磁盘之后， MySQL 突然宕机了，而 binlog 还没有来得及写入**。那么就可能造成主库数据已经更新了但从库数据还未更新。

**如果在将 binlog 刷入到磁盘之后， MySQL 突然宕机了，而 redo log 还没有来得及写入**。那么就可能造成从库数据更新了,但主库数据未更新

两阶段提交就是为了**保证 redo log 和 binlog 数据的安全一致性**。只有在这两个日志文件逻辑上高度一致了才能放心的使用。

**MySQL 为了避免出现两份日志之间的逻辑不一致和Mysql宕机重启后可能造成主从数据不一致的问题，使用了「两阶段提交」来解决**

### 4.8 两阶段提交详解

**两阶段提交把单个事务的提交拆分成了 2 个阶段，分别是「准备（Prepare）阶段」和「提交（Commit）阶段」**

为了保证这两个日志的一致性，MySQL 使用了**内部 XA 事务(分布式事务)**

***什么是分布式事务?***

*对于分布式系统而言，需要保证分布式系统中的数据一致性，**保证数据在子系统中始终保持一致**，避免业务出现问题。分布式系统中对数要么一起成功，要么一起失败，必须是一个整体性的事务*

具体过程:

- **prepare 阶段**：将 XID（内部 XA 事务的 ID） 写入到 redo log，同时将 redo log 对应的事务状态**设置为 prepare**，然后将 redo log 刷新到硬盘；
- **commit 阶段**：把 XID 写入到 binlog，然后将 binlog 刷新到磁盘，接着调用引擎的提交事务接口，将 redo log 状态设置为 **commit**（将事务设置为 commit 状态后，刷入到磁盘 redo log 文件，**所以 commit 状态也是会刷盘的）**

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/how_update/%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4.drawio.png?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0" alt="两阶段提交" style="zoom:67%;" />

**两阶段提交是以 binlog 写成功为事务提交成功的标识**，因为 binlog 写成功了，就意味着能在 binlog 中查找到与 redo log 相同的 XID。这样就保证了主从一致性  

在事务未提交的情况下会发生下图两种情况:

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/how_update/%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4%E5%B4%A9%E6%BA%83%E7%82%B9.drawio.png?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0" alt="时刻 A 与时刻 B" style="zoom:50%;" />

时刻A:redolog 中有XID但binlog中没有,恢复时就会发生回滚;

时刻B:redolog 中有XID并且binlog也有有,恢复时就会提交事务;

### 4.9 两阶段提交存在的问题

- **磁盘 I/O 次数高**：对于“双1”配置，每个事务提交都会进行两次 fsync（刷盘），一次是 redo log 刷盘，另一次是 binlog 刷盘。
- **锁竞争激烈**：两阶段提交虽然能够保证「单事务」两个日志的内容一致，但在「多事务」的情况下，却不能保证两者的提交顺序一致，因此，在两阶段提交的流程基础上，还需要加一个锁来保证提交的原子性，从而保证多事务的情况下，两个日志的提交顺序一致。

### 4.10 组提交

**MySQL 引入了 binlog 组提交（group commit）机制，当有多个事务提交的时候，会将多个 binlog 刷盘操作合并成一个，从而减少磁盘 I/O 的次数**,将**多个事务合并为一组**,大大降低了磁盘IO的系统开销

引入了组提交机制后，prepare 阶段不变，只针对 commit 阶段，将 commit 阶段拆分为三个过程：

- **flush 阶段**：多个事务按进入的顺序将 binlog 从 cache 写入文件（不刷盘）；
- **sync 阶段**：对 binlog 文件做 **fsync** (fsync能确保立即写回磁盘)操作（多个事务的 binlog 合并一次刷盘）；
- **commit 阶段**：各个事务按顺序做 InnoDB commit 操作；

> fsync与sync的区别:
>
> sync函数只是将所有修改过的块缓冲区排入写队列，然后就返回，它并不等待实际写磁盘操作结束。
>
> fsync函数是系统提供的系统调用。只对由文件描述符filedes指定的单一文件起作用，并且等待写磁盘操作结束，然后返回。

上面的**每个阶段都有一个队列**，每个阶段有锁进行保护，因此保证了事务写入的顺序，第一个进入队列的事务会成为 leader，leader领导所在队列的所有事务，全权负责整队的操作，完成后通知队内其他事务操作结束。

<img src="http://keithlan.github.io/image/mysql_innodb_arch/commit_4.png" alt="每个阶段都有一个队列" style="zoom:67%;" />

对每个阶段引入了队列后，锁就只针对每个队列进行保护，不再锁住提交事务的整个过程，可以看的出来，**锁粒度减小了，这样就使得多个阶段可以并发执行，从而提升效率**。

# 七、内存（BufferPool）

## 1、什么是BufferPool？为什么要有BufferPool？有多大？

BufferPool是innodb存储引擎中的一个缓冲池，虽然Mysql中的数据都是存储在磁盘中的，但是不能每次读写都访问磁盘，所以bufferPool它是为提高数据库的读写性能而设计的。

如果数据所在的数据页存储在BufferPool中，那么会直接从BuferPool中读取；

当修改磁盘中页面数据时首先也是修改BufferPool中相应的数据页，此时数据页变成脏页，在适当的时机会刷新回磁盘。

Buffer Pool 是在 MySQL 启动的时候，向操作系统申请的一片连续的内存空间，**默认配置下 Buffer Pool 只有 `128MB` 。**

可以通过调整 `innodb_buffer_pool_size` 参数来设置 Buffer Pool 的大小，**一般建议设置成可用物理内存的 60%~80%。**

## 2、BufferPool中缓存什么？

在 MySQL 启动的时候，**InnoDB 会为 Buffer Pool 申请一片连续的内存空间，然后按照默认的`16KB`的大小划分出一个个的页， Buffer Pool 中的页就叫做缓存页**。

Buffer Pool 除了缓存「索引页」和「数据页」，还包括了 undo 页，插入缓存、自适应哈希索引、锁信息等等。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/innodb/bufferpool%E5%86%85%E5%AE%B9.drawio.png" alt="img" style="zoom: 67%;" />

为了更好的管理这些在 Buffer Pool 中的缓存页，InnoDB 为每一个缓存页都创建了一个**控制块**，控制块信息包括「缓存页的表空间、页号、缓存页地址、链表节点」等等。

控制块也是占有内存空间的，它是**放在 Buffer Pool 的最前面**，接着才是缓存页

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/innodb/%E7%BC%93%E5%AD%98%E9%A1%B5.drawio.png" alt="img" style="zoom:67%;" />

### 2.1、为什么会存在内存碎片？

一个控制块都对应一个缓存页，当空间分配得差不多时，可能剩下的空间无法满足一个控制块和缓存页，所以就产生了内存碎片。

## 3、如何管理BufferPool?

### 3.1如何管理空闲页？

为了能够快速找到空闲的缓存页，可以使用链表结构，将空闲缓存页的「控制块」作为链表的节点，这个链表称为 **Free 链表**（空闲链表）。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/innodb/freelist.drawio.png" alt="img" style="zoom: 50%;" />

Free链表是由一个一个的控制块组成，但除了控制块，还有一个Free链表的头节点，该头节点有一个指向**首控制块的指针和一个指向尾控制块的指针，还有一个空闲页计数器（记录目前有多少个空闲页）。**

### 3.2、如何管理脏页？

为了提高读写效率，被修改的页面不必每次都写回磁盘，而是写回缓存页，由后台线程在何时的时机flush进磁盘，为了管理这些脏页，**Flush链表**横空出世。

Flush链表也是由一个个控制块组成，与Free链表唯一不同的是Flush链表中的控制块指向的是脏页。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/innodb/Flush.drawio.png" alt="img" style="zoom: 50%;" />

​															**Buffer Pool 里有三种页和链表来管理数据**

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/innodb/bufferpoll_page.png" alt="img" style="zoom: 50%;" />

- Free Page（空闲页），表示此页未被使用，位于 Free 链表；
- Clean Page（干净页），表示此页已被使用，但是页面未发生修改，位于LRU 链表。
- Dirty Page（脏页），表示此页「已被使用」且「已经被修改」，其数据和磁盘上的数据已经不一致。当脏页上的数据写入磁盘后，内存数据和磁盘数据一致，那么该页就变成了干净页。**脏页同时存在于 LRU 链表和 Flush 链表。**

### 3.3、如何提高缓存命中率？

**~预读失效问题：**

MySQL 在加载数据页时，会提前把它相邻的数据页一并加载进来，目的是为了减少磁盘 IO。

但是可能这些**被提前加载进来的数据页，并没有被访问**，相当于这个预读是白做了，这个就是**预读失效**。

​	**如何解决？**

要避免预读失效带来影响，最好就是**让预读的页停留在 Buffer Pool 里的时间要尽可能的短，让真正被访问的页才移动到 LRU 链表的头部，从而保证真正被读取的热数据留在 Buffer Pool 里的时间尽可能长**。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/innodb/young%2Bold.png" alt="img" style="zoom:67%;" />

MySQL改进了 LRU 算法，将 LRU 划分了 2 个区域：**old 区域 和 young 区域**。

old 区域占整个 LRU 链表长度的比例可以通过 `innodb_old_blocks_pc` 参数来设置，默认是 37，代表整个 LRU 链表中 young 区域与 old 区域比例是 63:37（约为2：1）。

**预读的页会首先插入old区域头部，当其被访问时会被插入到young区头部，如果它一直不被访问到，那么就会比young区的数据先淘汰。**

**~BufferPool失效问题**

当某一个 SQL 语句**扫描了大量的数据**时，在 Buffer Pool 空间比较有限的情况下，可能会将 **Buffer Pool 里的所有页都替换出去，导致大量热数据被淘汰了**,等这些热数据重新被读取时又会发生大量的磁盘IO，由此造成数据库性能极具下降。

​	:smile_cat:**如何解决？**

MySQL 是这样做的，对于即将进入到 young 区域的控制块增加了一个**停留在 old 区域的时间判断**。

具体是这样做的，在对某个处在 old 区域的缓存页进行第一次访问时，就在它对应的控制块中记录下来这个访问时间：

- 如果后续的访问时间与第一次访问的时间**在某个时间间隔内**，那么**该缓存页就不会被从 old 区域移动到 young 区域的头部**；
- 如果后续的访问时间与第一次访问的时间**不在某个时间间隔内**，那么**该缓存页移动到 young 区域的头部**；

这个间隔时间是由 `innodb_old_blocks_time` 控制的，默认是 1000 ms。

**只有同时满足「被访问」与「在 old 区域停留时间超过 1 秒」两个条件，才会被插入到 young 区域头部**，这样即使大量数据被读入时也只是插入到了old区，young区数据不会收到影响，这样就解决了 Buffer Pool 污染的问题 。

另外，MySQL 针对 young 区域其实做了一个优化，为了防止 young 区域节点频繁移动到头部。**young 区域前面 1/4 被访问不会移动到链表头部，只有后面的 3/4被访问了才会。**

### 3.4、脏页刷新时机

- 当 redo log 日志满了的情况下，会主动触发脏页刷新到磁盘；
- Buffer Pool 空间不足时，需要将一部分数据页淘汰掉，如果淘汰的是脏页，需要先将脏页同步到磁盘；
- MySQL 认为空闲时，后台线程回定期将适量的脏页刷入到磁盘；
- MySQL 正常关闭之前，会把所有的脏页刷入到磁盘；

# 八、面试题补充

## 1.B树和B+树

B-树： B-Tree，B树是一种多叉路衡查找树，相对于二叉树，B树每个节点可以有多个分支，即多叉。以一颗**最大度数（max-degree）**为5`(5阶)`的b-tree为例，那这个B树每个节点最多存储4个key，5个指针：一点是**B-树允许每个节点有更多的子节点**。下图是 B-树的简化图：

![image-20221116165944418](C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221116165944418.png)

- 5阶的B树，每一个节点最多存储4个key，对应5个指针。
- 一旦节点存储的key数量到达5，就会裂变，中间元素向上分裂。
- 在B树中，非叶子节点和叶子节点都会存放数据

B-树有如下特点:

- 所有键值分布在整颗树中（索引值和具体data都在每个节点里）；
- 任何一个关键字出现且只出现在一个结点中；
- 搜索有可能在非叶子结点结束（最好情况O(1)就能找到数据）；
- 在关键字全集内做一次查找,性能逼近二分查找；

**B-树是专门为外部存储器设计的，如磁盘，它对于读取和写入大块数据有良好的性能，所以一般被用在文件系统及数据库中。**

定义只需要知道**B-树允许每个节点有更多的子节点即可（多叉树）**。**子节点数量一般在上千**，具体数量依赖外部存储器的特性。

### 为什么会出现B-树这类数据结构？

传统用来搜索的平衡二叉树有很多，如 AVL 树，红黑树等。这些树在一般情况下查询性能非常好，但**当数据非常大的时候，这些树的高度就会非常大**。当数据量非常大时，内存不够用，大部分数据只能存放在磁盘上，只有需要的数据才加载到内存中。一般而言内存访问的时间约为 50 ns，而磁盘在 10 ms 左右。速度相差了近 5 个数量级，磁盘读取时间远远超过了数据在内存中比较的时间。

如果在读取数据时发生多次磁盘IO，程序大部分时间会阻塞在磁盘 IO 上。为了提高查询效率，减少磁盘 IO 次数，就引出了B-树这类结构。

:walking: **我们从“迎合”磁盘的角度来看看B-树的设计：**

**索引的效率依赖与磁盘 IO 的次数，快速索引需要有效的减少磁盘 IO 次数**，索引的原理其实是不断的缩小查找范围，为了更快，**B-树每次将范围分割为多个区间，区间越多，定位数据越快越精确。**那么如果节点为区间范围，每个节点就较大了。所以新建节点时，直接申请页大小的空间（磁盘存储单位是按 block 分的，一般为 512 Byte。磁盘 IO 一次读取若干个 block，我们称为一页，具体大小和操作系统有关，一般为 4 k，8 k或 16 k），计算机内存分配是按页对齐的，这样就实现了一个节点只需要一次 IO。

<img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNDQ2MDg3LWI2ZDQ3NTQzMDRmYTg5NTU?x-oss-process=image/format,png" alt="img" style="zoom:80%;" />

**B树的每个节点，都是存多个值的**，不像二叉树那样，一个节点就一个值，B树把每个节点都给了一点的范围区间，区间更多的情况下，搜索也就更快了，比如：有1-100个数，二叉树一次只能分两个范围，0-50和51-100，而B树，分成4个范围 1-25， 25-50，51-75，76-100一次就能筛选走四分之三的数据。所以作为多叉树的B树是更快的

### B+ 树

B+树是B-树的变体，也是一种多路搜索树, 它与 B- 树的不同之处在于:

1. 所有**数据存储在叶子节点**，非叶子节点只存储索引
2. 为所有叶子结点增加了一个链指针

![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNDQ2MDg3LTg5MTkwOTQzNzk0M2Q4ODg?x-oss-process=image/format,png)

**非叶子节点并不存储 data，所以一般B+树的叶子节点和非叶子节点大小不同，而B-树的每个节点大小一般是相同的，为一页。**

**MySQL索引数据结构对经典的B+Tree进行了优化。**在原B+Tree的基础上，增加一个**指向相邻叶子节点的链表指针**，就形成了带有顺序指针的B+Tree，提高区间访问的性能，利于排序。

![image-20221116170217066](C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221116170217066.png)

### B-树和B+树的区别

- B+树非叶子节点不存储数据，所有 data 存储在叶节点导致查询时间复杂度固定为 log n。而B-树查询时间复杂度不固定，与 key 在树中的位置有关，最好为O(1)。
- B+树叶节点两两相连可大大增加区间访问性，可使用在范围查询等，而B-树每个节点 key 和 data 在一起，则无法区间查找。
- B+树更适合外部存储。由于非叶子节点不存储数据，所以每个非节点能够存储更多索引数据，能索引的范围更大更精确，B+树比B树更加”矮胖”



### InnoDB主键索引的B+tree高度为多高呢?

![image-20221116170644984](C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221116170644984.png)

假设:

一行数据大小为1k，一页中可以存储16行这样的数据。InnoDB的指针占用6个字节的空间，主键即使为bigint，占用字节数为8。

设一页中存储的索引记录为n：

高度为2：

n * 8 + (n + 1) * 6 = 16*1024 , 算出n约为 1170

1171* 16 = 18736

也就是说，如果树的高度为2，则可以存储 18000 多条记录。

高度为3：

1171 * 1171 * 16 = 21939856

## 2.MySQL逻辑存储结构

![image-20221116165600349](C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221116165600349.png)

![image-20221128203927704](C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221128203927704.png)

- 表空间 : InnoDB存储引擎逻辑结构的最高层，**ibd文件（聚集索引文件）**其实就是表空间文件，在表空间中可以包含多个Segment段。
- 段 : 表空间是由各个段组成的， 常见的段有数据段、索引段、回滚段等。InnoDB中对于段的管理，都是引擎自身完成，不需要人为对其控制，一个段中包含多个区。
- 区 : 区是表空间的单元结构，每个区的大小为1M。 默认情况下， InnoDB存储引擎页大小为16K， 即一个区中一共有64个连续的页。
- 页 : 页是组成区的最小单元，**页也是InnoDB** **存储引擎磁盘管理的最小单元**，每个页的大小默认为 16KB。为了保证页的连续性，InnoDB 存储引擎每次从磁盘申请 4-5 个区。
- 行 : InnoDB 存储引擎是面向行的，也就是说数据是按行进行存放的，在每一行中除了定义表时，所指定的字段以外，还包含两个隐藏字段

## 3.InnoDB 是如何存储数据的？

MySQL 支持多种存储引擎，不同的存储引擎，存储数据的方式也是不同的，我们最常使用的是 InnoDB 存储引擎，所以就跟大家图解下InnoDB 是如何存储数据的。

记录是按照行来存储的，但是数据库的读取并不以「行」为单位，否则一次读取（也就是一次 I/O 操作）只能处理一行数据，效率会非常低。

因此，**InnoDB 的数据是按「数据页」为单位来读写的**，也就是说，当需要读一条记录的时候，并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。

数据库的 I/O 操作的最小单位是页，**InnoDB 数据页的默认大小是 16KB**，意味着数据库每次读写都是以 16KB 为单位的，一次最少从磁盘中读取 16K 的内容到内存中，一次最少把内存中的 16K 内容刷新到磁盘中。

<img src="https://img-blog.csdnimg.cn/img_convert/243b1466779a9e107ae3ef0155604a17.png" alt="图片" style="zoom:50%;" />

<img src="https://img-blog.csdnimg.cn/img_convert/fabd6dadd61a0aa342d7107213955a72.png" alt="图片" style="zoom:67%;" />

在 **File Header 中有两个指针**，分别指向上一个数据页和下一个数据页，连接起来的页相当于一个双向的链表，如下图所示：

![图片](https://img-blog.csdnimg.cn/img_convert/557d17e05ce90f18591c2305871af665.png)

采用链表的结构是让数据页之间不需要是物理上的连续的，而是逻辑上的连续。

数据页的主要作用是存储记录，也就是数据库的数据，所以重点说一下数据页中的 User Records 是怎么组织数据的。

**数据页中的记录按照「主键」顺序组成单向链表**，单向链表的特点就是插入、删除非常方便，但是检索效率不高，最差的情况下需要遍历链表上的所有节点才能完成检索。

因此，数据页中有一个**页目录**，起到记录的索引作用，就像我们书那样，针对书中内容的每个章节设立了一个目录，想看某个章节的时候，可以查看目录，快速找到对应的章节的页数，而数据页中的页目录就是为了能快速找到记录。

那 InnoDB 是如何给记录创建页目录的呢？页目录与记录的关系如下图：

<img src="https://img-blog.csdnimg.cn/img_convert/261011d237bec993821aa198b97ae8ce.png" alt="图片" style="zoom:50%;" />

页目录创建的过程如下：

1. 将所有的记录划分成几个组，这些记录包括最小记录和最大记录，但不包括标记为“已删除”的记录；
2. 每个记录组的最后一条记录就是组内最大的那条记录，并且最后一条记录的头信息中会存储该组一共有多少条记录，作为 n_owned 字段（上图中粉红色字段）
3. 页目录用来存储**每组最后一条记录的地址偏移量**，这些地址偏移量会按照先后顺序存储起来，每组的地址偏移量也被称之为槽（slot），**每个槽相当于指针指向了不同组的最后一个记录**。

从图可以看到，**页目录就是由多个槽组成的，槽相当于分组记录的索引**。然后，因为记录是按照「主键值」从小到大排序的，所以**我们通过槽查找记录时，可以使用二分法快速定位要查询的记录在哪个槽（哪个记录分组），定位到槽后，再遍历槽内的所有记录，找到对应的记录**，无需从最小记录开始遍历整个页中的记录链表。

以上面那张图举个例子，5 个槽的编号分别为 0，1，2，3，4，我想查找主键为 11 的用户记录：

- 先二分得出槽中间位是 (0+4)/2=2 ，2号槽里最大的记录为 8。因为 11 > 8，所以需要从 2 号槽后继续搜索记录；
- 再使用二分搜索出 2 号和 4 槽的中间位是 (2+4)/2= 3，3 号槽里最大的记录为 12。因为 11 < 12，所以主键为 11 的记录在 3 号槽里；
- 这里有个问题，**「槽对应的值都是这个组的主键最大的记录，如何找到组里最小的记录」**？比如槽 3 对应最大主键是 12 的记录，那如何找到最小记录 9。解决办法是：通过槽 3 找到 槽 2 对应的记录，也就是主键为 8 的记录。主键为 8 的记录的下一条记录就是槽 3 当中主键最小的 9 记录，然后开始向下搜索 2 次，定位到主键为 11 的记录，取出该条记录的信息即为我们想要查找的内容。

看到第三步的时候，可能有的同学会疑问，如果某个槽内的记录很多，然后因为记录都是单向链表串起来的，那这样在槽内查找某个记录的时间复杂度不就是 O(n) 了吗？

这点不用担心，**InnoDB 对每个分组中的记录条数都是有规定的**，槽内的记录就只有几条：

- 第一个分组中的记录只能有 1 条记录；
- 最后一个分组中的记录条数范围只能在 1-8 条之间；
- 剩下的分组中记录条数范围只能在 4-8 条之间。

## 4.B+ 树是如何进行查询的？

InnoDB 里的 B+ 树中的**每个节点都是一个数据页**，结构示意图如下：

<img src="https://img-blog.csdnimg.cn/img_convert/7c635d682bd3cdc421bb9eea33a5a413.png" alt="图片" style="zoom:67%;" />

通过上图，我们看出 B+ 树的特点：

- 只有叶子节点（最底层的节点）才存放了数据，非叶子节点（其他上层节）仅用来存放目录项作为索引。
- 非叶子节点分为不同层次，通过分层来降低每一层的搜索量；
- 所有节点按照索引键大小排序，构成一个双向链表，便于范围查询；

我们再看看 B+ 树如何实现快速查找主键为 6 的记录，以上图为例子：

- 从根节点开始，通过二分法快速定位到符合页内范围包含查询值的页，因为查询的主键值为 6，在[1, 7)范围之间，所以到页 30 中查找更详细的目录项；
- 在非叶子节点（页30）中，继续通过二分法快速定位到符合页内范围包含查询值的页，主键值大于 5，所以就到叶子节点（页16）查找记录；
- 接着，在叶子节点（页16）中，通过槽查找记录时，使用二分法快速定位要查询的记录在哪个槽（哪个记录分组），定位到槽后，再遍历槽内的所有记录，找到主键为 6 的记录。

可以看到，在定位记录所在哪一个页时，也是通过二分法快速定位到包含该记录的页。定位到该页后，又会在该页内进行二分法快速定位记录所在的分组（槽号），最后在分组内进行遍历查找。

## 5.为什么要用B+树作为索引而不是B-树作为索引？

-  B 树的每个节点都包含数据（索引+记录），而用户的记录数据的大小很有可能远远超过了索引数据，这就需要花费更多的磁盘 I/O 操作次数来读到「有用的索引数据」。
- 而且，在我们查询位于底层的某个节点（比如 A 记录）过程中，「非 A 记录节点」里的记录数据会从磁盘加载到内存，但是这些记录数据是没用的，我们只是想读取这些节点的索引数据来做比较查询，而「非 A 记录节点」里的记录数据对我们是没用的，这样不仅增多磁盘 I/O 操作次数，也占用内存资源。
- 另外，如果使用 B 树来做范围查询的话，需要使用中序遍历，这会涉及多个节点的磁盘 I/O 问题，从而导致整体速度下降。

## 6.MySQL 默认的存储引擎 InnoDB 为什么采用的是 B+ 作为索引的数据结构？

MySQL 默认的存储引擎 InnoDB 采用的是 B+ 作为索引的数据结构，原因有：

- B+ 树的非叶子节点不存放实际的记录数据，仅存放索引，因此数据量相同的情况下，相比存储即存索引又存记录的 B 树，B+树的非叶子节点可以存放更多的索引，因此 B+ 树可以比 B 树更「矮胖」，**查询底层节点的磁盘 I/O次数会更少。**
- **B+ 树有大量的冗余节点（所有非叶子节点都是冗余索引）**，这些冗余索引让 B+ 树在插入、删除的效率都更高，比如删除根节点的时候，**不会像 B 树那样会发生复杂的树的变化；**
- B+ 树叶子节点之间用链表连接了起来，**有利于范围查询**，而 B 树要实现范围查询，因此只能通过树的遍历来完成范围查询，这会涉及多个节点的磁盘 I/O 操作，范围查询效率不如 B+ 树。

## 7.物理日志和逻辑日志

### （1）物理日志

格式：记录在某个页面的某个偏移量初修改了几个字节的值，具体修改的内容
优点：

1. 日志是幂等的，重复执行改日志不会导致数据发生不一致的问题；
2. 可用于实现宕机前后事务并发控制的一致性。物理日志本身就是存储就是基于不可分隔的更新操作，因此其存储先后顺序就代表了执行器的调度顺序。而且由于很容易判断两个 page 是否是同一个 page，如果不是，完全可以安全并行地并行执行。
3. 日志重放效率高
    缺点：日志量大；

> figure1.典型的物理日志

我们可以看到，更新操作作用于 Page42，将字段 “Kemera” 修改为 “camera”。更新操作对应的日志为：

```css
"Page 42:image at 367,2; before:'ke';after:'ca'"
```

### （2）逻辑日志

格式：记录对于表的操作，类似update语句

优点：日志量小；
 缺点：

1. 恢复时无法保证幂等性；
2. 难以保证宕机前后事务并发控制的一致性。，逻辑日志很难实现一致的事务并发控制。由于逻辑日志难以携带并发执行顺序的信息，当同时有多个事务产生更新操作时，数据库内部会将这些操作调度为串行化序列执行，需要机制来保障每次回放操作的执行顺序与调度产生的顺序一致。
3. 日志重放效率低；

> Figure2.典型的逻辑日志

在上图中，有一张 CameraLingo 表，我们试图纠正 itermID 为 0 的拼写错误，即将 “Kemera” 修改为 “Cemera”。逻辑日志的格式如下：

```dart
CameraLingo:update(0,'Kermera'=>'camera')
```

## 2. 物理日志与逻辑日志的比较

### 2.1 事务并发控制

什么是事务并发控制，为什么需要事务并发控制？

我们需要使用事务并发控制的原因基于以下事实（以 MySQL 为语境解释）：

1. 事务由 SQL 语句构成，每一个 SQL 语句可分解为多个不可分隔的读/写操作；
2. 事务的执行实际上是一连串不可分割读写操作的执行；
3. 事务调度器负责调度不可分割读写操作的执行顺序，它们可能来自于不同事务；
4. 事务并发控制的一个目标就是实现并行化事务；

逻辑日志很难实现一致的事务并发控制。**由于逻辑日志难以携带并发执行顺序的信息**，当同时有多个事务产生更新操作时，数据库内部会将这些操作调度为串行化序列执行，需要机制来保障每次回放操作的执行顺序与调度产生的顺序一致。

另一方面，物理日志本身就是存储就是基于不可分隔的更新操作，因此其存储先后顺序就代表了执行器的调度顺序。而且由于很容易判断两个 page 是否是同一个 page，如果不是，完全可以安全并行地并行执行。

**为了实现宕机前后事务并发控制的一致性，数据库选择使用 Physical Logging 作为其 Redo Log。**

### 2.2 幂等性

幂等性在日志上的语义是：**无论日志回放多少次，最终得到的结果保持一致。**

**物理日志能够做的幂等性，因为其本质是对状态机某一个字段在更新前后状态的记录，无论执行多少次，最终得到的状态总是相同的**。下面是一个例子：

```css
"Page 42:image at 367,2; before:'ke';after:'ca'"
```

**逻辑日志并不能够提供幂等性的语义，因为某一个更新操作本身不具备幂等性**。例如：

```tsx
CameraLingo:update(0,age=>age+1)
```

如果 age 的原值为 0，如果执行一次，那么 age 更新为 1。如果执行两次，那么 age 更新为 2。

当然，如果更新操作本身是幂等的，逻辑日志也可以是幂等的，例如：

```css
CameraLingo:capitaLetter(term)
```

上述逻辑日志无论回放多少次（至少一次），最终得到的结果也就是将首字母大写。

### 2.3 数据量大小

逻辑也不是一无是处，其在日志数据量上占优。

来自客户端的一条更新语句可能会对应多个 page 上的更新，因此逻辑日志与物理日志在日志数量上有巨大的区别。

```css
#查询语句
CameraLingo:capitaLetter(term)
#物理日志
"Page 42:image at 3,1; before:'k';after:'K'"
"Page 42:image at 22,1; before:'a';after:'A'"
....
"Page 42:image at 442,1; before:'b';after:'B'"
#逻辑日志
CameraLingo:capitaLetter(term)
```

别小看！日志数据量大小是特别重要的特性，其对以下过程都有影响：

- 磁盘 I/O 吞吐量；
- 落盘文件大小；
- 网络带宽；

这里的**重点是网络带宽**。分布式系统会通过 primary 副本向 secondary 副本发送日志的方式来进行分布式事务的维护，因此使用物理日志进行传播就不合适。例如，**MySQL 就选择逻辑日志进行维护分布式事务。**

### 2.4 日志重放效率

逻辑日志比物理日志在重放时有着更低的效率，这主要有两个方面的原因：

- 额外的解释步骤：逻辑日志需要额外地解释更新语句、额外查找实际 page 位置；
- 物理日志可以并发进行：当系统判断两个物理日志作用于不同的 page 时，就可以进行完全的并行处理，而逻辑日志通常只能串行执行。