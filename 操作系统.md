# 一、处理器

## 1.处理器的定义

处理器也就是我们常说的 **CPU**，32 位和 64 位 CPU 最主要区别在于一次能计算多少字节数据：

- 32 位 CPU 一次可以计算 4 个字节；
- 64 位 CPU 一次可以计算 8 个字节；

这里的 32 位和 64 位，通常称为 CPU 的位宽。

## 2.CPU中的核心组件

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E7%A8%8B%E5%BA%8F%E6%89%A7%E8%A1%8C/%E5%86%AF%E8%AF%BA%E4%BE%9D%E6%9B%BC%E6%A8%A1%E5%9E%8B.png)

<img src="C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221031172309894.png" alt="image-20221031172309894" style="zoom:80%;" />

### I.运算器：

**定义：**运算器是计算机中进行各**种算术和逻辑运算**操作的部件， 其中**算术逻辑单元**是中央处理核心的部分。

:cake: **组成：**

1. **算术逻辑单元（ALU）**：算术逻辑单元是指能实现多组 算术运算与逻辑运算的**组合逻辑电路**，其是中央处理中的重要组成部分。对二进制信息**的定点算术运算、逻辑运算和各种移位操作**。**算术运算主要包括定点加、减、乘和除运算**。
2. **运算累加器（ACC）**：对于ACC来说，可以将它看成**可变长的累加器**。**累加器是一种寄存器**，用来**储存计算产生的中间结果。**如果没有像累加器这样的寄存器，那么在每次计算 (加法，乘法，移位等等) 后就必须要把结果写回到 内存，也许马上就得读回来。然而存取主存的速度是比从算术逻辑单元到有直接路径的累加器存取更慢。
3. **指令暂存器 (IR)**：其长度为 128 位，其通过操作数来决定实际长度。**储存现在正在被执行的指令**。**暂存器暂存由数据总线或通用寄存的东西。**
4. **描述字寄存器（DR）**：其主要应用于存放与修改描述字中。DR的长度为64位，为了简化数据结构处理，使用描述字发挥重要作用。
5. **B寄存器：**其在指令的修改中发挥重要作用，B 寄存器长度为32位，在修改地址过程中能**保存地址修改量**，主存地址只能用**描述字**进行修改。**指向数组中的第一个元素就是描述字**， 因此，访问数组中的其它元素应当需要用修改量。
6. **状态字（PWS）寄存器：**表示处理器处于用户态`（0）`或核心态`（1）`，**用户态（目态）只能执行非特权指令，核心态（管态）既可执行特权指令，也可执行非特权指令。**

### **II.控制器：**

:scroll: **定义：**控制器是指按照预定顺序改变主电路或控制电路的接线和 改变电路中电阻值来控制电动机的启动、调速、制动与反向的主令装置。其作为“决策机构”，主要任务就是发布命令，发挥着整个**计算机系统操作的协调与指挥作用**。

:satellite: **组成：**

- **指令控制器：**指令控制器是控制器中相当重要的部分，它要完成**取指令、分析指令**等操作，然后**交给执行单元**（ALU或FPU）来执行，同时还要**形成下一条指令的地址**。
- **总线控制器**：总线控制器主要用于控制CPU的内外部总线，包括地址总线、数 据总线、控制总线等等。
- **时序控制器**：时序控制器的作用是为**每条指令按时间顺序提供控制信号**。时序控制器包括**时钟发生器**和**倍频定义单元**，其中**时钟发生器就是CPU的主频**;而**倍频定义单元则定义了 CPU主频是存储器频率（总线频率）的几倍**。
- **中断控制器：**控制各种各样的中断请求，并根据优先级的高低 对中断请求进行排队，逐个交给CPU处理。

![img](https://pic3.zhimg.com/v2-ebcd37bfcea58389c22c881356600bf2_r.jpg)

### III.总线：

<img src="https://picx1.zhimg.com/v2-3301baf5d4c74a32bbaf98530e73fb10_r.jpg?source=1940ef5c" alt="img" style="zoom:50%;" />

<img src="https://bkimg.cdn.bcebos.com/pic/d4628535e5dde71190ef9d1222bbd91b9d16fcfacb80?x-bce-process=image/watermark,image_d2F0ZXIvYmFpa2UxNTA=,g_7,xp_5,yp_5" alt="img" style="zoom:50%;" />

总线就是一堆物理上实际存在导线。按照信息类型，物理意义上的总线**按照逻辑可划分**为：

- 地址总线
- 控制总线
- 数据总线

**CPU总线**像一条主干道，**数据和信号从这主干道上流到各个部件和外部设备**，也从各个部件流回CPU

### IV.寄存器：

寄存器是CPU内部用来存放数据的一些小型存储区域，用来暂时存放参与运算的数据和运算结果。寄存器是CPU内部的元件，包括**通用寄存器**、**段寄存器**和**标志寄存器**、**指令寄存器等**。寄存器拥有非常高的读写速度，所以在寄存器之间的数据传送非常快。这四组寄存器共同构成了一个**基本的指令执行环境**，**一个线程的上下文也基本上就是这些寄存器**，在执行线程切换的时候，就是修改它们的内容。

除了以上四组寄存器，还有**控制寄存器、调试寄存器、描述符寄存器、任务寄存器、MSR寄存器**。

:cake:**通用寄存器：**

这些的寄存器是程序执行代码最最常用，也是最最基础的寄存器，程序执行过程中，绝大部分时间都是在操作这些寄存器来实现指令功能。

- **eax**: 通常用来执行加法，函数调用的返回值一般也放在这里面
- **ebx**: 数据存取
- **ecx**: 通常用来作为计数器，比如for循环
- **edx**: 读写I/O端口时，edx用来存放端口号
- **esp**: 栈顶指针，指向栈的顶部
- **ebp**: 栈底指针，指向栈的底部，通常用`ebp+偏移量`的形式来定位函数存放在栈中的局部变量
- **esi**: 字符串操作时，用于存放数据源的地址
- **edi**: 字符串操作时，用于存放目的地址的，和esi两个经常搭配一起使用，执行字符串的复制等操作



**:sagittarius: 标志寄存器：**

标志寄存器，里面有众多标记位，记录了CPU执行指令过程中的一系列状态，这些标志大都由CPU自动设置和修改：

- CF 进位标志
- PF 奇偶标志
- ZF 零标志
- SF 符号标志
- OF 补码溢出标志
- TF 跟踪标志
- IF 中断标志
- ······
- <img src="https://pic2.zhimg.com/v2-bd49586108dbd2ad2fe8052fdc8fcc3d_r.jpg" alt="img" style="zoom:67%;" />

:ab: **指令寄存器（程序计数器）：**

> **eip**: 指令寄存器可以说是CPU中**最最重要的寄存器**了，它指向了下一条要执行的指令所存放的地址，CPU的工作其实就是不断取出它指向的指令，然后执行这条指令，同时指令寄存器继续指向下面一条指令，如此不断重复，这就是CPU工作的基本日常。

而在漏洞攻击中，**黑客想尽办法费尽心机都想要修改指令寄存器的地址，从而能够执行恶意代码**。

同样的，在x64架构下，32位的eip升级为64位的**rip**寄存器。

:sassy_man: **段寄存器：**

段寄存器与CPU的内存寻址技术紧密相关。

段寄存器有下面6个，前面4个是早期16位模式就引入了，到了32位时代，又新增了fs和gs两个段寄存器。

- **cs**: 代码段
- **ds**: 数据段
- **ss**: 栈段
- **es**: 扩展段
- **fs**: 数据段
- **gs**: 数据段

段寄存器里面存储的内容与CPU当前工作的内存寻址模式紧密相关。

当CPU处于16位实地址模式下时，段寄存器存储段的基地址，寻址时，将段寄存器内容左移4位（乘以16）得到段基地址+段内偏移得到最终的地址。

当CPU工作于保护模式下，段寄存器存储的内容不再是段基址了，此时的段寄存器中存放的是**段选择子**，用来指示当前这个段寄存器“指向”的是哪个分段。

<img src="https://pic1.zhimg.com/v2-375a3713f99e81e4b41cc50afa3c6b54_r.jpg" alt="img" style="zoom:67%;" />

:rabbit: **控制寄存器：**

CPU运行过程中自身的一些关键信息保存在控制寄存器

32位CPU总共有cr0-cr4共5个控制寄存器，64位增加了cr8。他们各自有不同的功能，但都存储了CPU工作时的重要信息：

- **cr0**: 存储了CPU控制标记和工作状态
- **cr1**: 保留未使用
- **cr2**: 页错误出现时保存导致出错的地址
- **cr3**: 存储了当前进程的虚拟地址空间的重要信息——**页目录**地址
- **cr4**: 也存储了CPU工作相关以及当前人任务的一些信息
- **cr8**: 64位新增扩展使用

其中，CR0尤其重要，它包含了太多重要的CPU信息，值得单独关注一下：



<img src="https://pic4.zhimg.com/80/v2-97dd20a6ac6151b7ab8c5ff8d9929b7f_720w.webp" alt="img" style="zoom:67%;" />



一些重要的标记位含义如下：

`PG`: 是否启用内存分页

`AM`: 是否启用内存对齐自动检查

`WP`: 是否开启内存写保护，若开启，对只读页面尝试写入时将触发异常，这一机制常常被用来实现**写时复制**功能

`PE`: 是否开启保护模式

除了CR0，另一个值得关注的寄存器是CR3，它保存了当前进程所使用的虚拟地址空间的页目录地址，可以说是整个虚拟地址翻译中的顶级指挥棒，在进程空间切换的时候，CR3也将同步切换。

### V.CPU缓存（Cache）：

#### :japanese_castle:高速缓存相关概念：

随着时间的推移，CPU 和内存的访问性能相差越来越大，于是就在 CPU 内部嵌入了 CPU Cache（高速缓存），CPU Cache 离 CPU 核心相当近，因此它的访问速度是很快的，于是它充当了 CPU 与内存之间的缓存角色。

高速缓存的硬件结构就是**SRAM（静态随机存取存储器）**，存取时间一般在**0.5~2.5ns，几乎比内存的访问速度快百倍。**并且造价也比内存贵许多

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E7%BC%93%E5%AD%98/%E8%AE%BF%E9%97%AE%E9%80%9F%E5%BA%A6%E8%A1%A8%E6%A0%BC.png" alt="img" style="zoom:67%;" />

CPU Cache 通常分为三级缓存：L1 Cache、L2 Cache、L3 Cache，**级别越低的离 CPU 核心越近，访问速度也快，但是存储容量相对就会越小**。其中，在多核心的 CPU 里，每个核心都有各自的 L1/L2 Cache，而 L3 Cache 是所有核心共享使用的。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84/CPU-Cache.png" alt="img" style="zoom: 50%;" />

**缓存中由指令缓存，数据缓存。**

#### :first_quarter_moon_with_face: CPU缓存的结构：

CPU Cache 是由很多个 Cache Line 组成的，CPU Line 是 CPU 从内存读取数据的基本单位，而 CPU Line 是由各种**标志（Tag）+ 数据块（Data Block）**组成

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7/Cache%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84.png" alt="img" style="zoom: 67%;" />

CPU Cache 的数据是从**内存中读取过来的**，它是以一小块一小块读取数据的，而不是按照单个数组元素来读取数据的，在 CPU Cache 中的，这样一小块一小块的数据，称为 **Cache Line（缓存块）**。

在Linux 系统，用下面这种方式来查看 CPU 的 Cache Line，你可以看我服务器的 L1 Cache Line 大小是 64 字节，也就意味着 **L1 Cache 一次载入数据的大小是 64 字节**。

```bash
[root@gcs101 ~]# cat /sys/devices/system/cpu/cpu0/cache/index0/coherency_line_size 
64
```

有一个 `int array[100]` 的数组，当载入 `array[0]` 时，由于这个数组元素的大小在内存只占 4 字节，不足 64 字节，CPU 就会**顺序加载**数组元素到 `array[15]`，意味着 `array[0]~array[15]` 数组元素都会被缓存在 CPU Cache 中了，因此当下次访问这些数组元素时，会直接从 CPU Cache 读取，而不用再从内存中读取，**大大提高了 CPU 读取数据的性能。**

#### :alien:Cache访问

CPU 访问内存数据时，是一小块一小块数据读取的，具体这一小块数据的大小，取决于 `coherency_line_size` 的值，一般 64 字节。在内存中，这一块的数据我们称为**内存块（Block）**，读取的时候我们要拿到数据所在内存块的地址。

**直接映射：**将**主存的块地址**映射到Cache的Cache Line上。`（块地址） mod （cache中的块数）`

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E7%BC%93%E5%AD%98/%E6%B1%82%E6%A8%A1%E6%98%A0%E5%B0%84%E7%AD%96%E7%95%A5.png" alt="img" style="zoom:67%;" />

使用取模方式映射的话，就会出现多个内存块对应同一个 CPU Cache Line。因此，**为了区别不同的内存块**，在对应的 CPU Cache Line 中我们还会存储一个**组标记（Tag）**。这个组标记会记录当前 CPU Cache Line 中存储的数据对应的内存块，我们可以用这个组标记来区分不同的内存块。

除了组标记信息外，CPU Cache Line 还有两个信息：

- 一个是，从内存加载过来的实际存放**数据（Data）**。
- 另一个是，**有效位（Valid bit）**，它是用来标记对应的 CPU Cache Line 中的数据是否是有效的，如果有效位是 0，无论 CPU Cache Line 中是否有数据，CPU 都会直接访问内存，重新加载数据。

CPU 在从 CPU Cache 读取数据的时候，并不是读取 CPU Cache Line 中的整个数据块，而是读取 CPU 所需要的一个数据片段，这样的数据统称为一个**字（Word）**，在32位CPU下1字=4byte。那要在对应的 CPU Cache Line 中数据块中找到所需的字就需要一个**偏移量（Offset）**。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E7%BC%93%E5%AD%98/%E7%9B%B4%E6%8E%A5Cache%E6%98%A0%E5%B0%84.png" alt="img" style="zoom:50%;" />

因此，一个**内存的访问地址**，包括**组标记（高20位）、CPU Cache Line 索引（2~11位）、偏移量（低2位）**这三种信息，于是 CPU 就能通过这些信息，在 CPU Cache 中找到缓存的数据。而对于 CPU Cache 里的数据结构，则是由**索引 + 有效位 + 组标记 + 数据块**组成。

## 3、CPU指令执行的流程：

​																				**取指 ->译码 -> 执行 -> 写回**

现代大多数 CPU 都使用来**流水线的方式**来执行指令，所谓的流水线就是把一个任务拆分成多个小任务，于是一条指令通常分为 4 个阶段，称为 4 级流水线，如下图：

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E7%A8%8B%E5%BA%8F%E6%89%A7%E8%A1%8C/CPU%E6%8C%87%E4%BB%A4%E5%91%A8%E6%9C%9F.png" alt="img" style="zoom:67%;" />

四个阶段的具体含义：

1. CPU 通过程序计数器读取对应内存地址的指令，这个部分称为 **Fetch（取得指令）**；
2. CPU 对指令进行解码，这个部分称为 **Decode（指令译码）**；
3. CPU 执行指令，这个部分称为 **Execution（执行指令）**；
4. CPU存储器访问，这个部分成为**MEM（访存）**；
5. CPU 将计算结果**存回寄存器**或者将寄存器的值**存入内存**，这个部分称为 **Store（数据回写）**；

上面这 4 个阶段，我们称为**指令周期（Instrution Cycle）**，CPU 的工作就是一个周期接着一个周期，周而复始。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E7%A8%8B%E5%BA%8F%E6%89%A7%E8%A1%8C/%E6%8C%87%E4%BB%A4%E5%91%A8%E6%9C%9F%E5%B7%A5%E4%BD%9C%E7%BB%84%E4%BB%B6.png" alt="img" style="zoom: 67%;" />

- 取指令的阶段，我们的指令是存放在**存储器**里的，实际上，通过程序计数器和指令寄存器取出指令的过程，是由**控制器**操作的；
- 指令的译码过程，也是由**控制器**进行的；控制单元也是由**逻辑门组成的**。比如**控制单元**要**检查操作码是否为LOAD A**。
- 指令执行的过程，无论是进行算术操作、逻辑操作，还是进行数据传输、条件分支操作，都是由**算术逻辑单元**操作的，也就是由**运算器**处理的。但是如果是一个简单的无条件地址跳转，则是直接在**控制器**里面完成的，不需要用到运算器。

## 4、指令的分类

指令从功能角度划分，可以分为 5 大类：

- *数据传输类型的指令*，比如 `store/load` 是寄存器与内存间数据传输的指令，`mov` 是将一个内存地址的数据移动到另一个内存地址的指令；
- *运算类型的指令*，比如加减乘除、位运算、比较大小等等，它们最多只能处理两个寄存器中的数据；
- *跳转类型的指令*，通过修改程序计数器的值来达到跳转执行指令的过程，比如编程中常见的 `if-else`、`switch-case`、函数调用等。
- *信号类型的指令*，比如发生中断的指令 `trap`；
- *闲置类型的指令*，比如指令 `nop`，执行后 CPU 会空转一个周期；

## 5、汇编指令与机器指令

### :dancers: 汇编指令：

每一条汇编指令由两部分构成	**操作码 + 操作数**

- 操作码就是用于操作操作数的，比如add，sub，bne等等
- 操作数就是指出指令执行的操作所需要数据的来源，操作数这个字段中可以放操作数本身，也可以放操作地址，还可以放操作地址的计算方法。

比如MIPS指令集中：

```shell
add $t0, $s1, $s2 #表示将寄存器$s1, $s2中相加存储到$t0寄存器中
```

**add为操作码**

**$s1，$s1是源操作数，$t0是目的操作数**

### :sailboat:机器指令:

指令在计算机内部是以若干或高或低的电信号序列表示的，指令各个部分都可以看成一个独立的数，并将这些数拼接到一起形成指令。

说白了，**机器指令就是一串二进制数，这一串数字中的某些特定位（field）对应汇编指令中的操作码，操作数，并标识了指令类型，位移量，功能码。**

比如MIPS指令集对应的**指令格式**如下：

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E7%A8%8B%E5%BA%8F%E6%89%A7%E8%A1%8C/MIPS%E6%8C%87%E4%BB%A4%E9%9B%86.png" alt="img" style="zoom: 80%;" />

:school: **机器指令的分类：**

- *R 指令*，用在**算术和逻辑操作**，里面有读取和写入数据的寄存器地址。如果是逻辑位移操作，后面还有位移操作的「位移量」，而最后的「功能码」则是再前面的操作码不够的时候，扩展操作码来表示对应的具体指令的；
- *I 指令*，用在**数据传输、条件分支等**。这个类型的指令，就没有了位移量和功能码，也没有了第三个寄存器，而是把这三部分直接合并成了一个地址值或一个常数；
- *J 指令*，用在跳转，高 6 位之外的 26 位都是一个跳转后的地址；

:capital_abcd: **指令中的字段作用：**

- op(操作码)：指令的基本操作，如addi、sub、sw等，它们由一个6位的二进制数标识
- rs：第一个源操作数寄存器
- rt：第二个源操作数寄存器
- rd：存放操作结果的目的寄存器
- shamt：在执行位移运算如 >>/<<等会会用到
- funct：用于指明op字段中操作的特定变式，**在R类型指令中，它和op字段共同确定一条指令**

**每一个寄存器都有一个属于自己的编号，该编号转化为二进制即可与指令字段中的操作数对应上。**

## 6、一个程序从编译到执行的流程

1. 当我们用C语言编写了`hello.c`文件，此时该文件被称为C程序源文件，内容是程序的源代码。
2. **编译**：c源程序要想跑起来，必须先交给**编译器编译**，经编译器编译后的源文件就变成了一种机器能理解的符号形式的**汇编语言程序**
3. **汇编：**汇编语言对于高层次软件（c程序，Java程序等）是一个接口，它简化了编程过程。但机器并不认识汇编指令（**伪指令**），这时就要将汇编语言程序交给汇编器，汇编器会把汇编指令翻译为当前硬件体系下的**机器指令**，这时汇编语言程序就成为了**目标文件**。
4. **链接：**
   -  当源程序很大时，可以将它分为多个源程序，通过编译可以形成多个目标文件(Unix: .o)，这时我们需要用**链接器**把它们连接到一起，生成一个可**执行文件**；
   - 程序中调用了某个**库文件中的子程序**, 需要将这个库文件和该程序生成的目标文件连接到一起，生成一个可执行文件；
   - 一个源程序编译后，得到了存有机器码的目标文件，目标文件中的有些内容还不能直接用来生成可执行文件，链接器程序将这些内容处理为最终的可执行信息. 所以，在只有一个源程序文件，而又不需要调用某个库中的子程序时，也必须用链接器对目标文件进行处理，生成可执行文件.
5. 经以上步骤，可执行的程序已经存储在磁盘中了，我们可以手动运行该程序，最后**加载器**会按一系列步骤执行程序。

### :zap: **C语言的编译过程：**

![img](https://pic1.zhimg.com/v2-eb8b671b8370eb97484b42b25061088c_r.jpg)

**预处理、编译、汇编、链接**

1. **预处理：**gcc -E hello.c -o hello.i  ，`-E表示预编译之后就退出`，hello.c预处理之后的生成`hello.i  文件`
2. **编译：**gcc -S hello.i –o hello.s，hello.i编译后生成`hello.s汇编程序文件`
3. **汇编：**gcc -c hello.s -o hello.o ，hello.s汇编后生成`hello.o目标文件`
4. **链接：**gcc hello.o -o hello，目标文件hello.o生成`hello`可执行文件

:sailboat:1. **编译预处理：**读取c源程序，对其中的**伪指令**（以# 开头的指令）和特殊符号进行处理。**展开头文件、替换宏定义内容、去注释、条件编译**

伪指令主要包括以下四个方面：

1. 宏定义指令：如# define Name TokenString，# undef等。对于前一个伪指令，预编译所要做的是将程序中的所有Name用TokenString替换。
2. 条件编译指令：如# ifdef，# ifndef，# else，# elif，# endif等。这些伪指令的引入使得程序员可以通过定义不同的宏来决定编译程序对哪些代码进行处理。预编译程序将根据有关的文件，将那些不必要的代码过滤掉。
3. 头文件包含指令：如# include “FileName” 或者# include < FileName> 等。采用头文件的目的主要是为了使某些定义可以供多个不同的C源程序使用，预编译程序将把头文件中的定义统统都加入到它所产生的输出文件中，以供编译程序对之进行处理。
4. 特殊符号，预编译程序可以识别一些特殊的符号。例如在源程序中出现的**LINE标识将被解释为当前行号**（十进制数），**FILE则被解释为当前被编译的C源程序的名称**。预编译程序对于在源程序中出现的这些串将用合适的值进行替换。

预编译程序所完成的基本上是对源程序的“替代”工作。经过此种替代，生成一个**没有宏定义、没有条件编译指令、没有特殊符号**的输出文件。这个文件的含义同没有经过预处理的源文件是相同的，但内容有所不同。下一步，此输出文件将作为编译程序的输入而被翻译成为机器指令。

:alarm_clock: **2.编译：**编译程序所要作得工作就是通过词法分析和语法分析，在确认所有的指令都符合语法规则之后，将其翻译成等价的中间代码表示或汇编代码。同时还会对程序进行性能的优化

  将预处理之后的.i 文件生成 .s 汇编文件

:package: **3、汇编：**

汇编过程实际上指把汇编语言代码翻译成目标机器指令的过程。对于被翻译系统处理的每一个C语言源程序，都将最终经过这一处理而得到相应的**目标文件**。目标文件中所存放的也就是与源程序等效的目标的机器语言代码。

目标文件由段组成。通常一个目标文件中至少有两个段：

1. 代码段：该段中所包含的主要是**程序的指令**。该段一般是可读和可执行的，但一般却不可写。
2. 数据段：主要存放程序中要用到的各种**全局变量或静态的数据**。一般数据段都是可读，可写，可执行的。

**:lantern: 4、链接**

链接程序的主要工作就是将有关的目标文件彼此相连接，也即将在一个文件中引用的符号同该符号在另外一个文件中的定义连接起来，使得所有的这些目标文件成为一个能够被操作系统装入执行的统一整体。

根据开发人员指定的同库函数的链接方式的不同，链接处理可分为两种：

- **静态链接**：在这种链接方式下，函数的代码将从其所在的静态链接库中被拷贝到最终的可执行程序中。这样该程序在被执行时这些代码将被装入到该进程的虚拟地址空间中。静态链接库实际上是一个目标文件的集合，其中的每个文件含有库中的一个或者一组相关函数的代码。
- **动态链接**：在程序执行过程中才被链接的库程序

### :dart:可执行文件/被加载运行中的可执行文件

<img src="https://img-blog.csdnimg.cn/img_convert/95738b640707382cd30ba7809b3a7539.png" alt="查看源图像" style="zoom:80%;" />

<img src="C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221101003153832.png" alt="image-20221101003153832" style="zoom: 20%;" />

- ELF (Executable and Linking Format) ：是一种对象文件的格式。
- 程序头表：间连续的文件节映射到运行时内存段
- .init：程序的入口点，程序运行时第一条指令执行的地址
- .text ：里装载了可执行代码；
- .data ：里面装载了被初始化的数据；
- .bss ：里面装载了未被初始化的数据；
- .rodata：只读数据，一般常量，字符串常量等

​																				**以下是.o文件，以及.o文件与可执行文件的对应关系**

------

<img src="https://pic2.zhimg.com/v2-292d68399317ad971e97443f68dec061_r.jpg" alt="查看源图像" style="zoom: 67%;" />

<img src="https://pic1.zhimg.com/v2-d4898822c003c4d79181418289642bb4_r.jpg" alt="查看源图像" style="zoom:80%;" />

 **Section(节)**是在ELF文件里头，用以装载内容数据的最小容器，是**被链接器使用**的。在ELF文件里面，每一个section内都装在了性质属性都一样的内容

当链接完成以后，多个section就变成了segment，**segments** **是被加载器所使用的。加载器会将所需要的** **segment** **加载到内存空间中运行。**重要的三个segment是：**代码段、数据段和堆栈段。**

## 7.衡量CPU指令的性能

### I.几个重要概念

1. **吞吐量：**指系统在单位时间内处理请求的数量 ；从用户观点看，它是**评价计算机系统性能的综合参数**
2. **响应时间：**指从用户向计算机发送一个请求，到系统对该请求做出响应并获得所需结构的等待时间。
3. **时钟周期：**通常为节拍脉冲或T周期，即主频的倒数，它是**CPU中最小的时间单位，每个动作至少需要1个时钟周期**。主频(CPU时钟频率)。机器内部主时钟的频率，是衡量机器速度的重要参数。
4. **主频(CPU时钟频率)：**机器内部主时钟的频率，是衡量机器速度的重要参数，大小为**CPU时钟周期的倒数**
5. **CPI：**表示执行每条指令所需的时钟周期数的平均值。
6. **CPU执行时间：**也称为**CPU时间**，执行某一任务在CPU上所花费的时间，它只表示在CPU上花费的时间，而**不包括等待I/O或行其它程序的时间**。（注意：用户所感受到的是响应时间而不是CPU时间）CPU时间还可以进一步划分位**用户CPU时间**和**系统CPU时间**，前者是用户程序执行的时间，后者是OS为用户服务花费的用去的CPU时间。
7. **CPU时钟周期数：**一个程序的所有指令执行完所需的时钟周期数量
8. **MIPS(Million Instructions Per Second)：**每秒执行多少百万条指令。

### **II.经典的CPU性能公式：**

**CPU时间 = 指令数 * CPI * 时钟周期时间**

**MIPS = 指令数 / (CPU执行时间 * 10^6) = 主频 / (CPI * 10^6)**

### III.CPU性能的决定性因素：

CPU的性能取决于三个要素:主频、CPI 、指令条数。

衡量CPU运算速度的指标有很多，不能以单独的某个指标来判断CPU的好坏。CPU的主频，即CPU内核工作的时钟频率。CPU的主频表示CPU内数字脉冲信号振荡的速度，主频和实际的运算速度存在一定的关系，但目前还没有一个确定的公式能够定量两者的数值关系，因为CPU的运算速度还要看CPU的流水线的各方面的性能指标（架构、缓存、指令集、CPU的位数、 Cache大小等）。由于主频并不直接代表运算速度，因此在一定情况下很可能会出现主频较高的CPU实际运算速度较低的现象

## 8、CPU缓存一致性（重点）

### I.写策略：

数据不光是只有读操作，还有写操作，那么如果数据写入 Cache 之后，内存与 Cache 相对应的数据将会不同，这种情况下 Cache 和内存数据都不一致了，于是我们肯定是要把 Cache 中的数据同步到内存里的。

那在什么时机才把 Cache 中的数据写回到内存呢？为了应对这个问题，下面介绍两种针对写入数据的方法：

- 写直达（*Write Through*）：保持内存与 Cache 一致性最简单的方式是，**把数据同时写入内存和 Cache 中**，这种方法称为**写直达（Write Through）**。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7/%E5%86%99%E7%9B%B4%E8%BE%BE.png" alt="img" style="zoom:67%;" />

- 写回（*Write Back*）：，**当发生写操作时，新的数据仅仅被写入 Cache Block 里，只有当修改过的 Cache Block「被替换」时才需要写到内存中**，减少了数据写回内存的频率，这样便可以提高系统的性能。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7/%E5%86%99%E5%9B%9E1.png" alt="img" style="zoom:67%;" />

### II.缓存一致性问题：

现在 CPU 都是多核的，由于 L1/L2 Cache 是多个核心各自独有的，那么会带来多核心的**缓存一致性（Cache Coherence）** 的问题，如果不能保证缓存一致性的问题，就可能造成结果错误。

比如CPU1对共享变量i做i++操作，完成后将结果写入自己的L1/L2缓存，然后将Cache中对应的Block标记为脏的，由于数据并未被写回内存，此时CPU2来读共享变量i，得到的值就是错误的。**这个就是所谓的缓存一致性问题，A 号核心和 B 号核心的缓存，在这个时候是不一致，从而会导致执行结果的错误。**

**要解决这一问题**，就需要一种机制，来同步两个不同核心里面的缓存数据。要实现的这个机制的话，要保证做到下面这 2 点：

- 第一点，某个 CPU 核心里的 Cache 数据更新时，必须要传播到其他核心的 Cache，这个称为**写传播（Write Propagation）**；
- 第二点，某个 CPU 核心里对数据的操作顺序，必须在其他核心看起来顺序是一样的，这个称为**事务的串行化（Transaction Serialization）**。

**写传播：**当某个核心在 Cache 更新了数据，就需要同步到其他核心的 Cache 里，即不同CPU能立刻感知到数据的变化。

**串行化：**仅保证数据变化的可见性还不能保证CPU缓存的一致性，同时还要保证不同CPU都能看到**相同顺序的数据变化**，这就是所谓的串行化。

写传播的原则就是当某个 CPU 核心更新了 Cache 中的数据，要把该事件广播通知到其他核心。最常见实现的方式是**总线嗅探（Bus Snooping）**。

总线嗅探只是保证了某个 CPU 核心的 Cache 更新数据这个事件能被其他 CPU 核心知道，但是并不能保证事务串行化。

于是，有一个协议**基于总线嗅探机制实现了事务串行化**，也用状态机机制降低了总线带宽压力，这个协议就是 MESI 协议，这个协议就做到了 CPU **缓存一致性（MESI）**。

### III. 基于总线嗅探的MESI 协议：

MESI 协议其实是 4 个状态单词的开头字母缩写，分别是：

- *Modified*，已修改
- *Exclusive*，独占
- *Shared*，共享
- *Invalidated*，已失效

**标记 Cache Line 四个不同的状态。**

1. 「已修改」状态：就是前面提到的**脏标记**，代表该 Cache Block 上的数据**已经被更新过，但是还没有写到内存里**
2. 「已失效」状态：表示的是这个 Cache Block 里的数据已经失效了，**不可以读取该状态的数据**。
3. 「独占」和「共享」状态都代表 Cache Block 里的数据是干净的，也就是说，这个时候 **Cache Block 里的数据和内存里面的数据是一致**的。
4. 「独占」和「共享」的差别在于，独占状态的时候，**数据只存储在一个 CPU 核心的 Cache 里**，而其他 CPU 核心的 Cache 没有该数据。
5. 如果要向独占的 Cache 写数据，就可以直接自由地写入，而不需要通知其他 CPU 核心，因为只有你这有这个数据，就不存在缓存一致性的问题了，
6. 在「独占」状态下的数据，如果有其他核心从内存读取了相同的数据到各自的 Cache ，那么这个时候，独占状态下的数据就会变成共享状态。
7. 「共享」状态代表着相同的数据在多个 CPU 核心的 Cache 里都有，所以当我们要更新 Cache 里面的数据的时候，不能直接修改，而是要先向所有的其他 CPU 核心广播一个请求，要求先把其他核心的 Cache 中对应的 Cache Line 标记为「无效」状态，然后再更新当前 Cache 里面的数据。

整个 MESI 的状态可以用一个有限状态机来表示它的状态流转。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7/MESI%E5%8D%8F%E8%AE%AE.png" alt="img" style="zoom:67%;" />

## 9、CPU中断（异常）：

![image-20221121204658677](C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221121204658677.png)

![image-20221121204711672](C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221121204711672.png)

### I.定义：

中断是系统用来响应硬件设备请求的一种机制，操作系统收到硬件的中断请求，会打断正在执行的进程，然后调用内核中的**中断处理程序**来响应请求。**中断是CPU从用户态到核心态切换的唯一途径。**

### II.中断事件举例：

- I/O设备请求：外部中断
- 系统调用：内部中断，系统调用是由中断来实现的
- 算术溢出：内部中断
- 使用未定义指令：内部中断
- 硬件故障：内部或外部中断

### III.中断事件的处理：

中断（异常）发生时处理器必须进行的基本操作是：

- 在异常程序计数器（`EPC`）中保存出错指令的地址，并将操作权交给操作系统的特定地址（中断处理程序）。
- 当OS处理完异常处理后，OS可以**选择终止程序，也可以继续执行程序**，若要执行程序，由EPC来决定程序重新执行的位置
- 为了得知异常类型，通常使用**向量中断**，所谓向量中断就是**由异常原因决定中断转移地址（调用中断处理程序的入口地址）的中断。**
- 为了实现异常处理，我们需要两个寄存器来保证：
  - EPC：32位寄存器，保存发生异常的指令地址
  - Cause：记录异常原因寄存器，在MIPS体系结构中它是32位的，虽然一些位还未被用到。

### IV.中断的实质：

- 交换指令执行的地址
- 交换CPU的状态
- 异常线程保护和恢复
- 参数传递（通信）

### V.软硬中断：

#### 硬中断：

硬中断是**硬件产生信号**，通过某些芯片变成中断信号，再引到CPU的中断信号管脚上，从而引发的中断，这类中断可通过操作CPU的中断屏蔽寄存器、优先级寄存器等来设置其是否被允许中断以及该中断在全部硬中断中的优先级等等。**外部设备**（例如磁盘、网卡、键盘）等产生硬中断，用来通知操作系统外设状态的变化

> **时钟中断：** 一种硬中断，用于定期打断 CPU 执行的线程，以便切换给其他线程以得到执行机会。

**硬中断**的处理流程如下：

1. **外设** 将中断请求发送给中断控制器；
2. **中断控制器** 根据中断优先级，有序地将中断传递给 CPU；
3. **CPU** 终止执行当前程序流，将 CPU 所有寄存器的数值保存到栈中；
4. **CPU** 根据中断向量，从中断向量表中查找中断处理程序的入口地址，执行中断处理程序；
5. **CPU** 恢复寄存器中的数值，返回原程序流停止位置继续执行。

#### **软中断**

**软中断并不是真正的中断**，软中断是由当前**正在运行的进程或操作系统**引发的中断，它的**优先级低于硬中断**，如果硬中断的处理时间较长，可能会导致软中断滞后，产生的现象就是程序假死，因为现代的**多线程操作系统本质上就是通过定时器中断来实现多线程的**，如果计算机的配置很低，硬盘的读写操作引发的中断无法及时处理完，你就会看到，硬盘灯一直亮着，而程序界面卡住不动，过了很久才会看到鼠标指针动一下。

**软中断通过软件的方式**模拟了硬中断的处理过程，处于**硬中断的下半部分**：

1. **CPU** 终止执行当前程序流，将 CPU 所有寄存器的数值保存到栈中；
2. **CPU** 根据中断向量，从中断向量表中查找中断处理程序的入口地址，执行中断处理程序；
3. **CPU** 恢复寄存器中的数值，返回原程序流停止位置继续执行。

#### 软中断和硬中断的结合：

软中断是利用硬件中断的概念，**用软件方式进行模拟**，实现宏观上的异步执行效果。很多情况下，软中断和"信号"有些类似，同时，软中断又是和硬中断相对应的，**"硬中断是外部设备对CPU的中断"，"软中断通常是硬中断服务程序对内核的中断"**，"信号则是由内核（或其他进程）对某个进程的中断"。

Linux 系统**为了解决中断处理程序执行过长和中断丢失的问题，将中断过程分成了两个阶段，分别是「上半部和下半部分」，可以认为软中断为硬中断服务**。

- 中断上半部：处理硬中断，期间暂时屏蔽中断，快速执行一些耗时短的操作，然后将需要执行的中断处理函数地址放入Tasklet任务列表中
- 中断下半部：内核发起内核线程软中断daemon，执行硬中断阶段指定的中断处理函数，**期间内核能够正常接收硬中断**

这样的好处是，内核只需要在接收中断请求时屏蔽其他中断，然后自行进行中断处理函数的工作，此时又能继续接收新的中断请求了。

:jack_o_lantern:**举例：**

网卡收到网络包后，通过 DMA 方式将接收到的数据写入内存，接着会通过**硬件中断**通知内核有新的数据到了，于是内核就会调用对应的中断处理程序来处理该事件，这个事件的处理也是会分成上半部和下半部。

上部分要做的事情很少，会先**禁止网卡中断，避免频繁硬中断**，而降低内核的工作效率。接着，内核会触发一个**软中断**，把一些处理比较耗时且复杂的事情，交给「软中断处理程序」去做，也就是中断的下半部

所以，中断处理程序的上部分和下半部可以理解为：

- **上半部直接处理硬件请求，将中断请求转发给中断控制器，控制器再根据中断的优先级转发给CPU，CPU中止进程保存现场，触发调用中断处理程序，这个过程也就是硬中断**，主要是负责耗时短的工作，特点是快速执行；
- **下半部是由内核触发，执行中断处理程序，也就说软中断，软中断处理程序是由内核线程来执行的**，主要是负责上半部未完成的工作，通常都是耗时比较长的事情，特点是延迟执行；

#### 软中断和硬中断的关系：

软中断并不是真正的中断，它的存在是因为中断处理流程不宜过长，因此中断的处理程序实际仅对内核做简单通知，处理过程由内核来做，内核做中断处理的时候真正的中断（硬中断）已经结束了。接下来的中断处理就是软中断来做。所以**真正的中断被称为”硬“，实际的中断处理被称为“软”。**

### VI.软中断与软件中断：

- **软中断**(softIRQ)，即中断下半部机制。ISR运行时间不易过长，linux将中断中的一部分逻辑推后执行，这就是softIRQ，它完全由软件实现；**软中断并不是真正的中断，它是软件对中断的模拟。**
- **软件中断**(Software Interrupt)：是指因**硬件出错**（如突然掉电、奇偶校验错等）或**运算出错**（除数为零、运算溢出、单步中断等）或**系统调用**所引起的中断。这种中断并非来自外部设备而是**处理器异常或是指令等内部自发产生的中断**。在32位x86中，为了实现linux用户态到内核态的切换，linux使用软中断指令“int 0x80”来触发异常，切换CPU特权级，实现系统调用。软件中断是真正的中断，它不来自外设，属于硬件中断中的内部中断。
- **硬件中断：**一般是指由计算机**外设发出的中断请求**，如：键盘中断、打印机中断、定时器中断等。外部中断是可以屏蔽的中断，也就是说，利用中断控制器可以屏蔽这些外部设备的中断请求。

### IV.**查看软中断次数**

用**vmstat**命令可以看到软中断的次数,其中si是软中断次数

```bash
[root@gcs101 ~]# vmstat 
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 2  0 437760 124176      4 654176    5   10   116    39  110  205  2  1 97  0  0
```

## 10、CPU上下文切换：

大多数操作系统都是多任务，通常支持大于 CPU 数量的任务同时运行。实际上，这些任务并不是同时运行的，只是因为系统在很短的时间内，让各个任务分别在 CPU 运行，于是就造成同时运行的错觉。所以，在每个认任务在即将执行之前，操作系统需要事先帮 CPU 设置好 **CPU 寄存器和程序计数器**。

CPU 寄存器和程序计数是 CPU 在运行任何任务前，**所必须依赖的环境**，这些环境就叫做 **CPU 上下文**。

CPU 上下文切换就是先把**前一个任务的 CPU 上下文（CPU 寄存器和程序计数器）保存起来**，然后加载新任务的上下文到这些寄存器和程序计数器，最后再跳转到程序计数器所指的新位置，运行新任务。系统内核会存储保持下来的上下文信息，当此任务再次被分配给 CPU 运行时，CPU 会重新加载这些上下文，这样就能保证任务原来的状态不受影响，让任务看起来还是连续运行。

上面说到所谓的「任务」，主要包含进程、线程和中断。所以，可以根据任务的不同，把 CPU 上下文切换分成：

- **进程上下文切换**
- **线程上下文切换**
- **中断上下文切换**

## 11.处理机调度

![img](https://img-blog.csdnimg.cn/20200405110107763.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70)

**高级调度（面向作业）（创建、分配资源）：**

<img src="https://img-blog.csdnimg.cn/20200405104148359.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:50%;" />

**中级调度（面向内存）：（挂起、就绪）**

<img src="https://img-blog.csdnimg.cn/20200405104747400.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom: 50%;" />

**低级调度（面向进程）： ** **（就绪、运行）**

<img src="https://img-blog.csdnimg.cn/20200405105806738.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:50%;" />

## 12、系统调用

### I.定义：

系统调用是操作系统提供给应用程序的的接口，应用程序可以通过系统调用请求操作系统服务，**当进行系统调用的时候处理器会由用户态切换到核心态。**

### II.系统调用的实质

系统调用相关功能涉及系统资源管理，因此必须执行某些特权指令才能实现，所以系统调用发生后必须陷入内核。调用系统调用函数时实际上是执行了**陷入指令（访管指令/trap指令）**，此时会产生内中断，使系统陷入内核。

### III.为什么系统调用开销大？

使用软件中断触发的系统调用**会打断指令执行流水线，并且需要保存堆栈和返回地址等信息，还要在中断描述表中查找系统调用的响应函数，以及做一些访问权限的检查**虽然多数的操作系统不会使用 INT 0x80 触发系统调用，但是在一些特殊场景下，我们仍然需要利用这一古老的技术；

使用汇编指令 SYSCALL / SYSENTER 执行系统调用是今天最常见的方法，作为专门为系统调用打造的指令，它们可以省去一些不必要的步骤，降低系统调用的开销；

**以下是简单的printf("hello world")，其间进行了几十次系统调用**

```bash
[root@gcs100 c_dir]# strace ./hello
execve("./hello", ["./hello"], 0x7ffca6910560 /* 48 vars */) = 0
brk(NULL)                               = 0x20a1000
mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f55497e2000
access("/etc/ld.so.preload", R_OK)      = -1 ENOENT (没有那个文件或目录)
open("/etc/ld.so.cache", O_RDONLY|O_CLOEXEC) = 3
fstat(3, {st_mode=S_IFREG|0644, st_size=90460, ...}) = 0
mmap(NULL, 90460, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7f55497cb000
close(3)                                = 0
open("/lib64/libc.so.6", O_RDONLY|O_CLOEXEC) = 3
read(3, "\177ELF\2\1\1\3\0\0\0\0\0\0\0\0\3\0>\0\1\0\0\0`&\2\0\0\0\0\0"..., 832) = 832
fstat(3, {st_mode=S_IFREG|0755, st_size=2156592, ...}) = 0
mmap(NULL, 3985920, PROT_READ|PROT_EXEC, MAP_PRIVATE|MAP_DENYWRITE, 3, 0) = 0x7f55491f4000
mprotect(0x7f55493b8000, 2093056, PROT_NONE) = 0
mmap(0x7f55495b7000, 24576, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_DENYWRITE, 3, 0x1c3000) = 0x7f55495b7000
mmap(0x7f55495bd000, 16896, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_ANONYMOUS, -1, 0) = 0x7f55495bd000
close(3)                                = 0
mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f55497ca000
mmap(NULL, 8192, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f55497c8000
arch_prctl(ARCH_SET_FS, 0x7f55497c8740) = 0
access("/etc/sysconfig/strcasecmp-nonascii", F_OK) = -1 ENOENT (没有那个文件或目录)
access("/etc/sysconfig/strcasecmp-nonascii", F_OK) = -1 ENOENT (没有那个文件或目录)
mprotect(0x7f55495b7000, 16384, PROT_READ) = 0
mprotect(0x600000, 4096, PROT_READ)     = 0
mprotect(0x7f55497e3000, 4096, PROT_READ) = 0
munmap(0x7f55497cb000, 90460)           = 0
fstat(1, {st_mode=S_IFCHR|0620, st_rdev=makedev(136, 1), ...}) = 0
mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f55497e1000
write(1, "hello world!\n", 13hello world!
)          = 13
exit_group(0)                           = ?
+++ exited with 0 +++
```

```text
用户态–>系统调用–>内核态–>返回用户态
```

<img src="https://pic4.zhimg.com/v2-7f5eb3c97342632b5975493cbdd45b9f_r.jpg" alt="img" style="zoom:67%;" />

# 二、进程与线程

## 1.进程基础

### I.定义：

我们编写的代码只是一个存储在硬盘的静态文件，通过预处理、编译、汇编、链接后就会生成二进制可执行文件，当这个这个可执行文件被装入加载器后，它会被装载到内存中，接着 CPU 会执行程序中的每一条指令，那么这个**运行中的程序，就被称为「进程」（Process）**。

### II.进程的组成：

#### :alembic: 基本组成：

进程是由**代码段、数据段、PCB**三个部分组成的。

<img src="https://img-blog.csdnimg.cn/20200309183432616.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:50%;" />

其中最重要的就是**进程控制块PCB**（Process Control Block）

#### :scorpion: **PCB 具体包含什么信息呢？**

**进程描述信息：**

- 进程标识符：标识各个进程，每个进程都有一个并且唯一的标识符；
- 用户标识符：进程归属的用户，用户标识符主要为共享和保护服务；

**进程控制和管理信息：**

- 进程当前状态，如 new、ready、running、waiting 或 blocked 等；
- 进程优先级：进程抢占 CPU 时的优先级；

**资源分配清单：**

- 有关内存地址空间或虚拟地址空间的信息，所打开文件的列表和所使用的 I/O 设备信息。

**CPU 相关信息：**

- CPU 中各个寄存器的值，当进程被切换时，CPU 的状态信息都会被保存在相应的 PCB 中，以便进程重新执行时，能从断点处继续执行。

<img src="https://img-blog.csdnimg.cn/20200309191134423.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:80%;" />

**PCB数据结构：**

```c
struct task_struct{
...
unsigned short uid;
int pid;
int processor;
...
volatile long state;
long prority;
unsighed long rt_prority;
long counter;
unsigned long flags;
unsigned long policy;
...
Struct task_struct *next_task, *prev_task;
Struct task_struct *next_run,*prev_run;
Struct task_struct *p_opptr,*p_pptr,*p_cptr,*pysptr,*p_ptr;
...
};
```



#### :panda_face:**PCB是如何组织的？**

**1)链接方式：**按照进程的状态将进程组成不同的队列，把具有**相同状态的进程链在一起，组成各种队列**。比如：

- 将所有处于就绪状态的进程链在一起，称为**就绪队列**；
- 把所有因等待某事件而处于等待状态的进程链在一起就组成各种**阻塞队列**；
- 另外，对于运行队列在单核 CPU 系统中则只有一个运行指针了，因为单核 CPU 在某个时间，只能运行一个程序。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/12-PCB%E7%8A%B6%E6%80%81%E9%93%BE%E8%A1%A8%E7%BB%84%E7%BB%87.jpg" alt="就绪队列和阻塞队列" style="zoom:50%;" />

**2）索引方式：**将**同一状态的进程组织在一个索引表**中，索引表项指向相应的 PCB，不同状态对应不同的索引表。**操作系统持有各个索引表的指针。**

<img src="https://img-blog.csdnimg.cn/20200309193441596.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:50%;" />

### III.进程的状态：

一个进程并不是自始至终连续不停地运行的，它与并发执行中的其他进程的执行是相互制约的。**在一个进程的活动期间至少具备三种基本状态，即运行状态、就绪状态、阻塞状态。**

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/7-%E8%BF%9B%E7%A8%8B%E4%B8%89%E4%B8%AA%E5%9F%BA%E6%9C%AC%E7%8A%B6%E6%80%81.jpg" alt="进程的三种基本状态" style="zoom: 67%;" />

- 运行状态（*Running*）：该时刻进程占用 CPU；
- 就绪状态（*Ready*）：具备运行条件，但还没有空闲的CPU，暂时不能运行。
- 阻塞状态（*Blocked*）：该进程正在**等待某一事件发生**（如等待输入/输出操作的完成，等待操作系统分配打印机资源，等待读取磁盘文件的返回）而暂时停止运行，这时，即使给它CPU控制权，它也无法运行；

进程还有另外两个基本状态：

- 创建状态（*new*）：进程正在被创建时的状态，**操作系统为进程分配资源，初始化PCB；**
- 结束状态（*Exit*）：进程正在从系统中撤销的状态，**系统回收进程资源，释放所占有的PCB；**基于UNIX系统，主程序执行完后返回0，在父进程中进行wait()调用，等待子进程完成，告诉OS可以清理与这个进程相关的所有数据结构，如果没有清理，该子进程将变为僵死进程。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/8-%E8%BF%9B%E7%A8%8B%E4%BA%94%E4%B8%AA%E7%8A%B6%E6%80%81.jpg" alt="进程五种状态的变迁" style="zoom:50%;" />

![img](https://img-blog.csdnimg.cn/20200312152430501.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70)

### IV.进程的控制：

#### **:airplane:原语（补）：**

- 原语是一种特殊的程序，处于OS最底层，是最接近硬件的部分
- 这种程序运行具有原子性，程序运行时间短，调用频繁
- 在我们用户进程进行系统调用时，如fork()，pthread_create()创建进程和线程时，其底层都是OS底层原语支持的，这些函数相当于对多条原语的封装。

#### :alarm_clock: **进程控制定义：**

进程的**创建、终止、阻塞、唤醒**的过程，这些过程就是**进程的控制**。进程状态的转化是通过**操作系统原语**实现的。

**01 创建进程（创建原语）**

操作系统允许一个进程创建另一个进程，而且允许子进程继承父进程所拥有的资源。

创建进程的过程如下：

- 申请一个空白的 PCB，并向 PCB 中填写一些控制和管理进程的信息，比如进程的唯一标识等；
- 为该进程分配运行时所必需的资源，比如内存资源；
- 将 PCB 插入到就绪队列，等待被调度运行；

**02 终止进程（终止原语）**

进程可以有 3 种终止方式：正常结束、异常结束以及外界干预（信号 `kill` 掉）。

当子进程被终止时，其在父进程处继承的资源应当还给父进程。而当父进程被终止时，该父进程的子进程就变为孤儿进程，会被 1 号进程收养，并由 1 号进程对它们完成状态收集工作。

终止进程的过程如下：

- 查找需要终止的进程的 PCB；
- 如果处于执行状态，则立即终止该进程的执行，然后将 CPU 资源分配给其他进程；
- 如果其还有子进程，则应将该进程的子进程交给 1 号进程接管；
- 将该进程所拥有的全部资源都归还给操作系统；
- 将其从 PCB 所在队列中删除；

**03 阻塞进程（阻塞原语）**

当进程需要等待某一事件完成时，它可以调用阻塞语句把自己阻塞等待。而一旦被阻塞等待，它只能由另一个进程唤醒。

阻塞进程的过程如下：

- 找到将要被阻塞进程标识号对应的 PCB；
- 如果该进程为运行状态，则保护其现场，将其状态转为阻塞状态，停止运行；
- 将该 PCB 插入到阻塞队列中去；

**04 唤醒进程（唤醒原语）**

进程由「运行」转变为「阻塞」状态是由于进程必须等待某一事件的完成，所以处于阻塞状态的进程是绝对不可能叫醒自己的。

如果某进程正在等待 I/O 事件，需由别的进程发消息给它，则只有当该进程所期待的事件出现时，才由发现者进程用唤醒语句叫醒它。

唤醒进程的过程如下：

- 在该事件的阻塞队列中找到相应进程的 PCB；
- 将其从阻塞队列中移出，并置其状态为就绪状态；
- 把该 PCB 插入到就绪队列中，等待调度程序调度；

进程的阻塞和唤醒是一对功能相反的语句，如果某个进程调用了阻塞语句，则必有一个与之对应的唤醒语句。

### V.进程的上下文切换：

#### :dango:定义：

各个进程之间是共享 CPU 资源的，在不同的时候进程之间需要切换，让不同的进程可以在 CPU 执行，那么这个**一个进程切换到另一个进程运行，称为进程的上下文切换**。

进程的上下文切换不仅包含了**虚拟内存、栈、全局变量等<u>用户空间</u>的资源**，还包括了**内核堆栈、寄存器等<u>内核空间</u>的资源**。

通常，会把交换的信息**保存在进程的 PCB**，当要运行另外一个进程的时候，我们需要从这个进程的 PCB 取出上下文，然后恢复到 CPU 中，这使得这个进程可以继续执行

#### :parasol_on_ground: 进程上下文切换的场景：

- 当某个进程的时间片耗尽了，进程就从运行状态变为就绪状态，系统从就绪队列选择另外一个进程运行；
- 进程在系统资源不足（比如内存不足）时，要等到资源满足后才可以运行，这个时候进程也会被挂起，并由系统调度其他进程运行；
- 当进程通过睡眠函数 sleep 这样的方法将自己主动挂起时，自然也会重新调度；
- 当有优先级更高的进程运行时，为了保证高优先级进程的运行，当前进程会被挂起，由高优先级进程来运行；
- 发生**硬件中断时**，CPU 上的进程会被中断挂起，转而**执行内核中的中断服务程序**；

## 2、线程基础

### I.定义：

线程是一个**基本的CPU执行单元**，也是**程序执行流的最小单位**。线程也被称为**轻量级进程。**

同一个进程内多个线程之间可以共享代码段、数据段、打开的文件等资源，但每个线程各自都有一套独立的寄存器和栈，这样可以确保线程的控制流是相对独立的。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/16-%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84.jpg" alt="多线程" style="zoom:50%;" />

### II.进程与线程的比较

- 进程是**资源**（包括内存、打开的文件等）**分配的单位**，线程是 **CPU 调度的单位**；
- 进程拥有一个完整的资源平台，而线程只独享必不可少的资源，如寄存器和栈；
- 线程同样具有就绪、阻塞、执行三种基本状态，同样具有状态之间的转换关系；
- 线程能**减少并发执行的时间和空间开销**；

### III.线程系统开销比进程小的原因

- 线程的创建时间比进程快，因为进程在创建的过程中，还需要资源管理信息，比如内存管理信息、文件管理信息，而线程在创建的过程中，不会涉及这些资源管理信息，而是共享它们；
- **线程的终止时间比进程快**，因为线程释放的资源相比进程少很多，只需要释放自己独有的栈，寄存器等资源；
- 同一个进程内的线程切换比进程切换快，因为**线程具有相同的地址空间**（虚拟内存共享），这意味着**同一个进程的线程都具有同一个页表，**那么在切换的时候不需要切换页表。
- 由于同一进程的**各线程间共享内存和文件资源**，那么在线程之间数据传递的时候，就不需要经过内核了，这就使得线程之间的数据交互效率更高了；

### IV.线程的上下文切换

这还得看线程是不是属于同一个进程：

- 当两个线程不是属于同一个进程，则切换的过程就跟进程上下文切换一样；
- **当两个线程是属于同一个进程，因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，只需要切换线程的私有数据、寄存器等不共享的数据**；

所以，线程的上下文切换相比进程，开销要小很多。

### V.线程的三种实现方式：

- **用户线程（User Thread）**：在用户空间实现的线程，不是由内核管理的线程，是由用户态的**线程库**来完成线程的管理；
- **内核线程（Kernel Thread）**：在内核中实现的线程，是由内核管理的线程；
- **轻量级进程（LightWeight Process）**：在内核中来支持用户线程；**一个进程可有一个或多个 LWP**，每个 LWP 是跟内核线程一对一映射的，也就是 LWP 都是由一个内核线程支持，而且 LWP 是由内核管理并像普通进程一样被调度。**LWP 与用户线程的有三种对应关系。**

只有内核级线程才是OS看得见的，所以只有**内核级线程才是CPU调度的基本单位。**

<img src="C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221102150835032.png" alt="image-20221102150835032" style="zoom:67%;" />

<img src="C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221102150847577.png" alt="image-20221102150847577" style="zoom:67%;" />

<img src="C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221102150911043.png" alt="image-20221102150911043" style="zoom:67%;" />

### VI.多线程模型（基于LWP）：

1. **一对一模型：**一个用户线程映射到一个内核级线程上
   - 优点：并发能力强，一个线程被阻塞，另一个线程继续执行
   - 缺点：一个用户进程会占用多个内核线程，内存消耗较大，并且用户线程的上下文切换都会进入内核态，系统开销大
2. **多对一模型**：每个线程都映射到同一个内核级线程上
   - 优点：线程的切换可以在用户态下完成，不需要陷入内核
   - 缺点：一个线程被阻塞，整个进程都被阻塞，并发能力弱
3. **多对多模型**：n个用户级线程映射到m个内核级线程上（m<n）
   - 中和了以上两种模型的优点，是一种理想的多线程模型

## 3.进程的调度（低级调度，进程都在内存中）

### 补：挂起与阻塞

**进程的阻塞：**  正在执行的进程由于发生某时间（如I/O请求、申请缓冲区失败等）暂时无法继续执行。此时引起进程调度，OS把处理机分配给另一个就绪进程，而让受阻进程处于暂停状态，一般将这种状态称为阻塞状态。

**进程的挂起：**  挂起进程在操作系统中可以定义为暂时被淘汰出内存的进程，机器的资源是有限的，在资源不足的情况下，操作系统对在内存中的程序进行合理的安排，其中有的进程被暂时调离出内存，当条件允许的时候，会被操作系统再次调回内存，重新进入等待被执行的状态即就绪态，系统在超过一定的时间没有任何动作

**二者区别**：

1. 对系统资源占用不同：虽然都释放了CPU，但**阻塞的进程仍处于内存中**，而**挂起的进程通过“对换”技术被换出到外存**（磁盘）中。 
2. 发生时机不同：阻塞一般在进程**等待资源（IO资源、信号量等）时发生**；而挂起是由于用户和系统的需要，例如，终端用户需要暂停程序`（Ctrl + C）`研究其执行情况或对其进行修改、OS为了提高内存利用率需要将暂时不能运行的进程（处于就绪或阻塞队列的进程）调出到磁盘，进入挂起对列。
3. 恢复时机不同：阻塞要在等待的资源得到满足（例如获得了锁）后，才会进入就绪状态，等待被调度而执行；被挂起的进程由将其挂起的对象（如用户、系统）在时机符合时（调试结束、被调度进程选中需要重新执行）将其主动激活
   

### I.定义：

一旦操作系统把进程切换到运行状态，也就意味着该进程占用着 CPU 在执行，但是当操作系统把进程切换到其他状态时，那就不能在 CPU 中执行了，于是操作系统会选择下一个要运行的进程。

选择一个进程运行这一功能是在操作系统中完成的，通常称为**调度程序**（*scheduler*）。

### II.调度时机：

- *从就绪态 -> 运行态*：当进程被创建时（高级调度），会进入到就绪队列（中级调度），操作系统会从就绪队列选择一个进程运行（低级调度）；
- *从运行态 -> 阻塞态*：当进程发生 I/O 事件而阻塞时，操作系统必须选择另外一个进程运行；
- *从运行态 -> 结束态*：当进程退出结束后，操作系统得从就绪队列选择另外一个进程运行（低级调度）；

### III.两种基本的调度算法：

1. **非抢占式调度算法**挑选一个进程，然后让该进程运行直到被阻塞，或者直到该进程退出，才会调用另外一个进程，也就是说不会理时钟中断这个事情。
2. **抢占式调度算法**挑选一个进程，然后让该进程只运行某段时间，如果在该时段结束时，该进程仍然在运行时，则会把它挂起，接着调度程序从就绪队列挑选另外一个进程。这种抢占式调度处理，需要在时间间隔的末端发生**时钟中断**，以便把 CPU 控制返回给调度程序进行调度，也就是常说的**时间片机制**。

### IV.调度算法的评价指标：

**1.CPU利用率：**调度程序应确保 CPU 是始终匆忙的状态，这可提高 CPU 的利用率；

![image-20221102161241215](C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221102161241215.png)

**2.系统吞吐量**：吞吐量表示的是**单位时间内** CPU 完成进程的数量，长作业的进程会占用较长的 CPU 资源，因此会降低吞吐量，相反，短作业的进程会提升系统吞吐量；

![image-20221102161329658](C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221102161329658.png)

**3.周转时间：**作业被提交给系统开始到完成这段时间`（作业完成时间 - 作业提交时间）`，一个进程的周转时间越小越好；

​																**进程周转时间=进程运行+阻塞时间+等待时间**

![image-20221102161419478](C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221102161419478.png)

<img src="C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221102161638290.png" alt="image-20221102161638290" style="zoom: 67%;" />

**4.等待时间**：这个等待时间不是阻塞状态的时间，而是**进程处于就绪队列的时间**，等待的时间越长，用户越不满意；

![image-20221102161949433](C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221102161949433.png)

**5.响应时间**：用户提交请求到系统第一次产生响应所花费的时间`(比如输入一条命令然后计算机返回响应)`，在交互式系统中，响应时间是衡量调度算法好坏的主要标准。

## 4、六大进程调度算法

#### I.先来先服务调度算法——FCFS（非抢占式）

最简单的一个调度算法，就是非抢占式的**先来先服务（First Come First Severd, FCFS）算法。**

先来后到，每次从就绪队列选择最先进入队列的进程，然后一直运行，**直到进程退出或被阻塞**，才会继续从队列中选择第一个进程接着运行。

当一个长作业先运行了，后面的短作业等待的时间就会很长，不利于短作业。

该算法不会导致**饥饿**（某一作业或进程长时间得不到服务）

FCFS 对长作业有利，适用于 **CPU 繁忙型作业的系统**，而**不适用于 I/O 繁忙型作业的系统**。

#### II.最短作业优先调度算法——SJF（非抢占式）

**最短作业优先（Shortest Job First, SJF）调度算法**同样也是顾名思义，它会**优先选择运行时间最短的进程来运行**，这有助于**提高系统的吞吐量。**

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/25-%E6%9C%80%E7%9F%AD%E4%BD%9C%E4%B8%9A%E4%BC%98%E5%85%88%E7%AE%97%E6%B3%95.jpg" alt="SJF 调度算法" style="zoom:67%;" />

这显然对长作业不利，很容易造成**饥饿，长作业一直得不到服务**

#### III.高响应比优先—HRRN（非抢占式）

前面的「先来先服务调度算法」和「最短作业优先调度算法」都没有很好的权衡短作业和长作业。

那么，**高响应比优先 （Highest Response Ratio Next, HRRN）调度算法**主要是权衡了短作业和长作业。

**响应比（优先权）计算公式如下：**

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/26-%E5%93%8D%E5%BA%94%E6%AF%94%E5%85%AC%E5%BC%8F.jpg)

- 如果两个进程等待时间相等，那么要求服务时间越短，优先权越高（短作业更容易被选中）
- 如果两个进程要求服务时间相等，等待时间越久，优先权越高（长作业更容易被选中）

![image-20221102163725157](C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221102163725157.png)

#### IV.时间片轮转调度算法——RR（抢占式）

按照各个进程到达就绪队列的顺序，轮流让各个进程执行一个**时间片**。若进程未在一个时间片内执行完，则直接剥夺处理器，将进程重新放进就绪队列。

时间片的长度就是一个很关键的点：

- 如果时间片设得太短会导致过多的进程上下文切换，降低了 CPU 效率；
- 如果设得太长又可能引起对短作业进程的响应时间变长。将

通常时间片设为 `20ms~50ms` 通常是一个比较合理的折中值。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/27-%E6%97%B6%E9%97%B4%E7%89%87%E8%BD%AE%E8%AF%A2.jpg" alt="RR 调度算法" style="zoom:67%;" />

- 优点：公平，响应快，适用于分时操作系统，**不会产生饥饿**
- 缺点：进程切换频繁，系统开销大，并且不会区分任务的紧急程度

#### V.最高优先级调度算法

对于多用户计算机系统就有不同的看法了，它们希望调度是有优先级的，即希望调度程序能**从就绪队列中选择最高优先级的进程进行运行，这称为最高优先级（Highest Priority First，HPF）调度算法**。

进程的优先级可以分为，静态优先级或动态优先级：

- 静态优先级：创建进程时候，就已经确定了优先级了，然后整个运行时间优先级都不会变化；
- 动态优先级：根据进程的动态变化调整优先级，比如如果进程运行时间增加，则降低其优先级，如果进程等待时间（就绪队列的等待时间）增加，则升高其优先级，也就是**随着时间的推移增加等待进程的优先级**。

该算法也有两种处理优先级高的方法，非抢占式和抢占式：

- 非抢占式：当就绪队列中出现优先级高的进程，运行完当前进程，再选择优先级高的进程。
- 抢占式：当就绪队列中出现优先级高的进程，当前进程挂起，调度优先级高的进程运行。

但是依然有缺点，可能会导致**低优先级的进程永远不会运行，会产生饥饿现象**。

#### VI.多级反馈队列调度算法（抢占式）

- 「多级」表示有多个队列，每个**队列优先级从高到低，同时优先级越高时间片越短**。
- 「反馈」表示如果有新的进程加入优先级高的队列时，立刻停止当前正在运行的进程，转而去运行优先级高的队列；

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/28-%E5%A4%9A%E7%BA%A7%E9%98%9F%E5%88%97.jpg" alt="多级反馈队列" style="zoom:67%;" />

- 设置为多个队列设置多个优先级，优先级从高到低，优先级越高，时间片越短
- 当第一个进程进入队列后，会将其调入CPU中运行，如果在它运行期间有其它进程进入就绪队列，新的进程会被放入到第一级队列的末尾，按先来先服务的原则排队等待被调度。
- 如果在正在CPU运行的进程没有在当前所在队列规定的时间片没运行完成，则将其转入到第下一级队列的末尾，以此类推，直至完成；
- 新进程都是从1级就绪队列为入口的，中当较高优先级的队列为空，才调度较低优先级的队列中的进程运行。如果进程运行时，有新进程进入较高优先级的队列，则停止当前运行的进程并将其移入到原队列末尾，接着让较高优先级的进程运行；

该算法会导致**饥饿**现象。

<img src="https://img-blog.csdnimg.cn/20200405221830211.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom: 50%;" />

## 5、 进程间的通信方式

### I.定义：

为了保证程序的安全性，**一个进程不能直接访问另一个进程的地址空间**，但是进程之间的**信息交换又是必须实现**的。为了保证进程之间的安全通信，操作系统提供了一些通信方式。

**:alembic: 进程有六大通信方式：**

- **管道**
- **消息队列**
- **共享内存**
- **信号量**
- **信号**
- **Socket**

### II.管道：

**所谓的管道，就是内核里面的一串缓存**。从管道的一段写入的数据，实际上是缓存在内核中的，另一端读取，也就是从内核中读取这段数据。另外，管道传输的数据是无格式的流且大小受限。**管道只能采用半双工通信，某一时间段内只能实现单向传输**。如果要实现双向同时通信，则需要设置两个管道。

当我们执行以下命令：

```sh
$ ps -ef | grep mysql
```

命令行里的「`|`」竖线就是一个**管道**，它的功能是将前一个命令（`ps -ef`）的输出，作为后一个命令（`grep mysql`）的输入，从这功能描述，可以看出**管道传输数据是单向的**，如果想相互通信，我们需要创建两个管道才行。

同时，我们得知上面这种管道是没有名字，所以「`|`」表示的管道称为**匿名管道**，用完了就销毁。

匿名管道的创建，需要通过下面这个系统调用：

```c
int pipe(int fd[2])
```

这里表示创建一个匿名管道，并返回了两个描述符，一个是管道的读取端描述符 `fd[0]`，另一个是管道的写入端描述符 `fd[1]`。注意，这个匿名管道是特殊的文件，只存在于内存，不存于文件系统中。

在 shell 里面执行 `A | B`命令的时候，A 进程和 B 进程都是 shell 创建出来的子进程，A 和 B 之间不存在父子关系，它俩的父进程都是 shell。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/8-%E7%AE%A1%E9%81%93-pipe-shell.jpg" alt="img" style="zoom: 50%;" />

- **对于匿名管道，它的通信范围是存在父子关系的进程**。因为管道没有实体，也就是没有管道文件，只能通过 fork 来复制父进程 fd 文件描述符，来达到通信的目的。
- 另外，**对于命名管道，它可以在不相关的进程间也能相互通信**。因为命令管道，提前创建了一个类型为管道的设备文件，在进程里只要使用这个设备文件，就可以相互通信。
- 不管是匿名管道还是命名管道，进程写入的数据都是缓存在内核中，另一个进程读取数据时候自然也是从内核中获取，同时通信数据都遵循**先进先出**原则，不支持 lseek 之类的文件定位操作。

###  III.消息队列

**消息队列是保存在内核中的消息链表**，在发送数据时，会分成一个一个独立的数据单元，也就是消息体（数据块），消息体是用户自定义的数据类型，消息的发送方和接收方要约定好消息体的数据类型，所以每个消息体都是固定大小的存储块，不像管道是无格式的字节流数据。如果进程从消息队列中读取了消息体，内核就会把这个消息体删除。

消息队列生命周期随内核，**如果没有释放消息队列或者没有关闭操作系统，消息队列会一直存在**，而前面提到的匿名管道的生命周期，是随进程的创建而建立，随进程的结束而销毁。

消息这种模型，两个进程之间的通信就像平时发邮件一样，你来一封，我回一封，可以频繁沟通了。

但邮件的通信方式存在不足的地方有两点，**一是通信不及时，二是附件也有大小限制**，这同样也是消息队列通信不足的点。

**消息队列不适合比较大数据的传输**，因为在内核中每个消息体都有一个最大长度的限制，同时所有队列所包含的全部消息体的总长度也是有上限。在 Linux 内核中，会有两个宏定义 `MSGMAX` 和 `MSGMNB`，它们以字节为单位，分别定义了一条消息的最大长度和一个队列的最大长度。

**消息队列通信过程中，存在用户态与内核态之间的数据拷贝开销**，因为进程写入数据到内核中的消息队列时，会发生从用户态拷贝数据到内核态的过程，同理另一进程读取内核中的消息数据时，会发生从内核态拷贝数据到用户态的过程。

### IV.共享内存

**共享内存的机制，就是拿出一块虚拟地址空间来，映射到相同的物理内存中**。这样这个进程写入的东西，另外一个进程马上就能看到了，都不需要内存的来回拷贝，大大提高了进程间通信的速度。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/9-%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98.jpg" alt="img" style="zoom:67%;" />

### V.信号量

为了防止多进程竞争共享资源，而造成的数据错乱，所以需要保护机制，使得共享的资源，在任意时刻只能被一个进程访问。正好，**信号量**就实现了这一保护机制。

**信号量其实是一个整型的计数器，主要用于实现进程间的互斥与同步，而不是用于缓存进程间通信的数据**。

信号量表示**资源的数量**，控制信号量的方式有两种原子操作（**P、V操作**）：

- 一个是 **P 操作**，这个操作会把信号量减去 1，相减后如果信号量 < 0，则表明资源已被占用，进程需阻塞等待；相减后如果信号量 >= 0，则表明还有资源可使用，进程可正常继续执行。
- 另一个是 **V 操作**，这个操作会把信号量加上 1，相加后如果信号量 <= 0，则表明当前有阻塞中的进程，于是会将该进程唤醒运行；相加后如果信号量 > 0，则表明当前没有阻塞中的进程；

P 操作是用在进入共享资源之前，V 操作是用在离开共享资源之后，这两个操作是必须成对出现的。

### VI.信号

上面说的进程间通信，都是常规状态下的工作模式。**对于异常情况下的工作模式，就需要用「信号」的方式来通知进程。**

在 Linux 操作系统中， 为了响应各种各样的事件，提供了几十种信号，分别代表不同的意义。我们可以通过 `kill -l` 命令，查看所有的信号：

```shell
kill -l
 1) SIGHUP       2) SIGINT       3) SIGQUIT      4) SIGILL       5) SIGTRAP
 6) SIGABRT      7) SIGBUS       8) SIGFPE       9) SIGKILL     10) SIGUSR1
11) SIGSEGV     12) SIGUSR2     13) SIGPIPE     14) SIGALRM     15) SIGTERM
16) SIGSTKFLT   17) SIGCHLD     18) SIGCONT     19) SIGSTOP     20) SIGTSTP
21) SIGTTIN     22) SIGTTOU     23) SIGURG      24) SIGXCPU     25) SIGXFSZ
26) SIGVTALRM   27) SIGPROF     28) SIGWINCH    29) SIGIO       30) SIGPWR
31) SIGSYS      34) SIGRTMIN    35) SIGRTMIN+1  36) SIGRTMIN+2  37) SIGRTMIN+3
38) SIGRTMIN+4  39) SIGRTMIN+5  40) SIGRTMIN+6  41) SIGRTMIN+7  42) SIGRTMIN+8
43) SIGRTMIN+9  44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+12 47) SIGRTMIN+13
48) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-14 51) SIGRTMAX-13 52) SIGRTMAX-12
53) SIGRTMAX-11 54) SIGRTMAX-10 55) SIGRTMAX-9  56) SIGRTMAX-8  57) SIGRTMAX-7
58) SIGRTMAX-6  59) SIGRTMAX-5  60) SIGRTMAX-4  61) SIGRTMAX-3  62) SIGRTMAX-2
63) SIGRTMAX-1  64) SIGRTMAX
```

运行在 **shell 终端的进程**，我们可以通过键盘输入某些组合键的时候，给进程发送信号。例如

- Ctrl+C 产生 `SIGINT` 信号，表示终止该进程；
- Ctrl+Z 产生 `SIGTSTP` 信号，表示停止该进程，但还未结束；

如果**进程在后台运行**，可以通过 `kill` 命令的方式给进程发送信号，但前提需要知道运行中的进程 PID 号，例如：

- kill -9 1050 ，表示给 PID 为 1050 的进程发送 `SIGKILL` 信号，用来立即结束该进程；

所以，信号事件的来源主要有硬件来源（如键盘 Cltr+C ）和软件来源（如 kill 命令）。

信号是进程间通信机制中**唯一的异步通信机制**，因为可以在任何时候发送信号给某一进程，一旦有信号产生，我们就有下面这几种，用户进程对信号的处理方式。

### VII.Socket

前面提到的管道、消息队列、共享内存、信号量和信号都是在同一台主机上进行进程间通信，那要想**跨网络与不同主机上的进程之间通信，就需要 Socket 通信了。**Socket 实际上**不仅用于不同的主机进程间通信，还可以用于本地主机进程间通信**，可根据创建 Socket 的类型不同，分为三种常见的通信方式，一个是基于 **TCP 协议**的通信方式，一个是基于 **UDP 协议**的通信方式，一个是**本地进程间**通信方式。

## 6、进程/线程的同步与互斥

### I.进程同步

**同步也称为直接制约关系**

在多道程序环境下，进程是并发执行的，**不同进程之间存在着不同的相互制约关系**。为了协调进程之间的相互制约关系,如等待、传递信息等，引入了进程同步的概念。进程同步是为了解决进程的异步问题。

例子，线程 1 是负责读入数据的，而线程 2 是负责处理数据的，这两个线程是相互合作、相互依赖的。线程 2 在没有收到线程 1 的唤醒通知时，就会一直阻塞等待，当线程 1 读完数据需要把数据传给线程 2 时，线程 1 会唤醒线程 2，并把数据交给线程 2 处理。

**所谓同步，就是并发进程/线程在一些关键点上可能需要互相等待与互通消息，这种相互制约的等待与互通信息称为进程/线程同步**。

### II.进程互斥

互斥，亦称间接制约关系。进程互斥指当一个进程访问某临界资源时，另一个想要访问该临界资源的进程必须等待。当前访问临界资源的进程访问结束，释放该资源之后，另一个进程才能去访问临界资源。

我们把一个时间段内只允许一个进程使用的资源称为临界资源。许多物理设备(比如摄像头、打印机)都属于临界资源。此外还有许多变量、数据、内存缓冲区等都属于临界资源。

对临界资源的访问，必须互斥地进行。


<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/10-%E4%B8%B4%E7%95%8C%E5%8C%BA.jpg" alt="互斥" style="zoom:67%;" />

### III.互斥和同步的实现方式

为了实现进程/线程间正确的协作，操作系统必须提供实现进程协作的措施和方法，主要的方法有两种：

- *锁*：加锁、解锁操作；
- *信号量*：P、V 操作；

#### 锁

使用加锁操作和解锁操作可以解决并发线程/进程的互斥问题。任何想要访问临界资源的线程，必须先加锁，当结束对临界资源的访问后要进行解锁操作，如果锁被其它线程占有，则线程进阻塞队列等待资源。

根据锁的实现不同，可以分为**「忙等待锁」和「无忙等待锁」**

「忙等待锁」的硬件实现（**TSL**）：

##### :seat:**「忙等待锁」（自旋锁）:**

**原子操作指令 —— 测试和置位（Test-and-Set）指令实现的**

如果用 C 代码表示 Test-and-Set 指令，形式如下：

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/13-TestAndSet.jpg" alt="img" style="zoom:30%;" />

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/14-%E8%87%AA%E6%97%8B%E9%94%81.jpg" alt="img" style="zoom:30%;" />

很明显，当获取不到锁时，线程就会一直 while 循环，不做任何事情，所以就被称为「忙等待锁」，也被称为**自旋锁（spin lock）**。

这是最简单的一种锁，一直自旋，利用 CPU 周期，直到锁可用。在单处理器上，需要抢占式的调度器（即不断通过时钟中断一个线程，运行其他线程）。否则，自旋锁在单 CPU 上无法使用，因为一个自旋的线程永远不会放弃 CPU。

**原子操作指令 ——交换（XCHG）指令实现的**

**XCHG**

- 用于交换两个操作数
- 具备原子性，CPU会自动加LOCK前缀，保证在进行操作的时候，不会让其它cpu操作同一个内存

<img src="https://img-blog.csdnimg.cn/20200318164026883.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:50%;" />

对于自旋锁，在**单CPU的情况下性能开销很大**，假设线程A和线程B，C，D……同时去竞争锁，线程A竞争成功进入临界区，但此时线程A时间片被抢占，而此时其它线程都在同时自旋竞争锁，由于锁并未被线程A释放，其它线程在放弃CPU时间之前必定是竞争不到锁的，这样CPU时间片就被白白浪费了。

但对于**多CPU的情况下自旋锁性能就比较好**了，自旋竞争是发生在别的CPU上的，**并不影响持有锁的线程访问临界资源**。

##### :sassy_woman:「无忙等待锁」

无等待锁顾明思议就是获取不到锁的时候，**不用自旋**。

既然不想自旋，那当没获取到锁的时候，就把当前线程放入到锁的**等待队列**，然后执行调度程序，把 CPU 让给其他线程执行。

这种加锁方式在单CPU上运行十分有效，单一个线程竞争失败，立即放弃CPU时间片，由运行态转为就绪态，但在多CPU条件下这种方式比较糟糕，因为会**涉及大量的线程上下文切换，系统开销较大。**

##### :watermelon:Linux两阶段加锁：

Linux采用的是两阶段加锁方式来实现线程的互斥与同步，第一阶段先获取自旋锁，第二阶段进入睡眠。

如果第一阶段自旋失败，第二阶段进入睡眠状态，直到锁可用。Linux用这种方式来加锁解锁，**只自旋一次。**

#### 信号量：

信号量是操作系统提供的一种协调共享资源访问的方法。

通常**信号量表示资源的数量**，对应的变量是一个整型（`sem`）变量。

另外，还有**两个原子操作（原语）的系统调用函数来控制信号量的**，分别是：

- *P 操作*：将 `sem` 减 `1`，相减后，如果 `sem < 0`，则进程/线程进入阻塞等待，否则继续，表明 P 操作可能会阻塞；
- *V 操作*：将 `sem` 加 `1`，相加后，如果 `sem <= 0`，唤醒一个等待中的进程/线程，表明 V 操作不会阻塞；

> PV 操作如何使用的呢？

信号量不仅可以实现临界区的互斥访问控制，还可以**线程间的事件同步。**

经典的生产者消费者问题、哲学家就餐问题、读者写者问题都可以根据对整型变量的P,V操作来实现。

## 7、死锁

### I. 定义：

各个线程同时等待对方持有的资源时就会造成死锁。

### II.死锁发生的四个必要条件：

1. 互斥条件：多个线程**不能同时共享**一个资源。
2. 持有并等待条件：线程 A 在等待线程B持有的资源 2 的同时并**不会释放自己已经持有的资源 1**。
3. 不剥夺条件:当线程已经持有了资源 ，**在自己使用完之前不能被其他线程获取**
4. 循环等待条件:若干线程之间形成一种头尾相接的循环等待资源关系。

### III.避免死锁的方法：

产生死锁的四个必要条件是：互斥条件、持有并等待条件、不可剥夺条件、环路等待条件。

那么避免死锁问题就只需要**破环其中一个条件**就可以，最常见的并且可行的就是**使用资源有序分配法，来破环环路等待条件**。

- 避免一个线程同时获取多个锁；
- 避免一个线程在锁内同时占有多个资源，尽量保证每个锁只占有一个资源；
- 尝试使用定时锁，使用`tryLock(timeout) `来替代使用内部锁机制;
- 对于数据库锁，加锁和解锁必须在一个数据库连接里，否则会出现解锁失败的情况；

## 8.一个进程可以创建多少个线程？

这个问题跟两个东西有关系：

- **进程的虚拟内存空间上限**，因为创建一个线程，操作系统需要为其分配一个栈空间，如果线程数量越多，所需的栈空间就要越大，那么虚拟内存就会占用的越多。
- **系统参数限制**，虽然 Linux 并没有内核参数来控制单个进程创建的最大线程个数，但是有系统级别的参数来控制整个系统的最大线程个数。

我们先看看，在进程里创建一个线程需要消耗多少虚拟内存大小

我们可以执行 ulimit -a 这条命令，**查看进程创建线程时默认分配的栈空间大小**，比如我这台虚拟机**默认分配给线程的栈空间大小为 8M。**

```bash
[root@gcs100 myredis]# ulimit -a
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 7154
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 1024
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192 #8M
cpu time               (seconds, -t) unlimited
max user processes              (-u) 7154
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited
```

在 32 位 Linux 系统里，一个进程的虚拟空间是 4G，内核分走了1G，**留给用户用的只有 3G**。

那么假设创建一个线程需要占用 10M 虚拟内存，总共有 3G 虚拟内存可以使用。于是我们可以算出，最多可以创建差不多 300 个（3G/10M）左右的线程。

## 9.线程崩溃了，进程也会崩溃吗？

 C/C++ 语言里，线程崩溃后，进程也会崩溃，而 Java 语言里却不会

一般来说如果线程是因为**非法访问内存引起的崩溃，那么进程肯定会崩溃**，为什么系统要让进程崩溃呢，这主要是因为在进程中，**各个线程的地址空间是共享的**，既然是共享，那么某个线程对地址的非法访问就会导致内存的不确定性，进而可能会影响到其他线程，这种操作是危险的，操作系统会认为这很可能导致一系列严重的后果，于是干脆让整个进程崩溃

### 进程是如何崩溃的-信号机制简介

1. CPU 执行正常的进程指令
2. 调用 kill 系统调用向进程发送信号
3. 进程收到操作系统发的信号，CPU 暂停当前程序运行，并将控制权转交给操作系统
4. 调用 kill 系统调用向进程发送信号（假设为 11，即 SIGSEGV，一般非法访问内存报的都是这个错误）
5. **操作系统根据情况执行相应的信号处理程序（函数），一般执行完信号处理程序逻辑后会让进程退出**

注意上面的第五步，如果进程没有注册自己的信号处理函数，那么操作系统会执行默认的信号处理程序（一般最后会让进程退出），但如果注册了，则会执行自己的信号处理函数，这样的话就给了进程一个垂死挣扎的机会，它收到 kill 信号后，可以调用 exit() 来退出，**但也可以使用 sigsetjmp，siglongjmp 这两个函数来恢复进程的执行**

### 为什么线程崩溃不会导致 JVM 进程崩溃

因为 **JVM 自定义了自己的信号处理函数**，**拦截了 SIGSEGV 信号**，针对这两者不让它们崩溃。

<img src="https://img-blog.csdnimg.cn/474ddf8657a0438da1822e0f6fa59af7.png" alt="img" style="zoom: 67%;" />

以看到，在启动 JVM 的时候，也设置了信号处理函数，**收到 SIGSEGV，SIGPIPE 等信号后最终会调用 JVM_handle_linux_signal 这个自定义信号处理函数**，再来看下这个函数的主要逻辑。

```c++
JVM_handle_linux_signal(int sig,
                        siginfo_t* info,
                        void* ucVoid,
                        int abort_if_unrecognized) {

   // Must do this before SignalHandlerMark, if crash protection installed we will longjmp away
  // 这段代码里会调用 siglongjmp，主要做线程恢复之用
  os::ThreadCrashProtection::check_crash_protection(sig, t);

  if (info != NULL && uc != NULL && thread != NULL) {
    pc = (address) os::Linux::ucontext_get_pc(uc);

    // Handle ALL stack overflow variations here
    if (sig == SIGSEGV) {
      // Si_addr may not be valid due to a bug in the linux-ppc64 kernel (see
      // comment below). Use get_stack_bang_address instead of si_addr.
      address addr = ((NativeInstruction*)pc)->get_stack_bang_address(uc);

      // 判断是否栈溢出了
      if (addr < thread->stack_base() &&
          addr >= thread->stack_base() - thread->stack_size()) {
        if (thread->thread_state() == _thread_in_Java) {            // 针对栈溢出 JVM 的内部处理
            stub = SharedRuntime::continuation_for_implicit_exception(thread, pc, SharedRuntime::STACK_OVERFLOW);
        }
      }
    }
  }

  if (sig == SIGSEGV &&
               !MacroAssembler::needs_explicit_null_check((intptr_t)info->si_addr)) {
         // 此处会做空指针检查
      stub = SharedRuntime::continuation_for_implicit_exception(thread, pc, SharedRuntime::IMPLICIT_NULL);
  }


  // 如果是栈溢出或者空指针最终会返回 true，不会走最后的 report_and_die，所以 JVM 不会退出
  if (stub != NULL) {
    // save all thread context in case we need to restore it
    if (thread != NULL) thread->set_saved_exception_pc(pc);

    uc->uc_mcontext.gregs[REG_PC] = (greg_t)stub;
    // 返回 true 代表 JVM 进程不会退出
    return true;
  }

  VMError err(t, sig, pc, info, ucVoid);
  // 生成 hs_err_pid_xxx.log 文件并退出
  err.report_and_die();

  ShouldNotReachHere();
  return true; // Mute compiler
}
```

**虚拟机内部定义了信号处理函数，而在信号处理函数中对这两者做了额外的处理以让 JVM 不崩溃，另一方面也可以看出如果 JVM 不对信号做额外的处理，最后会自己退出并产生 crash 文件 hs_err_pid_xxx.log（可以通过 -XX:ErrorFile=/var/\*log\*/hs_err.log 这样的方式指定），这个文件记录了虚拟机崩溃的重要原因**。

所以也可以说，虚拟机是否崩溃只要看它是否会产生此崩溃日志文件

## 10、管程

### 定义：

管程在功能上和信号量及PV操作类似，属于一种进程同步互斥工具，但是具有与信号量及PV操作不同的属性。管程(monitor)保证了同一时刻**只有一个进程在管程内活动**，即管程内定义的操作在同一时刻只被一个进程调用

### 引入管程的原因

**信号量机制的缺点：**进程自备同步操作，**P(S)和V(S)操作大量分散在各个进程中，不易管理，易发生死锁。**

**管程特点：**管程封装了同步操作，对进程隐蔽了同步细节，简化了同步功能的调用界面。用户编写并发程序如同编写顺序(串行)程序。

**引入管程机制的目的：**

1. 把分散在各进程中的临界区集中起来进行管理；
2. 防止进程有意或无意的违法同步操作；
3. 便于用高级语言来书写程序，也便于程序正确性验证。

# 三、内存管理

## 1.内存硬件基础

### I.什么是DRAM？

**动态随机存取存储器**（**Dynamic Random Access Memory**，**DRAM**）是一种半导体存储器，主要的作用原理是利用**电容内存储电荷的多少来代表一个二进制比特（bit）是1还是0。**由于在现实中晶体管会有漏电电流的现象，导致电容上所存储的电荷数量并不足以正确的判别数据，而导致数据毁损。因此**对于DRAM来说，周期性地充电是一个无可避免的要件。**由于这种需要定时刷新的特性，因此被称为“动态”存储器。相对来说，静态存储器（SRAM）只要存入数据后，纵使不刷新也不会丢失记忆。

与大部分的随机存取存储器（RAM）一样，由于存在DRAM中的数据会在电力切断以后很快消失，因此它属于一种**易失性存储器设备。** 

**DRAM一般用作计算机的主存**，DRAM 的数据访问电路和刷新电路都比 SRAM 更复杂，所以访问的速度会更慢，访问内存速度大概在 `200~300` 个 时钟周期之间。

<img src="https://img-blog.csdnimg.cn/20210318000728619.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:67%;" />

## 2.指令的工作原理

<img src="https://img-blog.csdnimg.cn/20200421180308143.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:50%;" />

### :last_quarter_moon:运行原理：

**指令的工作原理—操作码+若干操作数（可能包含地址参数）**

当我们写出代码后，编译器会先将我们写出的程序编译为汇编程序（伪指令），之后由汇编器将汇编指令汇编为计算机能够识别的机器码，每一条机器码中的特定位数的比特数（01序列）对应操作码或操作数。

比如在以下的数据传送指令中就包含了目的地址的地址值，但程序经过编译、汇编生成机器指令的时候并不知道进程的数据要被放到什么位置**（因为运行时数据才被载入内存）**。所以**编译时生成的地址一般为逻辑地址。**

![image-20221104140657524](C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221104140657524.png)

### :first_quarter_moon_with_face: 程序的载入有三种方式

程序在运行前都要经过四个阶段：编译、汇编、链接、载入。**当程序真正被载入时，绝大部分数据的物理地址值才真正确定下来。**

**①绝对装入：**在编译时，如果知道程序将放在内存中的那个位置，编译器将绝对地址的目标代码。装入程序按照装入模块中的地址，将程序和数据装入内存。比如JAVA中static final修饰的数据（常量），都是在编译时就确定下来的，所以访问它们不会触发类加载。

**②静态重定位：**又称**可重定位装入。**编译。链接后的装入模块的地址都是从0开始的，指令使用的地址、数据存放的地址都是相对于起始而言的逻辑地址。可根据内存的当前情况，将装入模块装入到内存的适当位置。对数据的逻辑地址进行“重定位”，将逻辑地址转化为物理地址，地址的转化是在装入时完成的。比如JAVA中static修饰的变量，或C中static修饰的变量，都是静态重定位。

**③ 动态重定位**：又称**运行时装入**。编译。链接后的装入模块的地址都是从0开始的，装入程序把装入模块装入内存后，并不会把逻辑地址转变为物理地址，而是把地址推迟到真正执行到那段代码的时候才进行地址的转化，因此装入内存后所有的地址仍然是逻辑地址。这种方式需要一个**重定位寄存器**的支持。比如`malloc()`动态内存分配就是动态重定位的。

### :whale:链接的三种方式：

**①静态链接：**在程序运行之前先将各个目标模块以及它们所需的库函数连接成一个完整的可执行文件（目标模块）。

**②装入时动态链接：**将各个目标模块装入内存时，边装入边链接。

**③ 动态重定位：**在程序运行时需要该模块时，才对它进行链接。优点是便于修改和更新，便于实现目标模块的共享。

## 3.虚拟内存

### I.概念：

我们都知道物理内存的大小与硬件DRAM有关，所以物理内存总是会被用完的，假设在一个4G内存的电脑上运行一个4G大小的程序，如果把这个程序全部装入内存（操作系统的程序本身也占据内存），那么这台电脑也运行不了其它程序了，所以就引入了**虚拟内存（VM）**这个概念，即让操作系统为每个进程分配独立的一套「**虚拟地址**」，**操作系统会提供一种机制，将不同进程的虚拟地址和不同内存的物理地址映射起来。**如果程序要访问虚拟地址的时候，由操作系统转换成不同的物理地址，这样不同的进程运行的时候，写入的是不同的物理地址，这样就不会冲突了。当程序未被很久没有运行时它会被换出内存存入磁盘，当重新启动时有被换入内存。

虚拟内存系统的一个主要目标是对程序透明，应用程序是感知不到内存被虚拟化的，在幕后由操作系统和硬件让不同的程序复用物理内存。

我们所看到的程序地址其实都是虚拟地址，只有操作系统，通过虚拟化技术，知道了这些指令和数据所存在的物理地址。比如以下这个程序显示了代码段、堆、栈的起始位置，它们都是为进程抽象出来的虚拟地址。

```c
#include<stdio.h>
#include<stdlib.h>
int main()
{
        printf("location of code :%p\n",(void*)main);
        printf("location of heap: %p\n", (void*)malloc(1));
        int x = 6;
        printf("location of stack: %p\n", (void*)&x);
        return x;
}
```

```bash
[root@gcs100 c_dir]# ./address 
location of code :0x40057d
location of heap: 0x1f99010
location of stack: 0x7ffc8eb8a7bc
```

- 我们程序所使用的内存地址叫做**虚拟内存地址**（*Virtual Memory Address*）
- 实际存在硬件里面的空间地址叫**物理内存地址**（*Physical Memory Address*）。

操作系统引入了虚拟内存，进程持有的虚拟地址会通过 CPU 芯片中的**内存管理单元（MMU）**的映射关系，来转换变成物理地址，然后再通过物理地址访问内存，如下图所示：

MMU中有两个关键的寄存器：**基址寄存器、界限寄存器。**这对寄存器支持**地址转化和边界检查**。在进程切换时操作系统要为该进程保留这两个寄存器。

<img src="https://img-blog.csdnimg.cn/72ab76ba697e470b8ceb14d5fc5688d9.png" alt="img" style="zoom:67%;" />

### II.地址转换

#### :camel:内存分段：

##### :black_joker:**概念**：

程序是由若干个逻辑分段组成的，如可由**代码分段、数据分段、栈段、堆段**组成。我们知道堆和栈之间是还有大部分空间是未分配的，如果把整个程序的虚拟内存都载入，那必将造成浪费，所以根据**不同的段的不同的属性，所以就用分段（Segmentation）的形式把这些段分离出来。**

<img src="C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221104152711441.png" alt="image-20221104152711441" style="zoom:20%;" />

如图所示，程序被分为不同的段**放入不同的物理内存区域，同时还为操作系统保存一个段。**

##### :alarm_clock: **分段机制下，虚拟地址和物理地址是如何映射的？**

用虚拟地址的开头几位来表示不同的段**（段选择因子）**

用虚拟地址之后几位表示偏移量**（段内地址）**

<img src="https://img-blog.csdnimg.cn/a9ed979e2ed8414f9828767592aadc21.png" alt="img" style="zoom: 50%;" />

- **段选择子**就保存在段寄存器里面。段选择子里面最重要的是**段号**，用作段表的索引。**段表**里面保存的是这个**段的基地址、段的界限和特权等级**等。
- 虚拟地址中的**段内偏移量**应该位于 **0 和段界限之间**，如果段内偏移量是合法的，就将段基地址加上段内偏移量得到物理内存地址。

在上面，知道了虚拟地址是通过**段表**与物理地址进行映射的，分段机制会把程序的虚拟地址分成 4 个段，每个段在段表中有一个项，在这一项找到段的基地址，再加上偏移量，于是就能找到物理内存中的地址。

![image-20221104155233957](C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221104155233957.png)

**虚拟地址（段号 + 偏移量）-> 段号（段表索引） -> 段表项（记录段的物理基址与界限）-> 物理地址**

:amphora: **内存碎片：**

内存分段管理可以做到段根据实际需求分配内存，所以有多少需求就分配多大的段，所以**不会出现内部碎片**。

但是由于每个段的长度不固定，所以多个段未必能恰好使用所有的内存空间，会产生了多个不连续的小物理内存，导致新的程序无法被装载，所以**会出现外部碎片**的问题。

<img src="https://img-blog.csdnimg.cn/6142bc3c917e4a6298bdb62936e0d332.png" alt="img" style="zoom:50%;" />

解决「外部内存碎片」的问题就是**内存交换（紧凑）**。

可以把音乐程序占用的那 256MB 内存写到硬盘上，然后再从硬盘上读回来到内存里。再读回的时候，不能装载回原来的位置，而是紧紧跟着那已经被占用了的 512MB 内存后面。

这个内存交换空间，在 Linux 系统里，也就是我们常看到的 **Swap 空间**，这块空间是**从硬盘划分出来的**，用于**内存与硬盘的空间交换**。

##### :satellite: 分段的缺陷：

- 内存交换效率低：

对于多进程的系统来说，用分段的方式，外部内存碎片是很容易产生的，产生了外部内存碎片，那不得不重新 `Swap` 内存区域，这个过程会产生性能瓶颈。因为硬盘的访问速度要比内存慢太多，**如果频繁进行内存交换或者交换的是一个占内存空间很大的程序，这样整个机器都会显得卡顿。**

- 不足以支持稀疏地址空间：

例如一个很大但很稀疏的堆都在一个逻辑段中，整个堆仍然需要完整加载到内存中。

### :horse_racing:内存分页：

分段的好处就是能按照不同程序段的属性给程序分配空间，避免了内部碎片，但是会出现**「外部碎片和内存交换的空间太大」**的问题。

要解决这些问题，那么就要想出能少出现一些外部碎片的办法。另外，当需要进行内存交换的时候，让需要交换写入或者从磁盘装载的数据更少一点，这样就可以解决问题了。这个办法，也就是**内存分页**（*Paging*）。

#### :label:概念：

分页就是程序的地址空间分为固定大小的单元，**每一个单元称为一页**。相应的，我们将物理内存当作一个定长的数组，数组中的每个叫做**夜侦**，每个页帧包含一个虚拟页。

<img src="C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221105161149668.png" alt="image-20221105161149668" style="zoom:23%;" />

#### :sailboat:虚拟地址：

![image-20221105170650790](C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221105170650790.png)

**:dagger: 虚拟地址中的虚拟页号（VPN）对应的就是页表的索引**。

#### :yen:页表：

为了记录每一个虚拟页在物理内存中的位置，操作系统为每个进程保留一个数据结构——**页表（page table）,**页表的主要作用就是通过**内存管理单元** （*MMU*）为每个虚拟页面做地址转化。

<img src="https://img-blog.csdnimg.cn/08a8e315fedc4a858060db5cb4a654af.png" alt="img" style="zoom:50%;" />

#### :ear_of_rice:页表存在的位置：

假设有一个32位的地址空间，每个页有4KB`(2^12)`，那么该每个进程的虚拟页面分为2^20个页面，所以一个虚拟地址的**虚拟页号（VPN）**占`2^20`位，**页内偏移量（offset）**为`2^12`位。因为VPN=2^20，所以操作系统必须为每个进程管理2^20个地址转化换（页表项），一个**页表项（PTE）**32bit（4byte），所以一个页表大小为4MB。一个进程就占4MB，那么100个进程就占400MB，这是相当大的一块内存空间。

由于**操作系统的内存**本身也可以虚拟化，因此，页表多存于**操作系统的虚拟内存**中（甚至可以交换到磁盘上）。

#### :sailboat:页表项中到底有些什么？（物理页号 + 各种控制位）

![image-20221105164430623](C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221105164430623.png)

页表的作用就是将虚拟地址（虚拟页号）映射到物理地址（物理帧号），因此可以采用任何数据结构。最简单的形式就是数组，操作系统通过虚拟页号`VPN`定位到页表项`PTE`，之后通过`PTE`定位到物理帧号`PFN`，最后在通过虚拟地址上的偏移量就可以定位到物理地址。

对于每个PTE，除了页帧号还存在许多不同的位，比如：

- **保护位：**表明页是否可读，可写，当以非法的方式访问这些页时，操作系统会陷入内核
- **存在位：**表明该页是否在物理内存中，因为长时间为使用的页会被换出磁盘
- **脏位：**表明该页被换入内存后是否被修改过
- **访问位：**用于追踪哪些页被访问过以及被访问的次数，这对于页面置换十分有用
- **外存地址：**当发生缺页中断时会发生磁盘IO请求页面
- **……**

对于不同的硬件平台，每个页表项内容可能存在差异。

#### :lantern:我们如何找到页表？

为了进行地址转化，当一条虚拟地址生成以后，虚拟地址的VPN在经过MMU运算以后就会转化为页表中的索引，所以找到页表的基址很重要。硬件中有一个**页表寄存器（PTR）**，包含了**页表基址的物理地址**和**页表长度，** **进程未运行时，页表的起始地址和页表长度放在进程控制块（PCB）中**，当进程被调度时，操作系统就会将它们放入PTR。

**以下是虚拟地址转化为逻辑地址的基本流程：**

<img src="https://img-blog.csdnimg.cn/20200503223548369.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:50%;" />

#### :taco:TLB（快表）:

页表是虚拟内存和物理内存的桥梁，当我们访问物理内存时，都必须过页表这一关，由于页表是存于操作系统内核的内存中的，所以每次访问都会额外读一次内存。在CPU流水线中，访存阶段是十分耗时的（一般要十多个时钟周期），我们要尽可能减少内存的访问。所以引入了`旁路缓冲存储器（translation-lookaside-buffer）`，**（TLB也成为快表），**这一硬件结构来**缓存最常访问的物理页所对应的页表项**，**来避免频繁的地址转换**，以提高内存访问速度。

:scorpion: **访问流程如下：**

操作系统得到虚拟地址后，提取VPN，然后拿着VPN去检查TLB，看是否能转化为对应的页表项（PTE）。如果转换成功（TLB命中），我们就可以从缓存的页表项提取出物理页帧号（PFN），假定页表项中的保护检查没有失效，就可以直接访问内存了。

如果TLB未命中，操作系统就会通过VPN解析出来的页帧号直接访问页表项，然后将VPN和其对应的PTE缓存在TLB中，下次再访问该地址时，就可以通过TLB直接找到物理地址。

```c
VPN = (VirtualAddress & VPN_MASK) >> SHIFT;//提取虚拟页号
 2 (Success, TlbEntry) = TLB_Lookup(VPN);//检查TLB
 3 if (Success == true)
 4 {
 5     if (CanAccess(TlbEntry.ProtectBits) == true)
 6     {
 7         Offset = VirtualAddress & OFFSET_MASK;//提取页内偏移量
 8         PhysAddr = (TlbEntry.PFN << SHIFT) | Offset;//计算物理地址
 9         Register = AccessMemory(PhysAddr);//访问内存，将内存写入寄存器
10     }  
11     else
12     {
13         RaiseException(PROTECTION_FAULT);//保护位异常
14     }
15 }
16 else
17 {
18     PTEAddr = PTBR + (VPN * sizeof(PTE));//计算页表项地址
19     PTE = AccessMemory(PTEAddr);//提取页表项
20     if (PTE.Valid == False)
21         RaiseException(SEGMENTATION_FAULT);
22     else if (CanAccess(PTE.ProtectBits) == false)
23         RaiseException(PROTECTION_FAULT);
24     else
25         TLB_Insert(VPN, PTE.PFN, PTE.ProtectBits);//将VPN和PTE.PFN的映射关系缓存起来
26         RetryInstruction();
27 }
```

<img src="https://img-blog.csdnimg.cn/20200505112605873.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:50%;" />

<img src="https://img-blog.csdnimg.cn/2020050511441885.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:50%;" />

**:joystick:尽可能利用缓存：**

- **时间局部性：**最近访问的指令或数据项可能会再次访问
- **空间局部性：**当程序访问地址x时，很可能访问它相邻的内存

比如我们在访问数组的时候，步长尽可能小，连续访问，因为相邻的内存单元处于同一个页面，这样在连续访问时就可以大量命中TLB。

<img src="https://img2020.cnblogs.com/blog/589642/202007/589642-20200725145633493-1983425843.png" alt="img" style="zoom: 50%;" />

**:fax:谁来处理TLB未命中？**

方式TLB未命中时，硬件系统会抛出一个异常，这时会暂停当前指令流，将权限提升至内核模式，跳转至中断处理程序，接下来会在内核模式下用“特权指令”更新TLB，更新完后返回。

:tada:**TLB中有什么？**

典型的TLB有32项，64项，128项，并且是全相联的。

一条TLB项的内容如下：

​																						**VPN | PFN｜其它位**

TLB通常还有一个有效位，用来标识是不是有效的地址转换，还有一些保护位，用来标识是否有访问权限，除此之外，还包括地址空间标识符，脏位等...

:kaaba:**进程上下文切换时如何对TLB进行处理？**

TLB中包含的虚拟地址到物理地址的映射**只对当前进程有效**。所以当进程切换时，硬件与操作系统必须确保即将运行的进程不要误读了当前进程的TLB。为了解决这一问题，有两种解决方案：

1. 在上下文切换时直接清空TLB，但是如果**进程频繁的切换，这种开销就很大**
2. 在TLB上**添加一个地址空间标识符**（Address Space Identifier，ASID）。可以将其看作进程标识符，通常比PID位数少，这样不同的进程就可以共享TLB了。

<img src="C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221105201342496.png" alt="image-20221105201342496" style="zoom:45%;" />

#### :martial_arts_uniform:多级页表：

**只用一级页表管理内存的缺陷：**

- **内部碎片：**单个页面内部存在内部内存碎片会造成一定浪费
- **生成页表开销大：**由于每一个进程对应一个页表（除父子进程外），当进程很多时，页表就会占据很大一部分内存空间
- **页表项（PTE）空间的浪费：**因为有很大一部分页表项中的内容都是未映射到物理内存中的，所以这些页表项也在占有内存



**为何引入多级页表？**

根据空间局部性原理可知，进程在一段时间内**只需访问某几个页面就可以正常运行了**，因此**没有必要将整个页表都常驻内存**，为了**去掉页表中所有无效的区域**，而不用将**所有页表项**都保留在内存中，引入了多级页表。

对于单页表的实现方式，在 32 位和页大小 `4KB` 的环境下，一个进程的页表需要装下 100 多万个「页表项」，并且每个页表项是占用 4 字节大小的，于是相当于**每个页表需占用 4MB 大小的空间，100个进程就是400MB的内存空间**。

我们把这个 100 多万个「页表项」的单级页表再分页，将页表（一级页表）分为 `1024` 个页表（二级页表），每个表（二级页表）中包含 `1024` 个「页表项」，形成**二级分页**，这样二级页表就不用立马被装入内存，只需将一级页表装入，当需要对应的二级页表项时再将该二级页表项中所在的页表装入内存即可。此时，每个进程初始时所对应的页表占用的内存就变为`4KB`,这大大节省了内存空间。

<img src="https://img-blog.csdnimg.cn/19296e249b2240c29f9c52be70f611d5.png" alt="img" style="zoom:40%;" />

**页表一定要覆盖全部虚拟地址空间，不分级的页表就需要有 100 多万个页表项来映射，而二级分页则只需要 1024 个页表项**（此时一级页表覆盖到了全部虚拟地址空间，二级页表在需要时创建）。

**对于 64 位的系统**，两级分页肯定不够了，就变成了四级目录，分别是：

- 全局页目录项 PGD（*Page Global Directory*）；
- 上层页目录项 PUD（*Page Upper Directory*）；
- 中间页目录项 PMD（*Page Middle Directory*）；
- 页表项 PTE（*Page Table Entry*）；

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/%E5%9B%9B%E7%BA%A7%E5%88%86%E9%A1%B5.png" alt="img" style="zoom:35%;" />

#### :octopus:段页式管理内存：

**分页和分段的优缺点分析：**

<img src="C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221106155200311.png" alt="image-20221106155200311" style="zoom:67%;" />



**段页式内存管理实现的方式（先分段再分页）：**

- 先将程序划分为多个有逻辑意义的段，也就是前面提到的分段机制；
- 接着再把每个段划分为多个页，也就是对分段划分出来的连续空间，再划分固定大小的页

这样，地址结构就由**段号、段内页号和页偏移量**三部分组成。

![image-20221106155628006](C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221106155628006.png)

段页式地址变换中要得到物理地址须经过三次内存访问：

- 第一次访问段表，得到页表起始地址；
- 第二次访问页表，得到物理页号；
- 第三次将物理页号与页内位移组合，得到物理地址。

**段页式管理存在的缺陷：**

分段是不够灵活的，如果有一个大而稀疏的堆，那么堆段再分页仍然会导致大量页表的浪费，而且分段仍然会导致外部碎片的产生。



#### :hammer_and_pick:请求分页存储管理：

<img src="C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221106160844835.png" alt="image-20221106160844835" style="zoom:60%;" />

**交换机制：**

交换空间让操作系统为多个并发的进程都提供了巨大地址空间的假象。当OS启动时，第一件事就是再**硬盘上**开辟一部分空间**用于物理页的换入和换出**，再OS中，一般称这样的空间为**交换空间（swap space）**，交换空间的大小决定了系统某一时刻能够使用最大内存页数。

**页表项中几个重要的字段：**

<img src="C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221106161735196.png" alt="image-20221106161735196" style="zoom:67%;" />

**缺页中断机构：**

在请求分页系统中，先从虚拟地址的VPN定位到PTE，检查PTE的状态为（存在位）是否为1，如果为0，说明该页不在内存中，此时就会**触发缺页中断机制（页错误）**。此时缺页的进程阻塞，将进程放入阻塞队列，启动I/O硬件，当磁盘I/O完成时，操作系统就会更新页表，将次页标记为存在，更新页表项的PFN字段以记录新获取页的内存位置，并重试请求指令（返回到用户进程）。

<img src="C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221106162522726.png" alt="image-20221106162522726" style="zoom:67%;" />

磁盘I/O的访问速度比内存访问慢10000~100000倍，**为了防止频繁的缺页中断，操作系统通过合理的页面置换算法来解决这一问题**。

:man_judge:**内存访问整体流程：**

![img](https://img-blog.csdnimg.cn/20200507222657919.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70)

## 4.页面置换算法

### I.最佳置换算法：

每次选择淘汰的页面将是**以后永不使用**，或者在**最长时间内不被访问的页面**，这样可以保证最低的缺页率。

但是只有进程的**运行过程中才能知道接下来访问哪个页面**，操作系统无法提前预判页面的访问序列，所以**该算法无法实现。**

### II.先进先出置换算法（FIFO）

每次选择淘汰的页面是**最早进入内存**的页面。

**实现方法：**将调入内存的页面根据调入的先后顺序排成一个队列，需要换出页面时选择队头页面即可。队列的最大长度取决于系统为进程分配了多少个内存块。

<img src="C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221106165741923.png" alt="image-20221106165741923" style="zoom:60%;" />

<img src="C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221106165758748.png" alt="image-20221106165758748" style="zoom:60%;" />

**Belady异常**——当为进程分配的物理块数增大时，缺页次数不减反增的异常现象。

只有FIFO算法会产生Belady异常，另外，FIFO虽然实现简单，但**最先进入的页面有可能会被频繁访问**，因此算法性能差。

### III.最近最久未使用置换算法（LRU）

最近最久未使用（*最近最少使用*，*LRU*）的置换算法的基本思路是，发生缺页时，**选择最长时间没有被访问的页面进行置换**，也就是说，该算法假设已经很久没有使用的页面很有可能在未来较长的一段时间内仍然不会被使用。

实现方法：在每个页面对应的页表项中，用**访问字段**记录该页面上次被访问以来所**经历的时间t**，当需要淘汰一个页面时，选择**t值最大**，即最近最久未使用。

![image-20221106170553162](C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221106170553162.png)

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95/LRU%E7%BD%AE%E6%8D%A2%E7%AE%97%E6%B3%95.png" alt="最近最久未使用的置换算法" style="zoom:57%;" />

虽然 LRU 在理论上是可以实现的，但代价很高。为了完全实现 LRU，需要在内存中维护一个所有页面的链表，最近最多使用的页面在表头，最近最少使用的页面在表尾。

困难的是，在每次访问内存时都必须要更新「整个链表」。在链表中找到一个页面，删除它，然后把它移动到表头是一个非常费时的操作。

所以，LRU 虽然看上去不错，但是由于开销比较大，实际应用中比较少使用。

### IV.时钟置换算法（CLOCK）

该算法的思路是，把所有的页面都保存在一个类似钟面的「环形链表」中，**一个指针指向最老的页面**。

当发生缺页中断时，算法首先检查表针指向的页面：

- 如果它的访问位位是 0 （表示最近没有访问过）就淘汰该页面，并把新的页面插入这个位置，然后把表针前移一个位置；
- 如果访问位是 1 （表示最近访问过）就清除访问位，并把表针前移一个位置，重复这个过程直到找到了一个访问位为 0 的页面为止；

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95/%E6%97%B6%E9%92%9F%E7%BD%AE%E6%8D%A2%E7%AE%97%E6%B3%95.png" alt="时钟页面置换算法" style="zoom:50%;" />

### V.最不经常使用算法（LFU）

**LFU（least frequently used）：**当发生缺页中断时，选择「访问次数」最少的那个页面，并将其淘汰。

**实现方式：**对每个页面设置一个「访问计数器」，每当一个页面被访问时，该页面的访问计数器就累加 1。在发生缺页中断时，淘汰计数器值最小的那个页面。

**缺陷：**

- 要增加一个计数器来实现，这个硬件成本是比较高的，另外如果要对这个计数器查找哪个页面访问次数最小，查找链表本身，如果链表长度很大，是非常耗时的，效率不高。
- LFU 算法只考虑了频率问题，没考虑时间的问题，如果某一个页面在某一时间段内被频繁使用，之后就很少被访问了，那么也会造成该页面无法及时被淘汰。问题的解决的办法还是有的，可以**定期减少页面访问计数器中记录的该页面的访问计数**，比如当发生缺页中断时，把过去时间访问的页面的访问计数除以 2，也就说，随着时间的流失，**以前的高访问次数的页面会慢慢减少**，相当于加大了被置换的概率。

### VI.改造型时钟置换算法：

简单的时钟置换算法只考虑到了一个页面最近是否被访问过，事实上，如果淘汰的页面**没有被修改过**，就不需要立即执行I/O操作写回外存。

因此，除了考虑一个页面最近是否被访问过以外，还应考虑是否被修改过。**在其它条件都相同时，优先淘汰被修改过的页面。**

用`访问位`和`修改位`的形式来表示页面的状态。（1，1）表示一个**页面最近被访问过且被修改**。

<img src="C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221106172516569.png" alt="image-20221106172516569" style="zoom: 67%;" />

- 第一优先级：最近未被访问且未被修改
- 第二优先级：最近未被访问，但修改过
- 第三优先级：最近访问过，但没有修改
- 第四优先级：最近访问过，且修改过

写比读容忍度低，**优先级越考前，越先被淘汰**

## 5.malloc()分配内存

### Linux进程内存模型

在 Linux 操作系统中，虚拟地址空间的内部又被分为**内核空间和用户空间**两部分，不同位数的系统，地址空间的范围也不同。比如最常见的 32 位和 64 位系统，如下所示：

<img src="https://img-blog.csdnimg.cn/img_convert/1db038e1d2e5325b05e2bb80475d962a.png" alt="图片" style="zoom: 67%;" />

虽然**每个进程都各自有独立的虚拟内存**，但是**每个虚拟内存中的内核地址，其实关联的都是相同的物理地址（内核物理地址共享）**。这样，进程切换到内核态后，就可以很方便地访问内核空间内存。

<img src="https://img-blog.csdnimg.cn/img_convert/c88bda5db60029f3ea57e4306e7da936.png" alt="图片" style="zoom: 50%;" />

用户空间内存从**低到高**分别是 6 种不同的内存段：

<img src="https://img-blog.csdnimg.cn/img_convert/7b5b6b3728acde8df019350df3cb85c1.png" alt="图片" style="zoom:40%;" />

- 程序文件段，包括二进制可执行代码；
- 已初始化数据段，包括静态常量；
- 未初始化数据段，包括未初始化的静态变量；
- 堆段，包括动态分配的内存，从**低地址开始向上增长**；
- 文件映射段，包括动态库、共享内存等，从低地址开始向上增长（跟硬件和内核版本有关 ）；
- 栈段，包括局部变量和函数调用的上下文等。栈的大小是固定的，一般是 `8 MB`，**高地址向下增长**。当然系统也提供了参数，以便我们自定义大小；

在这 6 个内存段中，堆和文件映射段的内存是动态分配的。比如说，使用 C 标准库的 `malloc()` 或者 `mmap()` ，就可以分别在堆和文件映射段动态分配内存。

### malloc 是如何分配内存的？

**malloc() 封装了底层系统调用，是 C 库函数，用于动态分配内存。**

malloc 申请内存的时候，会有两种方式向操作系统申请堆内存。

- 方式一：通过 **`brk()`** 系统调用从堆分配内存
- 方式二：通过 **`mmap()`** 系统调用在文件映射区域分配内存；

方式一：通过 **brk() 函数将「堆顶」指针向高地址移动**，获得新的内存空间。如下图：

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/brk%E7%94%B3%E8%AF%B7.png" alt="img" style="zoom:50%;" />

方式二：通过 **mmap()** 系统调用中「私有匿名映射」的方式，在文件映射区分配一块内存，也就是从文件映射区“偷”了一块内存。如下图：

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/mmap%E7%94%B3%E8%AF%B7.png" alt="img" style="zoom: 50%;" />

malloc() 源码里默认定义了一个阈值：

- 如果用户分配的内存**小于 128 KB**，则通过 **brk()** 申请内存；
- 如果用户分配的内存**大于 128 KB**，则通过 **mmap()** 申请内存；

注意，不同的 glibc 版本定义的阈值也是不同的。

### free 释放内存，会归还给操作系统吗？

- malloc 通过 **brk()** 方式申请的内存，free 释放内存的时候，**并不会把内存归还给操作系统，而是缓存在 malloc 的内存池中，待下次使用**；
- malloc 通过 **mmap()** 方式申请的内存，free 释放内存的时候，**会把内存归还给操作系统，内存得到真正的释放**。

### 为什么不全部使用mmap()来分配内存？

- 每次mmap()分配内存都必然会发生系统调用，即CPU会从用户态切换为核心态
- mmap()分配的内存每次释放完后都会还给操作系统，所以mmap()分配地址都会触发缺页中断，CPU消耗过大

所以，为了减少系统调用和缺页中断的发生，通过brk()分配内存也是一种好办法。

### 为什么不全部使用brk()分配内存？

因为释放brk()分配的内存不会真正的释放，而是缓存在malloc()缓存池中。如果我们连续申请了 10k，20k，30k 这三片内存，如果 10k 和 20k 这两片释放了，变为了空闲内存空间，如果下次申请的内存小于 30k，那么就可以重用这个空闲内存空间。但如果申请的内内大于30K，内存分配指针就会继续往堆顶移动，如果一直发生这种情况，很可能造成内存泄漏。

### free() 函数只传入一个内存地址，为什么能知道要释放多大的内存？

还记得，我前面提到， malloc 返回给用户态的内存起始地址比进程的堆空间起始地址多了 16 字节吗？

这个多出来的 16 字节就是保存了该内存块的描述信息，比如有该内存块的大小。

![图片](https://img-blog.csdnimg.cn/img_convert/cb6e3ce4532ff0a6bfd60fe3e52a806e.png)

这样当执行 free() 函数时，free 会对传入进来的内存地址向左偏移 16 字节，然后从这个 16 字节的分析出当前的内存块的大小，自然就知道要释放多大的内存了。

# 四、文件管理

##  1.文件和目录

存储和虚拟化有两个关键抽象：

- **文件**：文件就是一个线性的字节数组，每个数组都可以读取或写入，每个文件都有一个inode号与其关联
- **目录**：目录与文件一样，也有一个inode号与其关联。目录是一个列表，该列表的每个条目就是目录项，该**目录项指向了其它文件或目录。**

用户可以通过目录和文件的关系构建**目录树。**

Linux 最经典的一句话是：「**一切皆文件**」，不仅**普通的文件和目录**，就连**块设备、IO设备、U盘、管道、socket** 等，也都是统一交给文件系统管理的。

## 2.文件系统接口

![img](https://img-blog.csdnimg.cn/20200522121606452.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70)

在调用文件系统接口时有一个重要的概念就是**文件描述符，**它是一个整数，是每个**进程私有的**，在UNIX系统中用于访问文件。

以下是一条`cat`命令**查看文件的过程，该过程涉及到了内存分配，文件的打开、读写、关闭等系统调用**。

```bash
[root@gcs100 c_dir]# strace cat hello.c
execve("/usr/bin/cat", ["cat", "hello.c"], 0x7ffc0721e0a8 /* 48 vars */) = 0
brk(NULL)                               = 0x20ed000
mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f93cf1de000
access("/etc/ld.so.preload", R_OK)      = -1 ENOENT (没有那个文件或目录)
open("/etc/ld.so.cache", O_RDONLY|O_CLOEXEC) = 3
fstat(3, {st_mode=S_IFREG|0644, st_size=90460, ...}) = 0
mmap(NULL, 90460, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7f93cf1c7000
close(3)                                = 0
open("/lib64/libc.so.6", O_RDONLY|O_CLOEXEC) = 3
read(3, "\177ELF\2\1\1\3\0\0\0\0\0\0\0\0\3\0>\0\1\0\0\0`&\2\0\0\0\0\0"..., 832) = 832
fstat(3, {st_mode=S_IFREG|0755, st_size=2156592, ...}) = 0
mmap(NULL, 3985920, PROT_READ|PROT_EXEC, MAP_PRIVATE|MAP_DENYWRITE, 3, 0) = 0x7f93cebf0000
mprotect(0x7f93cedb4000, 2093056, PROT_NONE) = 0
mmap(0x7f93cefb3000, 24576, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_DENYWRITE, 3, 0x1c3000) = 0x7f93cefb3000
mmap(0x7f93cefb9000, 16896, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_ANONYMOUS, -1, 0) = 0x7f93cefb9000
close(3)                                = 0
mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f93cf1c6000
mmap(NULL, 8192, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f93cf1c4000
arch_prctl(ARCH_SET_FS, 0x7f93cf1c4740) = 0
access("/etc/sysconfig/strcasecmp-nonascii", F_OK) = -1 ENOENT (没有那个文件或目录)
access("/etc/sysconfig/strcasecmp-nonascii", F_OK) = -1 ENOENT (没有那个文件或目录)
mprotect(0x7f93cefb3000, 16384, PROT_READ) = 0
mprotect(0x60b000, 4096, PROT_READ)     = 0
mprotect(0x7f93cf1df000, 4096, PROT_READ) = 0
munmap(0x7f93cf1c7000, 90460)           = 0
brk(NULL)                               = 0x20ed000
brk(0x210e000)                          = 0x210e000
brk(NULL)                               = 0x210e000
open("/usr/lib/locale/locale-archive", O_RDONLY|O_CLOEXEC) = 3
fstat(3, {st_mode=S_IFREG|0644, st_size=106172832, ...}) = 0
mmap(NULL, 106172832, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7f93c86ae000
close(3)                                = 0
fstat(1, {st_mode=S_IFCHR|0620, st_rdev=makedev(136, 1), ...}) = 0
open("hello.c", O_RDONLY)               = 3
fstat(3, {st_mode=S_IFREG|0644, st_size=90, ...}) = 0
fadvise64(3, 0, 0, POSIX_FADV_SEQUENTIAL) = 0
read(3, "#include <stdio.h>\n   \n   \n int "..., 65536) = 90
write(1, "#include <stdio.h>\n   \n   \n int "..., 90#include <stdio.h>
    
 int main()
 { 
  	printf("hello world!\n");
	return 0;    
 }
) = 90
read(3, "", 65536)                      = 0
close(3)                                = 0
close(1)                                = 0
close(2)                                = 0
exit_group(0)                           = ?
+++ exited with 0 +++
```

除了基本的读写、打开、关闭系统调用以外，还有以下几个**重要的系统调用函数：**

- **重命名函数**：

```c
rename(char* old, char* new);
```

- **获取文件信息：**

**stat()或fstat()系统调用**

以下是stat这个shell命令调用fstat()函数的过程：

```bash
[root@gcs100 c_dir]# strace stat hello1.c
............
fstat(3, {st_mode=S_IFREG|0755, st_size=402384, ...}) = 0
............
文件："hello1.c"
  大小：90        	块：8          IO 块：4096   普通文件
设备：803h/2051d	Inode：50991484    硬链接：1
权限：(0644/-rw-r--r--)  Uid：(    0/    root)   Gid：(    0/    root)
环境：unconfined_u:object_r:admin_home_t:s0
最近访问：2022-11-07 17:18:00.714788023 +0800
最近更改：2022-09-18 20:22:58.177924146 +0800
最近改动：2022-11-07 17:23:23.069229279 +0800
创建时间：-
```

- **创建目录：**

**mkdir()函数：**对应mkdir命令

- **立即写入**：

**fsync()系统调用：**当调用write()时，只是将写入的数据块写内核缓冲区内`（Linux OS的Page Cache）`,而调用fsync()会强制将所有数据立刻写入磁盘。

## 3.文件的逻辑结构

### I.无结构文件

无结构文件:文件内部的数据就是一系列二进制流或字符流组成。又称`“流式文件”`。

如: Windows 操作系统中的 .txt 文件。

![image-20221108162944010](C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221108162944010.png)

### II.有结构文件

有结构文件：由一组相似的记录组成，又称”记录式文件“。每条记录又由若干个数据项组成。如数据库表文件，一般来说，有一个数据项可以作为关键字（主键id）。

### 有结构文件的逻辑结构：

#### :satisfied:顺序文件：

<img src="https://img-blog.csdnimg.cn/20200522130936736.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom: 50%;" />

<img src="https://img-blog.csdnimg.cn/20200522131914765.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:50%;" />

#### :unlock: 索引文件

数据库的索引，聚簇索引和唯一索引等，索引表本身就是定长记录的顺序文件。

![img](https://img-blog.csdnimg.cn/20200522132434390.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70)

#### :yen: 索引顺序文件

比如innodb存储引擎中的索引文件是一个B+树数据结构，每一个叶子节点存放数据，但并非每个节点存放一条数据，而是存放一页数据，每页有多条记录。

<img src="https://img-blog.csdnimg.cn/20200522133637290.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom: 60%;" />

#### :rabbit: 多级索引顺序文件

<img src="https://img-blog.csdnimg.cn/20200522135535103.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:57%;" />

## 4.文件目录

一个文件对应一个**文件控制块（FCB）**，一个FCB就是一个目录项，多个FCB组成文件目录。可以对目录中的文件进行查询、创建、删除、显示、修改操作。

### I.文件控制块

文件控制块中包含了文件的基本信息（文件名、磁盘地址、存取权限等）。FCB实现了**文件名与文件之间的映射**。

<img src="C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221108164649774.png" alt="image-20221108164649774" style="zoom:67%;" />

### II.单级目录结构

<img src="https://img-blog.csdnimg.cn/20200523125322687.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:57%;" />

### III.两级目录结构

<img src="https://img-blog.csdnimg.cn/20200523125523952.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:52%;" />

### IV.多级目录结构(树形目录结构)

<img src="https://img-blog.csdnimg.cn/20200523130042443.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:50%;" />

<img src="C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221108165053969.png" alt="image-20221108165053969" style="zoom:60%;" />

### **V.无环图目录结构解决文件共享**

<img src="https://img-blog.csdnimg.cn/20200523130642259.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:57%;" />

### VI.索引节点(FCB的改进)瘦身

<img src="https://img-blog.csdnimg.cn/20200523131601605.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:50%;" />

<img src="C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221108165500690.png" alt="image-20221108165500690" style="zoom:70%;" />

## 5.虚拟文件系统的实现

文件系统的种类众多，而操作系统希望**对用户提供一个统一的接口**，于是在用户层与文件系统层引入了中间层，这个中间层就称为**虚拟文件系统（Virtual File System，VFS）。**

VFS 定义了一组所有文件系统都支持的数据结构和标准接口，这样程序员不需要了解文件系统的工作原理，只需要了解 VFS 提供的统一接口即可。

在 Linux 文件系统中，用户空间、系统调用、虚拟文件系统、缓存、文件系统以及存储之间的关系如下图：

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E8%99%9A%E6%8B%9F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F.png" alt="img" style="zoom:50%;" />

Linux 支持的文件系统也不少，根据存储位置的不同，可以把文件系统分为三类：

- *磁盘的文件系统*，它是直接把数据存储在磁盘中，比如 Ext 2/3/4、XFS 等都是这类文件系统。
- *内存的文件系统*，这类文件系统的数据不是存储在硬盘的，而是占用内存空间，我们经常用到的 `/proc` 和 `/sys` 文件系统都属于这一类，读写这类文件，实际上是读写内核中相关的数据。
- *网络的文件系统*，用来访问其他计算机主机数据的文件系统，比如 NFS、SMB 等等。

### I.整体组织：

**磁盘分块：**虚拟文件系统（VSFS）在磁盘上上是连续存储的。所以我们要将磁盘分块（block），可以将构建文件系统的磁盘分区看多一系列固定大小的块。

**虚拟文件系统的重要组件：**

- **数据块（D）**：任何文件系统大多数空间都存储用户数据，所以将这些空间称为**数据区域**`data region`。
- **inode表（III……）**：文件系统需要记录每个文件的信息，比如文件包含哪些数据块，文件大小，文件访问权限等。为了存储这些信息，文件系统通常有一个inode的结构，所以要将文件系统中的一部分块预留出来**做为inode表**，它是由多个inode组成的。
- **位图（i,d）**：有了数据块和inode表，还需要有一种结构来记录inode是否空闲或已分配，分配记录的方法有多种，一种流行且简便的方法就是位图法。所以需要一部分块来做为**数据位图**`data bitmap`和**inode位图**`(inode bitmap)`
- **超级块（S）**：最后还需一个结构——超级块，其记录了该文件系统的一些特殊信息，例如文件系统inode的数量，数据块的数量，inode表的起始位置，文件系统版本信息等，因此，挂载文件系统时操作系统首先会读取超级块，初始化各种参数，在将各个文件添加到文件系统树上。

![image-20221108171551584](C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221108171551584.png)

### II.inode:

inode（索引节点）是文件系统最重要的磁盘结构之一，几乎所有文件系统都有这个结构。

**inode的结构：**

- inumber：每个inode都由一个数字（inumber）隐式引用。根据inumber，可以直接计算出其对应inode在磁盘块中的位置，即inode所在的磁盘块号，最终通过计算可找到inode的具体位置。
- 元数据：在每个inode中，实际上还有许多关于文件的信息：文件类型、大小、分配的块数、保护信息、时间信息、文件在磁盘中的具体位置，这些信息都被称为**元数据。**
- 多级索引：为支持更大文件，须在inode中引入一个间接指针，它指向包含更多指针的块，这些块中的指针**指向用户数据（正在的文件）**。如果文件系统足够大，则会分配一个间接块（来自磁盘的数据块区域），并将inode的简介指针指向它。

<img src="C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221108173845514.png" alt="image-20221108173845514" style="zoom:57%;" />

## 6.软链接和硬链接

### I.硬链接：

硬链接是**多个目录项中的「索引节点」指向一个文件**，也就是**指向同一个 inode（共用一个inode）**，但是 inode 是不可能跨越文件系统的，每个文件系统都有各自的 inode 数据结构和列表，所以**硬链接是不可用于跨文件系统的**。当每创建一个硬链接，文件的inode号的引用计数加1，当删除一个硬链接或源文件时，inode的引用计数减一，**所以只有删除文件的所有硬链接以及源文件时，系统才会彻底删除该文件。**

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E7%A1%AC%E9%93%BE%E6%8E%A5-2.png" alt="硬链接" style="zoom:50%;" />

a1是a的硬链接，可见inode号一样：

```bash
[root@gcs101 /]# ls -i a
76 a
[root@gcs101 /]# ls -i a1
76 a1
```

以下是删除一个硬链接，inode引用计数减一：

```bash
[root@gcs101 /]# stat a
  文件："a"
设备：803h/2051d	Inode：76          硬链接：2
[root@gcs101 /]# rm a1
rm：是否删除普通空文件 "a1"？y
[root@gcs101 /]# stat a
  文件："a"
设备：803h/2051d	Inode：76          硬链接：1
```

### II.软链接：

软链接又称符号链接，软链接相当于重新创建一个文件，这个文件有**独立的 inode**，但是这个**文件的内容是另外一个文件的路径**，所以访问软链接的时候，实际上相当于**访问到了另外一个文件**，所以**软链接是可以跨文件系统的**，甚至**目标文件被删除了，链接文件还是在的，只不过指向的文件找不到了而已。**

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E8%BD%AF%E9%93%BE%E6%8E%A5.png" alt="软链接" style="zoom: 50%;" />

## 7.文件IO

### I.缓冲与非缓冲 I/O

- 缓冲 I/O，利用的是标准库的缓存实现文件的加速访问，而标准库再通过系统调用访问文件，实际上就是在标准库函数中开辟一个**buffer[]数组**来缓存读取数据，**按块读取和写入**。
- 非缓冲 I/O，直接通过系统调用访问文件，不经过标准库缓存，按字节读取。

这里所说的「缓冲」特指标准库内部实现的缓冲。

比方说，很多程序遇到换行时才真正输出，而换行前的内容，其实就是被标准库暂时缓存了起来，这样做的目的是，**减少系统调用的次数**，毕竟系统调用是有 CPU 上下文切换的开销的。

### II.直接与非直接 I/O

磁盘I/O速度十分缓慢，为了减少磁盘I/O次数，操作系统会将读取到的文件数据先缓存在内核空间（Page Cache），那么，**根据是「否利用操作系统的缓存」，可以把文件 I/O 分为直接 I/O 与非直接 I/O**：

- 直接 I/O，不会发生内核缓存和用户程序之间数据复制，而是直接经过文件系统访问磁盘。
- 非直接 I/O，读操作时，数据从**内核缓存中拷贝给用户程序**，写操作时，数据从**用户程序拷贝给内核缓存**，再由内核决定什么时候写入数据到磁盘。

如果你在使用文件操作类的系统调用函数时，指定了 `O_DIRECT` 标志，则表示使用直接 I/O。如果没有设置过，默认使用的是非直接 I/O。

以下几种场景会触发内核缓存的数据写入磁盘：

- 在调用 `write` 的最后，当发现内核缓存的数据太多的时候，内核会把数据写到磁盘上；
- 用户主动调用 `sync`，内核缓存会刷到磁盘上；
- 当内存十分紧张，无法再分配页面时，也会把内核缓存的数据刷到磁盘上；
- 内核缓存的数据的缓存时间超过某个时间时，也会把数据刷到磁盘上；

### III.阻塞与非阻塞 I/O VS 同步与异步 I/O

#### **阻塞I/O：**

当用户程序执行read()系统调用，用户程序陷入内核进入**阻塞等待**

**阻塞等待的是「内核数据准备好」和「数据从内核空间拷贝到用户空间」这两个过程**。这个过程中**CPU将会一直被该线程占用**。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E9%98%BB%E5%A1%9E%20I_O.png" alt="阻塞 I/O" style="zoom: 67%;" />

#### 非阻塞I/O：

非阻塞的 read 请求在数据未准备好的情况下立即返回，继续往下执行，在执行过程中不断的轮询内核，直到数据准备好，内核将数据拷贝到应用程序缓冲区，`read` 调用才可以获取到结果。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E9%9D%9E%E9%98%BB%E5%A1%9E%20I_O%20.png" alt="非阻塞 I/O" style="zoom:67%;" />

注意，**这里最后一次 read 调用，获取数据的过程，是一个同步的过程，是需要等待的过程。这里的同步指的是内核态的数据拷贝到用户程序的缓存区这个过程。**

#### 基于非阻塞的I/O多路复用：

I/O多路复用实际上是一种时间驱动型I/O模型， select、poll，它是通过 I/O 事件分发，当内核数据准备好时，再以事件通知应用程序进行操作。当调用select()函数的时候，用户程序将会阻塞，但此时CPU可以被其它线程使用，当内核准备好数据以后，会分发一个读取事件给应用程序，CPU发生中断，返回到原来阻塞的线程，然后它开始通过read（）将数据从内核拷贝会应用程序，拷贝完成后阻塞结束。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%9F%BA%E4%BA%8E%E9%9D%9E%E9%98%BB%E5%A1%9E%20I_O%20%E7%9A%84%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8.png" alt="I/O 多路复用" style="zoom: 67%;" />

实际上，无论是阻塞 I/O、非阻塞 I/O，还是基于非阻塞 I/O 的多路复用**都是同步调用。因为它们在 read 调用时，内核将数据从内核空间拷贝到应用程序空间，过程都是需要等待的，也就是说这个过程是同步的，如果内核实现的拷贝效率不高，read 调用就会在这个同步过程中等待比较长的时间。**

#### 异步I/O(AIO)：

**异步 I/O** 是**数据准备**和**数据从内核空间拷贝到用户空间**这两个过程都不用等待。当发起 `aio_read` 之后，就立即返回，程序继续往下执行，内核自动将数据准备好并将数据从内核空间拷贝到应用程序空间，拷贝完成之后发送一个通知给应用程序开始处理数据。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%BC%82%E6%AD%A5%20I_O.png" alt="异步 I/O" style="zoom:67%;" />

## 8.Page Cache

### I.概念：

Page Cache 的本质是由 **Linux 内核管理的内存区域**。我们通过 mmap 以及 buffered I/O 将文件读取到内存空间实际上都是读取到 Page Cache 中。

<img src="https://img-blog.csdnimg.cn/img_convert/72568a29816fa9b505f15edac68adee2.png" alt="img" style="zoom:50%;" />

### II.查看系统Page Cache：

```shell
[root@gcs100 c_dir]# cat /proc/meminfo 
MemTotal:        1863028 kB
MemFree:           82256 kB
MemAvailable:     555884 kB
Buffers:               4 kB #
Cached:           631516 kB #
SwapCached:        18400 kB #交换区缓存
Active:           794720 kB
Inactive:         617008 kB
Active(anon):     510760 kB #活跃匿名页
Inactive(anon):   327720 kB #非活跃匿名页
Active(file):     283960 kB #活跃文件备份页
Inactive(file):   289288 kB #非活跃文件备份页
Unevictable:           0 kB
Mlocked:               0 kB
SwapTotal:       2097148 kB
SwapFree:        1503996 kB
Dirty:             20432 kB
Writeback:             0 kB
AnonPages:        770132 kB
Mapped:            95520 kB
Shmem:             58268 kB #
Slab:             178972 kB
SReclaimable:      62956 kB
…………
```

根据上面的数据，可以得出这样的公式：

```text
Buffers + Cached + SwapCached = Active(file) + Inactive(file) + Shmem + SwapCached
```

两边等式都是 Page Cache

```text
Page Cache = Buffers + Cached + SwapCached 
```

### III.page 与 Page Cache

为了实现内存管理，物理内存一般会被分为数据页`（page）`，page 是内存管理分配的基本单位， Page Cache 由多个 page 构成。page 在操作系统中通常为 4KB 大小（32bits/64bits），而 Page Cache 的大小则为 4KB 的整数倍。

**另一方面，并不是所有 page 都被组织为 Page Cache**。

Linux 系统上供**用户可访问的内存**分为两个类型，即：

- **File-backed pages：**文件备份页也就是 Page Cache 中的 page，对应于**磁盘上的若干数据块**；对于这些页最大的问题是脏页回盘；
- **Anonymous pages：**匿名页不对应磁盘上的任何磁盘数据块，它们是**进程的运行是内存空间**（例如方法栈、局部变量表等属性）；

文件备份页是暂存磁盘内文件的，比如我们编辑word文件、向数据库表文件中读取一条记录，此时这些数据就可能被暂存在文件备份页中。

匿名页就是为进程准备的，进程栈区、堆区等都是要占用内存的，当运行时它们的内存就在匿名页中分配，当我们用malloc()分配内存时，分配的也是匿名页内存空间。

- File-backed pages（Page Cache）的内存回收代价较低。Page Cache 通常对应于一个文件上的若干顺序块，因此可以通过顺序 I/O 的方式落盘。另一方面，如果 Page Cache 上没有进行写操作（所谓的没有脏页），甚至不会将 Page Cache 回盘，因为数据的内容完全可以通过再次读取磁盘文件得到。
- Anonymous pages 的内存回收代价较高。这是因为 Anonymous pages 通常随机地写入持久化交换设备。另一方面，无论是否有写操作，为了确保数据不丢失，Anonymous pages 在 swap 时必须持久化到磁盘。

### IV.Swap与缺页中断

Swap 机制指的是当物理内存不够用，内存管理单元（Memory Mangament Unit，MMU）需要提供调度算法来回收相关内存空间，然后将清理出来的内存空间给当前内存申请方。

但主内存的空间是有限的，当主内存中不包含可以使用的空间时，操作系统会从选择合适的物理内存页驱逐回磁盘，为新的内存页让出位置，**选择待驱逐页的过程在操作系统中叫做页面替换（Page Replacement）**，**替换操作又会触发 swap 机制**。一句话，**Swap机制是操作系统内存调度算法的实现**。

Linux 通过一个 swappiness 参数来控制 Swap 机制：这个参数值可为 0-100，控制系统 swap 的优先级：

- 高数值：较高频率的 swap，进程不活跃时主动将其转换出物理内存。
- 低数值：较低频率的 swap，这可以确保交互式不因为内存空间频繁地交换到磁盘而提高响应延迟。

:earth_africa: **为什么 SwapCached 也是 Page Cache 的一部分？**

这是因为当匿名页（Inactive(anon) 以及 Active(anon)）先被交换（swap out）到磁盘上后，然后再加载回（swap in）内存中，由于读入到内存后原来的 Swap File 还在，所以 SwapCached 也可以认为是 File-backed page，即属于 Page Cache。这个过程如下图所示:

<img src="https://img-blog.csdnimg.cn/img_convert/cbba24cac4668625c4e32d8cf641cf9c.png" alt="图片" style="zoom:67%;" />

### V.Page Cache 与 buffer cache

执行 `free -m`命令，注意到会有两列名为 buffers 和 cached。

```bash
~ free -m
             total       used       free     shared    buffers     cached
Mem:        128956      96440      32515          0       5368      39900
Swap:        16002          0      16001
```

cached 列表示当前的页缓存（Page Cache）占用量，buffers 列表示当前的**块缓存（buffer cache）**占用量。

**Page Cache 用于缓存文件的页数据，buffer cache 用于缓存块设备（如磁盘）的块数据。**

- 页是逻辑上的概念，因此` Page Cache `是**与文件系统同级**的；
- 块是物理上的概念，因此 `buffer cache` 是与**块设备驱动程序同级**的。

Page Cache 与 buffer cache 的**共同目的都是加速数据 I/O**：

- 写数据时首先写到缓存，将写入的页标记为 dirty，然后向外部存储 flush，也就是缓存写机制中的 write-back（另一种是 write-through，Linux 默认情况下不采用）；
- 读数据时首先读取缓存，如果未命中，再去外部存储读取，并且将读取来的数据也加入缓存。操作系统总是积极地将所有空闲内存都用作 Page Cache 和 buffer cache，当内存不够用时也会用 LRU 等算法淘汰缓存页。

在 Linux 2.4 版本的内核之前，Page Cache 与 buffer cache 是完全分离的。但是，块设备大多是磁盘，磁盘上的数据又大多通过文件系统来组织，这种设计导致很多数据被缓存了两次，浪费内存。**所以在 2.4 版本内核之后，两块缓存近似融合在了一起：如果一个文件的页加载到了 Page Cache，那么同时 buffer cache 只需要维护块指向页的指针就可以了**。只有那些没有文件表示的块，或者绕过了文件系统直接操作（如dd命令）的块，才会真正放到 buffer cache 里。因此，我们现在提起 **Page Cache，基本上都同时指 Page Cache 和 buffer cache 两者**，本文之后也不再区分，直接统称为 Page Cache。

```bash
[root@gcs100 c_dir]# free -bkm
              total        used        free      shared  buff/cache   available
Mem:           1819         775         232          52         812         835
Swap:          2047         596        1451
```

下图近似地示出 32-bit Linux 系统中可能的一种 Page Cache 结构，其中 block size 大小为 1KB，page size 大小为 4KB。

<img src="https://img-blog.csdnimg.cn/img_convert/c81ffa0b7d11506ffad3c33001385444.png" alt="图片" style="zoom: 50%;" />

### VI.Page Cache 与预读

操作系统为基于 Page Cache 的读缓存机制提供**预读机制**（PAGE_READAHEAD），一个例子是：

- 用户线程仅仅请求读取磁盘上文件 A 的 offset 为 0-3KB 范围内的数据，由于磁盘的基本读写单位为 block（4KB），于是操作系统至少会读 0-4KB 的内容，这恰好可以在一个 page 中装下。
- 但是操作系统出于局部性原理会选择将磁盘块 offset [4KB,8KB)、[8KB,12KB) 以及 [12KB,16KB) 都加载到内存，于是额外在内存中申请了 3 个 page；

下图代表了操作系统的预读机制：

<img src="https://img-blog.csdnimg.cn/img_convert/ae8252378169c8c14b8b9907983f7d8b.png" alt="img" style="zoom:67%;" />

上图中，应用程序利用 read **系统调动读取 4KB 数据**，实际上内核使用 **readahead 机制完成了 16KB 数据的读取**。

### VII.Page Cache 与文件持久化的一致性&可靠性

任何系统引入缓存，就会引发一致性问题：内存中的数据与磁盘中的数据不一致，例如常见后端架构中的 Redis 缓存与 MySQL 数据库就存在一致性问题。

**文件 = 数据 + 元数据**。元数据用来描述文件的各种属性，也必须存储在磁盘上。因此，我们说保证文件一致性其实包含了两个方面：数据一致+元数据一致。

当前 Linux 下以两种方式实现文件一致性：

1. **Write Through（写穿）**：向用户层提供特定接口，应用程序可主动调用接口来保证文件一致性；
2. **Write back（写回）**：系统中存在定期任务（表现形式为内核线程），周期性地同步文件系统中文件脏数据块，这是默认的 Linux 一致性方案；

上述两种方式最终都依赖于系统调用，主要分为如下三种系统调用：

| 方法              | 含义                                                         |
| :---------------- | :----------------------------------------------------------- |
| fsync(int fd)     | fsync(fd)：将 fd 代表的文件的脏数据和脏元数据全部刷新至磁盘中。 |
| fdatasync(int fd) | fdatasync(fd)：将 fd 代表的文件的脏数据刷新至磁盘，同时对必要的元数据刷新至磁盘中，这里所说的必要的概念是指：对接下来访问文件有关键作用的信息，如文件大小，而文件修改时间等不属于必要信息 |
| sync()            | sync()：则是对系统中所有的脏的文件数据元数据刷新至磁盘中     |

Write Through 与 Write back 在持久化的可靠性上有所不同：

- Write Through 以**牺牲系统 I/O 吞吐量**作为代价，向上层应用确保一旦写入，数据就已经落盘，不会丢失；
- Write back 在系统发生宕机的情况下无法确保数据已经落盘，因此存在数据丢失的问题。不过，在程序挂了，例如被 kill -9，Page Cache 中的数据操作系统还是会确保落盘；

### VIII.Page Cache 的优劣势

#### Page Cache 的优势

**1.加快数据访问**

如果数据能够在内存中进行缓存，那么下一次访问就不需要通过磁盘 I/O 了，直接命中内存缓存即可。

**2.减少 I/O 次数，提高系统磁盘 I/O 吞吐量**

得益于 Page Cache 的缓存以及**预读能力**，而程序又往往**符合局部性原理**，因此通过一次 I/O 将多个 page 装入 Page Cache 能够减少磁盘 I/O 次数， 进而提高系统磁盘 I/O 吞吐量。

### Page Cache 的劣势

- 最直接的缺点是需要占用额外物理内存空间，物理内存在比较紧俏的时候可能会导致频繁的 swap 操作，最终导致系统的磁盘 I/O 负载的上升。
- Page Cache 的另一个缺陷是对应用层并没有提供很好的管理 API，几乎是透明管理。应用层即使想优化 Page Cache 的使用策略也很难进行。因此一些应用选择在用户空间实现自己的 page 管理，而不使用 page cache，例如 MySQL InnoDB 存储引擎以 16KB 的页进行管理。
- Page Cache 最后一个缺陷是在某些应用场景下比 Direct I/O(直接从磁盘读到用户空间) 多一次磁盘读 I/O 以及磁盘写 I/O。

## 9.五种Unix I/O

### 1.阻塞IO：

应用进程被阻塞，直到数据复制到应用进程缓冲区中才返回。

### 2.非阻塞IO:

应用进程执行系统调用之后，内核返回一个错误码。应用进程可以继续执行，但是需要不断的执行系统调用来获知 I/O 是否完成，这种方式称为轮询(polling)。

### 3.IO多路复用（事件驱动型I/O）:

使用 select 或者 poll 等待数据，并且可以等待多个套接字中的任何一个变为可读，这一过程会被阻塞，当某一个套接字可读时返回。之后再使用 recvfrom 把数据从内核复制到进程中。

使得一个进程具有处理多个IO事件的能力**(事件驱动型I/O)**，并且处理事件的线程可以复用，减少了线程反复创建的开销

![img](https://pdai.tech/_images/io/java-io-model-2.png)

### 4.信号驱动IO：

应用进程使用 **sigaction()** 系统调用，内核立即返回，应用进程可以继续执行，也就是说等待数据阶段应用进程是非阻塞的。内核在数据到达时向应用进程发送 SIGIO 信号，应用进程收到之后在信号处理程序中调用 recvfrom 将数据从内核复制到应用进程中。与非阻塞IO相比，CPU利用率更高

<img src="https://pdai.tech/_images/io/java-io-model-3.png" alt="img" style="zoom: 80%;" />

**以上四种IO都是同步IO**，最后将数据拷贝到用户空间的任务都是由用户态下的进程来实现的，而异步IO，内核帮我们完成了数据的拷贝，应用进程直接取数据即可。

### 5.异步IO ：

 `aio_read()` 系统调用会立即返回，应用进程继续执行，不会被阻塞，内核会在所有操作完成之后向应用进程发送信号。

异步 I/O 与信号驱动 I/O 的区别在于，异步 I/O 的信号是通知应用进程 I/O 完成，而信号驱动 I/O 的信号是通知应用进程可以开始从内核缓冲区拷贝数据。

<img src="https://pdai.tech/_images/io/java-io-model-4.png" alt="img" style="zoom:80%;" />

# 五、I/O管理

## 1.I/O设备概念 

### I.定义：

I/O设备就是输入输出设备，是一类与计算机内存与外界硬件交互的硬件设备，可以将数据输出到计算机也可以将计算机内部数据输出到外部设备。比如：鼠标、键盘（输入型设备）；显示器（输出型设备）；移动硬盘（既是输入又是输出设备）

### II．分类：

<img src="https://img-blog.csdnimg.cn/20200515103035186.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom: 50%;" />

<img src="https://img-blog.csdnimg.cn/20200515104001796.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:40%;" />

<img src="https://img-blog.csdnimg.cn/20200515104649817.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:50%;" />

- *块设备*，把数据存储在固定大小的块中，每个块有自己的地址，硬盘、USB 是常见的块设备。
- *字符设备*，以字符为单位发送或接收一个字符流，字符设备是不可寻址的，也没有任何寻道操作，鼠标是常见的字符设备。

**块设备通常传输的数据量会非常大**，于是控制器设立了一个可读写的**数据缓冲区**。

- CPU 写入数据到控制器的缓冲区时，当缓冲区的数据囤够了一部分，才会发给设备。
- CPU 从控制器的缓冲区读取数据时，也需要缓冲区囤够了一部分，才拷贝到内存。

这样做是为了，减少对设备的频繁操作。

## 2.I/O(设备)控制器

### I.定义：

我们的电脑设备可以接非常多的输入输出设备，比如键盘、鼠标、显示器、网卡、硬盘、打印机、音响等等，每个设备的用法和功能都不同，那操作系统是如何把这些输入输出设备统一管理的呢?

为了屏蔽设备之间的差异，**每个设备都有一个叫I/O(设备)控制器（Device Control） 的组件**，比如硬盘有硬盘控制器、显示器有视频控制器等。**由CPU控制I/O控制器，I/O控制器用于控制I/O设备。**

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA/I_O%E7%B3%BB%E7%BB%9F%E7%BB%93%E6%9E%84.png" alt="计算机 I/O 系统结构" style="zoom:50%;" />

### II.I/O控制器的功能：

<img src="C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221111134220467.png" alt="image-20221111134220467" style="zoom:67%;" />

### III.I/O控制器的组成：

<img src="https://img-blog.csdnimg.cn/20200515111916102.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:70%;" />

**I/O控制器中三个寄存器的作用：**

- *数据寄存器*，**CPU 向 I/O 设备写入需要传输的数据**，比如要打印的内容是「Hello」，CPU 就要先发送一个 H 字符给到对应的 I/O 设备。
- *命令寄存器*，**CPU 发送一个命令，告诉 I/O 设备用户想要进行的操作**，要进行输入/输出操作，于是就会交给 I/O 设备去工作，任务完成后，会把状态寄存器里面的状态标记为完成。
- *状态寄存器*，**目的是告诉 CPU当前设备的使用情况** ，现在已经在工作或工作已经完成，如果已经在工作状态，CPU 再发送数据或者命令过来，都是没有用的，直到前面的工作已经完成，状态寄存标记成已完成，CPU 才能发送下一个字符和命令。

<img src="C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221111134645349.png" alt="image-20221111134645349" style="zoom:67%;" />

#### I/O控制器的两种寄存器编址方式

<img src="https://img-blog.csdnimg.cn/20200515112219587.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:67%;" />

 CPU 是如何与设备的控制寄存器和数据缓冲区进行通信的？存在两个方法：

- ***端口 I/O***：每个控制寄存器被分配一个 I/O 端口，可以通过**特殊的汇编指令**操作这些寄存器，比如 `in/out` 类似的指令。
- ***内存映射 I/O***：将所有控制寄存器映射到内存空间中，这样就可以像读写内存一样读写数据缓冲区。

## 3.OS控制I/O设备的几种方式

### I.程序直接控制方式

轮询等待：状态寄存器中标识了当前设备的工作状态，所以可以让CPU一直轮询I/O控制器的状态寄存器，直到I/O结束后返回。这个过程**CPU会一直被占用，处于忙等状态，利用率极低**。

<img src="C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221111140259514.png" alt="image-20221111140259514" style="zoom:67%;" />

### II.中断驱动方式

由于轮询方式CPU利用效率很低，引入**中断**，当进程发生I/O事件的时候，直接将当前进程阻塞，切换到其它进程，**当I/O完成后，中断控制器向CPU发送一个中断信号**，当CPU检测到中断信号后，会保留当前进程上下文，切换到中断处理程序处理数据。

<img src="https://img-blog.csdnimg.cn/20200515165910621.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:67%;" />

### III.DMA方式

虽然**中断驱动方式解决了程序直接控制方式的问题，但是每一次只能读/写一个字**，导致CPU频繁切换，耗费了很多时间。于是人们又发明了DMA方式。

 **DMA（Direct Memory Access）** 功能，它可以使得设备在 **CPU 不参与**的情况下，能够**自行完成把设备 I/O 数据放入到内存，并且DMA方式是按块传输的**。那要实现 DMA 功能要有 「DMA 控制器」硬件的支持。

<img src="https://img-blog.csdnimg.cn/20200515171317683.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:67%;" />

DMA 的工作方式如下：

- CPU 需对 DMA 控制器下发指令，告诉它**想读取多少数据**，**设置MAR寄存器**以告知DMA控制器将读完的数据放在内存的某个地方就可以了；
- 接下来，DMA 控制器会向磁盘控制器发出指令，通知它从磁盘读数据到其内部的缓冲区中，接着磁盘控制器将缓冲区的数据传输到内存；
- 当磁盘控制器把数据传输到内存的操作完成后，磁盘控制器在总线上发出一个确认成功的信号到 DMA 控制器；
- DMA 控制器收到信号后，DMA 控制器发中断通知 CPU 指令完成，CPU 就可以直接用内存里面现成的数据了；

<img src="https://img-blog.csdnimg.cn/20200515171734106.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom: 67%;" />

### IV.通道控制方式

**通道控制方式是为了解决DMA方式连续存储的问题**

<img src="https://img-blog.csdnimg.cn/20200515172334709.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:57%;" />

<img src="https://img-blog.csdnimg.cn/20200515173712780.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:57%;" />

### V.总结与对比：

<img src="https://img-blog.csdnimg.cn/20200515173946936.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:60%;" />

## 4.I/O软件层次架构

### I.用户层软件

<img src="https://img-blog.csdnimg.cn/20200515181047151.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:57%;" />

### II.设备独立性软件

设备独立软件，又称为设备无关性软件。与设备的硬件特性无关的功能几乎都在这层实现。

#### **六大功能：**

- **向上层提供统一接口**（如read/write系统调用）
- **设备的保护**，与文件保护类似，不同的应用程序拥有不同的设备的访问权限
- **差错处理**
- **设备的分配与回收**
- **数据缓冲区管理**，屏蔽设备之间数据交换单位大小和传输速率的差异
- **建立逻辑设备名到物理设备名的映射关系**，**根据设备类型选择相应的设备驱动程序**

#### 逻辑设备表—LUT

<img src="https://img-blog.csdnimg.cn/20200515182429409.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom: 50%;" />

### III.设备驱动程序

<img src="https://img-blog.csdnimg.cn/20200515184018765.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:57%;" />

不同的**设备控制器**虽然功能不同，但是**设备驱动程序会提供统一的接口给操作系统**，这样不同的设备驱动程序，就可以以相同的方式接入操作系统。**设备驱动程序是服务于I/O(设备)控制器硬件的。**

当CPU发送给设备控制器一个I/O请求的时候，设备控制器就会安排设备区执行任务，设备完成了任务，**设备（I/O）控制器**则会发送**中断**来通知操作系统。那操作系统就需要有一个地方来处理这个中断，这个地方也就是在**设备驱动程序**里，它会及时响应控制器发来的中断请求，并根据这个中断的类型调用响应的**中断处理程序**进行处理。

### IV.中断处理程序

<img src="https://img-blog.csdnimg.cn/20200515184322628.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:57%;" />

### 总结：

<img src="https://img-blog.csdnimg.cn/20200515184834582.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:57%;" />

## 5.通用块层

对于块设备，为了减少不同块设备的差异带来的影响，Linux 通过一个统一的**通用块层**，来管理不同的块设备。

**通用块层是处于文件系统和磁盘驱动中间的一个块设备抽象层**，它主要有两个功能：

- 第一个功能，**向上为文件系统和应用程序，提供访问块设备的标准接口**，**向下把各种不同的磁盘设备抽象为统一的块设备**，并在内核层面，提供一个框架来管理这些设备的驱动程序；
- 第二功能，通用层还会给文件系统和应用程序发来的 I/O 请求排队，接着会对队列重新排序、请求合并等方式，也就是 I/O 调度，主要目的是为了提高磁盘读写的效率。

Linux 内存支持 5 种 I/O 调度算法，分别是：

- 没有调度算法
- 先入先出调度算法
- 完全公平调度算法
- 优先级调度
- 最终期限调度算法

第一种，**没有调度算法**，它不对文件系统和应用程序的 I/O 做任何处理，这种算法常用在虚拟机 I/O 中，此时磁盘 I/O 调度算法交由物理机系统负责。

第二种，**先入先出调度算法**，这是最简单的 I/O 调度算法，先进入 I/O 调度队列的 I/O 请求先发生。

第三种，**完全公平调度算法**，大部分系统都把这个算法作为默认的 I/O 调度器，它为每个进程维护了一个 I/O 调度队列，并按照时间片来均匀分布每个进程的 I/O 请求。

第四种，**优先级调度算法**，顾名思义，优先级高的 I/O 请求先发生， 它适用于运行大量进程的系统，像是桌面环境、多媒体应用等。

第五种，**最终期限调度算法**，分别**为读、写请求创建了不同的 I/O 队列**，这样可以提高机械磁盘的吞吐量，并确保达到最终期限的请求被优先处理，适用于在 I/O 压力比较大的场景，比如数据库等

## 6.存储系统 I/O 软件分层

可以把 Linux 存储系统的 I/O 由上到下可以分为**三个层次**，分别是**文件系统层、通用块层、设备层**。他们整个的层次关系如下图：

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA/I_O%E8%BD%AF%E4%BB%B6%E5%88%86%E5%B1%82.png" alt="img" style="zoom:60%;" />

这三个层次的作用是：

- 文件系统层，包括虚拟文件系统和其他文件系统的具体实现，它**向上为应用程序统一提供了标准的文件访问接口，向下会通过通用块层来存储和管理磁盘数据。**
- 通用块层，**包括块设备的 I/O 队列和 I/O 调度器**，它会对文件系统的 I/O 请求进行排队，再通过 I/O 调度器，选择一个 I/O 发给下一层的设备层。
- 设备层，包括**硬件设备、设备控制器和驱动程序**，负责最终物理设备的 I/O 操作。

另外，存储系统的 I/O 是整个系统最慢的一个环节，所以 Linux 提供了不少缓存机制来提高 I/O 的效率。

- 为了提高**文件**访问的效率，会使用**页缓存、索引节点缓存、目录项缓存**等多种缓存机制，目的是为了减少对块设备的直接调用。
- 为了提高**块设备**的访问效率， 会使用**缓冲区**，来缓存块设备的数据。

![image-20221111151513788](C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221111151513788.png)

<img src="C:\Users\龚chang\AppData\Roaming\Typora\typora-user-images\image-20221111151606722.png" alt="image-20221111151606722" style="zoom:67%;" />

# 六、操作系统内核与网络

## 1.零拷贝详解：

**零拷贝**：**CPU全程不参与数据的拷贝，只负责给DMA发送数据读取写入命令，由DMA完成全过程的数据拷贝，它需要网卡支持 SG-DMA。**在Linux2.4版本以上提供了`sendfile()`系统调用来传输数据，该API底层就是由零拷贝的支持，**零拷贝只发生两次数据拷贝和一次系统调用**，是传统I/O性能的两倍以上。

### I.DMA的引入：

**中断驱动I/O**：CPU需对I/O控制器下发读取指令，当I/O设备将一个字的数据准备好后，控制器将发送I/O中断信号，此时CPU挂起当前进程，执行中断处理程序去取一个字大小的数据，将数据拷贝到内核缓冲区，又将数据再拷贝到用户程序。该过程CPU全程参与并且每次只能读取一个字的数据，会消耗大量CPU时间。

为解决这一问题，降低CPU时间消耗，引入了DMA硬件。**CPU 需对 DMA 控制器下发读取指令**，告诉它想读取多少数据和一个存放数据的内存地址，此时当I/O设备将数据准备好以后，由DMA将数据拷贝到内核，此时DMA控制器向CPU发送一个中断信号，此时CPU再将内核缓冲区的数据拷贝到应用程序即可。这**个过程CPU只进行了一次内存拷贝**，并且由外部设备到内存这个比较耗时的步骤由DMA代替完成了，除此之外，**DMA是按块传输数据的**，发生中断的频率也比I/O中断驱动低。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/DRM%20I_O%20%E8%BF%87%E7%A8%8B.png" alt="img" style="zoom: 40%;" />

### II.传统的文件传输方式：

传统 I/O 的工作方式是，数据读取和写入是从用户空间到内核空间来回复制，而内核空间的数据是通过操作系统层面的 I/O 接口从磁盘读取或写入。

代码通常如下，一般会需要两个系统调用：

```c
read(file, tmp_buf, len);
write(socket, tmp_buf, len);
```

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/%E4%BC%A0%E7%BB%9F%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93.png" alt="img" style="zoom:50%;" />

- 发生read()系统调用时，**用户程序切换到内核态**，CPU发起I/O请求给DMA控制器，DMA控制器又发送I/O请求给磁盘控制器，磁盘硬件开始准备数据
- 当数据准备好后，**DMA**控制器将磁盘文件中的数据**拷贝**到内核缓冲区
- **此时CPU切换到用户态**，将内核缓冲区里的数据**拷贝**回用户缓冲区
- 然后又发送write()系统调用**切换为核心态**，CPU负责将数据**拷贝**到socket缓冲区
- 当拷贝结束后CPU通知DMA控制器，然后**切换到用户态**，DMA控制器负责将socket缓冲区内的数据**拷贝**到网卡

**可见，整个过程发生了四次CPU上下文切换和四次数据拷贝，这种传输速率是十分糟糕的。**所以，**要想提高文件传输的性能，就需要减少「用户态与内核态的上下文切换」和「内存拷贝」的次数**。

### III.零拷贝的实现：

零拷贝技术实现的方式通常有 2 种：

- mmap + write
- sendfile

#### mmap + write

```c
buf = mmap(file, len);
write(sockfd, buf, len);
```

`mmap()` 系统调用函数会直接把内核缓冲区里的数据「**映射**」到用户空间，这样，操作系统内核与用户空间就不需要再进行任何的数据拷贝操作。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/mmap%20%2B%20write%20%E9%9B%B6%E6%8B%B7%E8%B4%9D.png" alt="img" style="zoom:67%;" />

- 当发生`mmap()`系统调用时，**CPU切换到核心态**，**DMA**开始将磁盘设备准备好的数据**拷贝**到内核，之后用户程序开始和内核共享缓冲区。
- 此时**CPU用切换为用户态**，但不用将数据拷贝回用户缓冲区
- 当发生write()系统调用时，**CPU再次切换到核心态**，由于用户程序和内核共享缓冲区数据，**CPU**直接将内核缓冲区数据**拷贝**到socket缓冲区
- CPU切换到用户态，**DMA**将socket缓冲区数据**拷贝**到网卡。

可见整个过程发生了3次数据拷贝和4次CPU上下文切换，比起前者少了一次数据拷贝。

#### sendfile

在 Linux 内核版本 2.1 中，提供了一个专门发送文件的系统调用函数 `sendfile()`，函数形式如下：

```c
#include <sys/socket.h>
ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);
```

它的前两个参数分别是目的端和源端的文件描述符，后面两个参数是源端的偏移量和复制数据的长度，返回值是实际复制数据的长度。

首先，它可以替代前面的 `read()` 和 `write()` 这两个系统调用，这样就可以减少一次系统调用，也就减少了 2 次上下文切换的开销。

其次，该系统调用，可以直接把内核缓冲区里的数据拷贝到 socket 缓冲区里，不再拷贝到用户态，这样就只有 2 次上下文切换，和 3 次数据拷贝。如下图：

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/senfile-3%E6%AC%A1%E6%8B%B7%E8%B4%9D.png" alt="img" style="zoom: 50%;" />

但是这**还不是真正的零拷贝技术**，如果网卡支持 SG-DMA（*The Scatter-Gather Direct Memory Access*）技术（和普通的 DMA 有所不同），我们可以进一步减少通过 CPU 把内核缓冲区里的数据拷贝到 socket 缓冲区的过程。

你可以在你的 Linux 系统通过下面这个命令，查看网卡是否支持 scatter-gather 特性：

```bash
$ ethtool -k eth0 | grep scatter-gather
scatter-gather: on
```

于是，从 Linux 内核 `2.4` 版本开始起，对于支持网卡支持 SG-DMA 技术的情况下， `sendfile()` 系统调用的过程发生了点变化，具体过程如下：

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/senfile-%E9%9B%B6%E6%8B%B7%E8%B4%9D.png" alt="img" style="zoom:50%;" />

这就是所谓的**零拷贝（Zero-copy）技术，因为我们没有在内存层面去拷贝数据，也就是说全程没有通过 CPU 来搬运数据，所有的数据都是通过 DMA 来进行传输的。**。

零拷贝技术的文件传输方式相比传统文件传输的方式，减少了 2 次上下文切换和数据拷贝次数，**只需要 2 次上下文切换和数据拷贝次数，就可以完成文件的传输，而且 2 次的数据拷贝过程，都不需要通过 CPU，2 次都是由 DMA 来搬运。**

### IV.使用零拷贝技术的项目

**Kafka** 这个开源项目，就利用了「零拷贝」技术，从而大幅提升了 I/O 的吞吐率，这也是 Kafka 在处理海量数据为什么这么快的原因之一。

如果你追溯 Kafka 文件传输的代码，你会发现，最终它调用了 Java NIO 库里的 `transferTo` 方法：

```java
@Overridepublic 
long transferFrom(FileChannel fileChannel, long position, long count) throws IOException { 
    return fileChannel.transferTo(position, count, socketChannel);
}
```

如果 Linux 系统支持 `sendfile()` 系统调用，那么 `transferTo()` 实际上最后就会使用到 `sendfile()` 系统调用函数。

另外，**Nginx** 也支持零拷贝技术，一般默认是开启零拷贝技术，这样有利于提高文件传输的效率，是否开启零拷贝技术的配置如下：

```text
http {
...
    sendfile on
...
}
```

sendfile 配置的具体意思:

- 设置为 on 表示，使用零拷贝技术来传输文件：sendfile ，这样只需要 2 次上下文切换，和 2 次数据拷贝。
- 设置为 off 表示，使用传统的文件传输技术：read + write，这时就需要 4 次上下文切换，和 4 次数据拷贝。

当然，要使用 sendfile，Linux 内核版本必须要 2.1 以上的版本。

## 2.I/O多路复用 

为每个请求分配一个进程/线程的方式不合适，当请求剧增的时候，内存开销极大，操作系统吃不消。这时就引入了 **I/O 多路复用**技术。**I/O多路复用也称事件驱动型I/O**，使得**一个进程/线程由同时处理多个事件**`（连接事件/读事件等）`的能力。

select/poll/epoll 内核提供给用户态的多路复用系统调用，**进程可以通过一个系统调用函数从内核中获取多个事件**。

### select/poll

**select()函数允许程序监视多个文件描述符**，使用select函数可以完成非阻塞方式工作的程序，它能够监视我们需要监视的文件描述符的变化情况——读写或是异常。
非阻塞方式：non-block，就是进程或线程执行此函数时不必非要等待事件的发生，一旦执行肯定返回，以返回值的不同来反映函数的执行情况，如果事件发生则与阻塞方式相同，若事件没有发生，则返回一个代码来告知事件未发生，而进程或线程继续执行，所以效率较高。

select函数用来统一监视多个文件描述符的：

- 是否存在套接字接收数据？
- 无需阻塞传输数据的套接字有哪些?
- 哪些套接字发生了异常？

select函数调用过程：
![这里写图片描述](https://img-blog.csdn.net/20180401131306468?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hteHoybm4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

```c
#include <stdio.h>
#include <unistd.h>
#include <sys/time.h>
#include <sys/select.h>

#define BUF_SIZE 30

int main(int argc, char* argv[])
{
    fd_set reads,temps;
    int result, str_len;
    char buf[BUF_SIZE];
    struct timeval timeout;
    FD_ZERO(&reads);
    FD_SET(0, &reads);//监视文件描述符0的变化, 即标准输入的变化
    /*超时不能在此设置！
    因为调用select后，结构体timeval的成员tv_sec和tv_usec的值将被替换为超时前剩余时间.
    调用select函数前，每次都需要初始化timeval结构体变量.
    timeout.tv_sec = 5;
    timeout.tv_usec = 5000;*/
    while(1)
    {
        /*将准备好的fd_set变量reads的内容复制到temps变量，因为调用select函数后，除了发生变化的fd对应位外，
        剩下的所有位都将初始化为0，为了记住初始值，必须经过这种复制过程。*/
        temps = reads;
        //设置超时
        timeout.tv_sec = 5;
        timeout.tv_usec = 0;

        //调用select函数. 若有控制台输入数据，则返回大于0的整数，如果没有输入数据而引发超时，返回0.
        result = select(1, &temps, 0, 0, &timeout);
        if(result == -1)
        {
            perror("select() error");
            break;
        }
        else if(result == 0)
        {
            puts("timeout");
        }
        else
        {
            //读取数据并输出
            if(FD_ISSET(0, &temps))
            {
                str_len = read(0, buf, BUF_SIZE);
                buf[str_len] = 0;
                printf("message from console: %s", buf);
            }
        }
    }

    return 0;
}
```

select 实现多路复用的方式是，将已连接的 Socket 都放到一个**文件描述符集合**，然后调用 select 函数将文件描述符集合**拷贝**到内核里，让内核来检查是否有网络事件产生，检查的方式很粗暴，就是通过**遍历**文件描述符集合的方式，当检查到有事件产生后，将此 Socket 标记为可读或可写， 接着再把整个文件描述符集合**拷贝**回用户态里，然后用户态还需要再通过**遍历**的方法找到可读或可写的 Socket，然后再对其处理。

所以，对于 select 这种方式，需要进行 **2 次「遍历」文件描述符集合**，一次是在内核态里，一个次是在用户态里 ，而且还会发生 **2 次「拷贝」文件描述符集合**，先从用户空间传入内核空间，由内核修改后，再传出到用户空间中。

select 使用固定长度的 BitsMap，表示文件描述符集合，而且所支持的文件描述符的个数是有限制的，在 Linux 系统中，由内核中的 FD_SETSIZE 限制， 默认最大值为 `1024`，只能监听 0~1023 的文件描述符。

poll 不再用 BitsMap 来存储所关注的文件描述符，取而代之用动态数组，以链表形式来组织，突破了 select 的文件描述符个数限制，当然还会受到系统文件描述符限制。

但是 poll 和 select 并没有太大的本质区别，**都是使用「线性结构」存储进程关注的 Socket 集合，因此都需要遍历文件描述符集合来找到可读或可写的 Socket，时间复杂度为 O(n)，而且也需要在用户态与内核态之间拷贝文件描述符集合**，这种方式随着并发数上来，性能的损耗会呈指数级增长。

### epoll

先用e poll_create 创建一个 epol l对象 epfd，再通过 epoll_ctl 将需要监视的 socket 添加到epfd中，最后调用 epoll_wait 等待数据。

```c
int s = socket(AF_INET, SOCK_STREAM, 0);
bind(s, ...);
listen(s, ...)

int epfd = epoll_create(...);
epoll_ctl(epfd, ...); //将所有需要监听的socket添加到epfd中

while(1) {
    int n = epoll_wait(...);
    for(接收到数据的socket){
        //处理
    }
}
```

epoll 通过两个方面，很好解决了 select/poll 的问题。

*第一点*，epoll 在内核里使用**红黑树来跟踪进程所有待检测的文件描述字**，把需要监控的 socket 通过 `epoll_ctl()` 函数加入内核中的红黑树里，红黑树是个高效的数据结构，增删改一般时间复杂度是 `O(logn)`。而 select/poll 内核里没有类似 epoll 红黑树这种保存所有待检测的 socket 的数据结构，所以 select/poll 每次操作时都传入整个 socket 集合给内核，而 epoll 因为在内核维护了红黑树，可以保存所有待检测的 socket ，所以只需要传入一个待检测的 socket，减少了内核和用户空间大量的数据拷贝和内存分配。

*第二点*， epoll 使用**事件驱动**的机制，内核里**维护了一个链表来记录就绪事件**，当某个 socket 有事件发生时，通过**回调函数**内核会将其加入到这个就绪事件列表中，当用户调用 `epoll_wait()` 函数时，只会返回有事件发生的文件描述符的个数，不需要像 select/poll 那样轮询扫描整个 socket 集合，大大提高了检测的效率。

从下图你可以看到 epoll 相关的接口作用：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/epoll.png)

epoll 的方式即使监听的 Socket 数量越多的时候，效率不会大幅度降低，能够同时监听的 Socket 的数目也非常的多了，上限就为系统定义的进程打开的最大文件描述符个数。因而，**epoll 被称为解决 C10K 问题的利器**。

### 边缘触发和水平触发

epoll 支持两种事件触发模式，分别是**边缘触发（edge-triggered，ET）和水平触发（level-triggered，LT）**。

这两个术语还挺抽象的，其实它们的区别还是很好理解的。

- **水平触发(level-triggered)**

  - socket接收缓冲区不为空 有数据可读 读事件一直触发
  - socket发送缓冲区不满 可以继续写入数据 写事件一直触发

- **边缘触发(edge-triggered)**

  - socket的接收缓冲区状态变化时触发读事件，即空的接收缓冲区刚接收到数据时触发读事件
  - socket的发送缓冲区状态变化时触发写事件，即满的缓冲区刚空出空间时触发读事件

  **边缘触发仅触发一次，水平触发会一直触发。**

  水平触发的意思是只要满足事件的条件，比如内核中有数据需要读，就一直不断地把这个事件传递给用户；而边缘触发的意思是只有第一次满足条件的时候才触发，之后就不会再传递同样的事件了。

如果使用水平触发模式，当内核通知文件描述符可读写时，接下来还可以继续去检测它的状态，看它是否依然可读或可写。所以在收到通知后，没必要一次执行尽可能多的读写操作。

如果使用边缘触发模式，I/O 事件发生时只会通知一次，而且我们不知道到底能读写多少数据，所以在收到通知后应尽可能地读写数据，以免错失读写的机会。因此，我们会**循环**从文件描述符读写数据，那么如果文件描述符是阻塞的，没有数据可读写时，进程会阻塞在读写函数那里，程序就没办法继续往下执行。所以，**边缘触发模式一般和非阻塞 I/O 搭配使用**，程序会一直执行 I/O 操作，直到系统调用（如 `read` 和 `write`）返回错误，错误类型为 `EAGAIN` 或 `EWOULDBLOCK`。

**一般来说，边缘触发的效率比水平触发的效率要高**，因为边缘触发可以减少 epoll_wait 的系统调用次数，系统调用也是有一定的开销的的，毕竟也存在上下文的切换。

select/poll 只有水平触发模式，epoll 默认的触发模式是水平触发，但是可以根据应用场景设置为边缘触发模式。

# 七、磁盘组织与管理

## 1.磁盘整体架构

- 磁盘
- 磁道
- 扇区

<img src="https://img-blog.csdnimg.cn/20200527182515944.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:57%;" />

### 1.1 如何在磁盘中读/写数据

<img src="https://img-blog.csdnimg.cn/2020052718281993.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:50%;" />

### 1.2 盘面、柱面

<img src="https://img-blog.csdnimg.cn/20200527183213448.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:50%;" />

### 1.3 磁盘的分类

<img src="https://img-blog.csdnimg.cn/202005271833589.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom: 55%;" />

<img src="https://img-blog.csdnimg.cn/20200527183434406.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:50%;" />

## 2.磁盘调度算法

### 2.1 I/O时间

- **寻道时间`（T寻道）`：**在读取数据之前，磁头移动到相应磁道所需的时间
  - 启动磁头臂
  - 移动磁头
- **延迟时间**（旋转时间）:通过旋转磁盘，使磁头定位到目标扇区所需要的时间。
- **传输时间**：从磁盘读出或向磁盘写入数据的时间

​	:innocent: **I/O时间 = 寻道时间 + 旋转时间 + 传输时间**

### 2.2  先来先服务算法（*FCFS*）

<img src="https://img-blog.csdnimg.cn/20200527192035648.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:67%;" />

### 2.3 最短寻找时间优先算法（SSTF）

最短寻道时间优先（*Shortest Seek First，SSF*）算法的工作方式是，**优先选择从当前磁头位置所需寻道时间最短的请求**

<img src="https://img-blog.csdnimg.cn/20200527192305288.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:67%;" />

### 2.4 扫描算法（SCAN）

最短寻道时间优先算法会产生饥饿的原因在于：磁头有可能再一个小区域内来回得移动。

为了防止这个问题，可以规定：**磁头在一个方向上移动，访问所有未完成的请求，直到磁头到达该方向上的最后的磁道（每次都要移动到最外侧或最内侧），才调换方向，这就是扫描（Scan）算法**。

<img src="https://img-blog.csdnimg.cn/20200527193037761.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom: 50%;" />

### 2.5 LOOK算法

磁头在每个方向上仅仅移动到最远的请求位置，然后立即反向移动，而不需要移动到磁盘的最始端或最末端，**反向移动的途中会响应请求**。

<img src="https://img-blog.csdnimg.cn/20200527193300867.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:67%;" />

### 2.6 循环扫描算法（C-SCAN）

<img src="https://img-blog.csdnimg.cn/2020052719355596.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom: 50%;" />

### 2.7 C-LOOK算法

磁头在每个方向上仅仅移动到最远的请求位置，然后立即反向移动，而不需要移动到磁盘的最始端或最末端，**反向移动的途中不会响应请求**。**磁头永远只向一个方向移动。**

![img](https://img-blog.csdnimg.cn/20200527193744152.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkxNDYwNA==,size_16,color_FFFFFF,t_70)
